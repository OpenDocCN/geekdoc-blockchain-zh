- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2021Y.
    Maleh et al. (eds.)Artificial Intelligence and Blockchain for Future Cybersecurity
    ApplicationsStudies in Big Data90[https://doi.org/10.1007/978-3-030-74575-2_13](https://doi.org/10.1007/978-3-030-74575-2_13)
  prefs: []
  type: TYPE_NORMAL
- en: A Framework Using Artificial Intelligence for Vision-Based Automated Firearm
    Detection and Reporting in Smart Cities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Muhammad Hunain^([1](#Aff7) [ ](#ContactOfAuthor1)), Talha Iqbal^([1](#Aff7) [ ](#ContactOfAuthor2)),
    Muhammad Assad Siyal^([1](#Aff7)), Muhammad Azmi Umer^([2](#Aff8) [ ](#ContactOfAuthor4))
    and Muhammad Taha Jilani^([3](#Aff9) [ ](#ContactOfAuthor5))(1)DHA Suffa University,
    Karachi, Pakistan(2)DHA Suffa University, and KIET Karachi, Karachi, Pakistan(3)Karachi
    Institute of Economics and Technology, Karachi, PakistanMuhammad HunainEmail:
    [Hunain@techonventures.com](mailto:Hunain@techonventures.com)Talha IqbalEmail:
    [Talha@techonventures.com](mailto:Talha@techonventures.com)Muhammad Azmi Umer (Corresponding
    author)Email: [azmi.umer@dsu.edu.pk](mailto:azmi.umer@dsu.edu.pk)Muhammad Taha JilaniEmail:
    [m.taha@pafkiet.edu.pk](mailto:m.taha@pafkiet.edu.pk)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a few decades, mega-cities are facing some huge challenges. Among them,
    the prevention of crime seems to be more challenging than others. The safety of
    citizens in the dense urban population with conventional practices are unable
    to control the increasing crime rate. This work is aimed to develop a framework
    for the autonomous surveillance of public places, with visual-based handheld arms
    detection in a near real-time. It scans all the objects that come in front of
    the camera and when any type of weapon comes in contact with a lens it gives an
    alert, locks that object and the person holding it and identifies the person using
    facial recognition. If the alert does not get responded in a few minutes, the
    system will automatically notify the 3rd person or agency about the incident.
    It can also manually highlight any object in a frame to keep track of its movement
    for security purposes. Machine and Deep Learning techniques were used to train
    models for object detection and facial recognition. The model achieved an accuracy
    of 97.33% in object detection and 90% in facial recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the past few decades, various urban areas within developing countries have
    experienced a growing population and rural-to-urban migration rate. It is estimated
    that nearly half the population of the world is now living in the cities [[1](#CR1)],
    now making them mega-cities which can be seen by World Economic Forum (WEF) reports’
    statistics in Fig. [1](#Fig1). This rapid transition has presented many challenges,
    including risks to the immediate and surrounding environment, to natural resources,
    to health conditions, to social cohesion, and individual rights [[2](#CR2)]. The
    later has introduced the safety and security concerns for the citizens living
    in a megacity. Similarly, for governments and administrative agencies, one of
    the most important consideration is to monitor and control the criminal activities.
    Table [1](#Tab1) has described the number of incidences and crime rates in major
    cities of India.![../images/507793_1_En_13_Chapter/507793_1_En_13_Fig1_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Fig1_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 1
  prefs: []
  type: TYPE_NORMAL
- en: Urban population growth [[3](#CR3)]
  prefs: []
  type: TYPE_NORMAL
- en: Table 1
  prefs: []
  type: TYPE_NORMAL
- en: Incidences and crime rates in mega cities [[4](#CR4)]
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | No. of incidence | Crime rate |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2009 | 8,91,576 | 826.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2010 | 11,19,621 | 1037.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2011 | 11,49,059 | 713.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | 11,03,858 | 685.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | 12,03,514 | 748.8 |'
  prefs: []
  type: TYPE_TB
- en: In conventional practice, such issues are addressed by using CCTV based surveillance
    and monitoring only. However, current developments in ICT have opened new opportunities
    to develop some intelligent methods for effective control and monitoring of crime.
    Over the past few years, some topics have been top in research areas in computer
    technological era. Those are detection, tracking, and understanding the moving
    objects to prevent crime. Similarly, Intelligent visual surveillance system (IVSS)
    are one of the surveillance system that refers to automate visual monitoring process
    involving interpretation and analysis of object detection and behavior, also the
    tracking of that object to understand the current scene of that visual events.
    Two main tasks that are highly focused are discussed in [[5](#CR5)] i.e. scene
    anomaly and large area surveillance control. All detection and tracking of moving
    objects in a sequence and behavior analysis are in scene interpretation. The control
    task multiple cameras are to tackle captured or fixed objects which are in motion
    in a wide-area surveillance.
  prefs: []
  type: TYPE_NORMAL
- en: Detection of moving objects is a hectic task as well as it is an important task
    for any video surveillance system. Secondly, tracking is required in upper-level
    applications after detection because it requires the location and shape of objects
    in every camera region or frame via detection algorithm [[6](#CR6)]. A video surveillance
    might embody a minimum of one sensing unit capable of being operated in a very
    scanning mode and a video process unit coupled to the sensing unit, the video
    process unit to receive and method image information from the sensing unit and
    to find scene events and target activity [[7](#CR7)]. Similarly, a system proposed
    in [[8](#CR8)], which was mostly based on hardware devices like motion sensors,
    light sensors, alarms, etc. It detects the anomaly and reports the user through
    push notification on any handheld device like a mobile or laptop.
  prefs: []
  type: TYPE_NORMAL
- en: The campus security system was proposed in [[9](#CR9)]. This system is consist
    of a school gate state monitor, an entrance guide terminal and a base station,
    in this system entrance gate terminal monitors the presence of entrance guard
    or check whether the guard is on duty or not in the campus. Second is the school
    gate state monitor which monitors the state of the gate whether it is open or
    close. Third is a base station which receives information from the entrance guide
    terminal and school gate state monitor and generate alarm signals when the entrance
    guard is missing and the school gate is opened. This campus security system can
    monitor in real-time and can alert when detects an anomaly, which helps to improve
    the security system of the campus.
  prefs: []
  type: TYPE_NORMAL
- en: As the violent criminals, burglars and intruders have become so dangerous for
    the properties and lives of people. Protection and security for households become
    a necessity. Anti-Intruder Monitoring and Alarm [[10](#CR10)] with the purpose
    to help homeowners and make them informed about criminals and alarm triggering
    decisions. The alarm system uses images and locations of sensed motion and offers
    the option of allowing multiple key holders to receive security alerts via cellular
    network short message service (SMS). The alarm system also gives the option of
    sending distress messages to the police or trusted neighbors. The security system
    can be easily controlled by using a mobile device or remote control. The algorithm
    of this system has been designed simply and made the probability of false alarms
    almost non-existence.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the research carried out, there is no such application/software
    which is capable of doing surveillance and as well as identifying objects and
    people in real-time. Some products have some similarities in terms of facial recognition,
    data extraction, object detection, notifying 3rd party or security agency and
    generate an alarm system. But no one is completely satisfied by implementing all
    functionalities mentioned above in a single program as in the proposed system.
    These are the aspect that lead to this system an upend over previously launched
    products. Rest of the chapter is organized as follows: Sect. [2](#Sec2) is an
    overview of the related work. Section [3](#Sec3) has described the methodology.
    Section [4](#Sec12) has discussed the experimental evaluations and results, while
    Sect. [5](#Sec15) has discussed the conclusion and possible future work.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In today’s modern life there is an increasing interest in the precautionary
    and protective measures in the world and private space of social welfare. Therefore,
    there is a need to look for the surveillance arrangement to provide a safe and
    sound environment for the citizens. Currently, technologies like cameras, sensors,
    microphones, and detectors are being used. Trespasser detection is the new increasing
    demand in the commercial and private sectors. However, it is difficult to eradicate
    the concept of using these technologies without being detected. Hence looking
    at this flaw [[11](#CR11)] proposed the idea of a multi-sensor intelligent system
    that can operate on the principle of entropy from several sources to find the
    danger or any internet breach. Therefore they developed a generic ontology that
    allowed the integration of all the input heterogeneous knowledge in a homogeneous
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Handheld gun detection was performed in [[12](#CR12)]. They used Convolutional
    Neural Network (CNN) to detect guns from cluttered scenes. They particularly used
    Deep Convolution Network (DCN) through transfer learning. The model was evaluated
    on a benchmark Internet Movie Firearms Database (IMFDB). Similarly, CNN has been
    used in [[13](#CR13)] for gun detection. They got training accuracy of 93% and
    testing accuracy of 89%. Gun detection was also performed in [[14](#CR14)] using
    color-based segmentation. They used k-means clustering to omit objects other than
    the weapons from the images. Harris interest point detector and Fast Retina Keypoint
    (FREAK) were used to locate the weapons in the segmented images.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays home security and its safety become one of the biggest concerns for
    homeowners. Leveraging audio/video recording and communication devices provides
    methods for information about crime. An approach was proposed in [[15](#CR15)],
    which includes a method of comprising, a method of receiving from an audio/video
    recording, and a communication device. It has a first alert signal and a first
    video signal, the first video signal including images captured by a camera of
    the A/V recording and communication device, transmitting to a client device, in
    response to receiving the first alert signal and the first video signal, a second
    alert signal and a second video signal, the second video signal including the
    images captured by the camera of the A/V recording and communication device, receiving
    a report signal from the client device; It work on the images captured by the
    camera of the A/V recording and communication device, that a crime may have been
    committed, posting an offer of a reward for information about the crime.
  prefs: []
  type: TYPE_NORMAL
- en: An intelligent visual surveillance system has been proposed in [[16](#CR16)]
    with the help of cameras attached in the network to observe the people and vehicles.
    The system modules are proposed to perform critical works like the management
    of cameras, tracking objects, recognition of people via biometric technology,
    monitoring the crowd to catch anomaly. Similarly, [[17](#CR17)] is also based
    on the video surveillance system in which the system uses metadata rule for analyzing
    and exchange of information between intelligent video surveillance system that
    analyzes the required data through streaming on camera. The metadata rule is just
    to enhance the indexing method by indexing a large database and collaboratively
    searches and manages the integrated security environment more accurately and efficiently.
    The system focused on both high-level and low-level context to utilize metadata
    as a raw back source for security system services. Physical sensors (metal detector,
    cameras, scanners) in public areas are for the low-level context of the system.
    The situation is being captured in the high-level context-aware system by analyzing
    the context data coming through sensors in the low-level system. The system also
    provides the tracking system by moving an object in the field of view called FOVs.
    The system also supports real-time tracking of moving objects by tilting, panning
    and zooming in FOVs.
  prefs: []
  type: TYPE_NORMAL
- en: The digital surveillance system is pre-install by the ubiquitous approach and
    generates a huge amount of video streaming and other data as well. The development
    of the cloud environment has empowered to deploy intelligent video surveillance
    technologies through Web Services to enhance public security. The introduction
    of the novel system and the combination of cloud computing techniques with the
    automatized license plate recognition engines have been discussed in [[18](#CR18)].
    Its approach was to analyze big data to detect as well as to keep track of a target
    vehicle in a city with a license plate number issued to vehicles. Likewise, [[19](#CR19)]
    has discussed the reviews about the recent development techniques of relevant
    technologies like pattern recognition and computer vision. They have discussed
    the multi-camera tracking, topologies of computing with integrated cameras, multi-level
    frames object detection and tracking, identification and some sort of re-identification,
    and both static and active cameras’ cooperative video security. The detailed explanation
    of the technical aspect used by these terminologies and comparison of pros and
    cons between different approaches for solution has been provided. It mainly focuses
    on the connection and integration of different modules within the application.
    They have also focused on improving the efficiency, accuracy, and complexity.
    An intelligent video surveillance system (IVSS) has also been proposed in [[20](#CR20)]
    by having a functionality detection and identification of anomaly and alarming
    situations by sensing the moving objects. The main motive of this system design
    was to reduce video processing and transmission, therefore, allowing a huge number
    of cameras deploying on the system to satisfy its usage as a security solution
    with safety integration in smart cities. Here alarming and detection were performed
    based on moving objects using the feature parameters of performed detection results
    and also using ontologies and semantic reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Threat-detection in a distributed multi-camera surveillance system was proposed
    in [[21](#CR21)]. They observed the threats by analyzing the motion of an object
    in software installed at the first camera then detection of a suspicious object
    at the camera when motion of the object does not match to a motion flow model
    at the first camera. Then the tracking process is being entertained from the first
    frame to the second camera frame based upon the suspicious detection of objects.
    Just like the first camera, the second camera processing for detection is being
    done via the same software installed in it. As the first camera and assigned threat
    scores aside when motion of the object does not match to a motion flow model at
    the second camera, like the initial one and finally generating an alarm based
    on part of the threat scores detected at these frames of cameras and notifying
    the authorities.
  prefs: []
  type: TYPE_NORMAL
- en: The security system has been used for safety for homes and other areas greatly.
    The security system proposed in [[22](#CR22)] consists of a main automatic circuit
    which have motions detector for activating an audible alarm and provides further
    detections to identify criminals and crime. It has an emergency light flasher
    which is manually activated by the user. It provides an inside home control panel.
    Inside the home control panel also responds to remote manually. This system has
    been used more effectively that easily terminate possible home invasion or robbery.
    This system also enhances safety and security.
  prefs: []
  type: TYPE_NORMAL
- en: An intelligent image processing method for the video surveillance systems was
    proposed in [[23](#CR23)]. It includes a technology of tracking and detecting
    multiple moving objects, which can be easily applied to business and home surveillance
    systems consisting of a network video recorder (NVR) and internet protocol (IP)
    camera. It also provides the easiest way for detection and tracking, in which
    it uses the red-green-blue (RGB) color background modeling with a sensitivity
    parameter to extract moving regions, the blob-labeling to group moving objects
    and the morphology to eliminate noises. If it comes to the tracking of the fast-moving
    object then this method can define the direction as well as the velocity of the
    group formed by the objects which are in motion.
  prefs: []
  type: TYPE_NORMAL
- en: An intelligent video/sound analysis and ID database framework was proposed in [[24](#CR24)].
    It may define a security zone or gathering of zones. The framework may distinguish
    vehicles and people entering or leaving the zone through picture acknowledgement
    of the vehicle or individual when compared with prerecorded data available in
    a database. The framework may alarm the security workforce as to warrants or other
    data found relating to the perceived vehicle or individual coming out because
    of a database seek. The framework may analyze pictures of a presume vehicle, for
    example, an undercarriage picture to standard vehicle pictures recorded in the
    database. The framework may additionally take in the standard occasions and areas
    of vehicles or people followed by the framework and to make security workforce
    ready upon deviation from standard movement.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel execution of an ongoing canny video surveillance system on Illustrations
    Preparing Unit (GPU) was portrayed in [[25](#CR25)]. The system depends on foundation
    subtraction and made out of movement detection, camera attack detection (moved
    camera, out-of-center camera, and secured camera discovery), surrendered object
    detection, and object tracking algorithms. As the calculation algorithms have
    diverse qualities, their GPU executions have distinctive acceleration rates. Test
    results demonstrate that when all the available algorithms run simultaneously,
    parallelization in GPU influences the system to up to 21.88 times quicker than
    the central processing unit partner, empowering real-time analysis of a higher
    number of cameras.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine and Deep Learning techniques were used to train models for object detection
    and facial recognition. Further Transfer Learning was performed on the inception
    R-CNN V2 dataset. Extra layers were added to identify the weapon. Facial Identification
    was performed using the inception Haar Cascade Frontal Face dataset while Recognition
    was done using the MTCNN method. Object Locking was done using OCF-CRS algorithm.
    Data Extraction from social media was done using jsoup while the 3rd party notification
    was implemented using Twilio SMS. The complete GUI built in python using PyQt
    v4.11\. The comparison of the proposed system with existing work is described
    in Table [2](#Tab2).Table 2
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with existing work
  prefs: []
  type: TYPE_NORMAL
- en: '| Existing work | Features |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Object detection | Object tracking | Specification of unethical object |
    Database maintenance | Alarming system | Extraction of culprit’s information |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Proposed system | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figa_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figa_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figb_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figb_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figc_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figc_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figd_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figd_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Fige_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Fige_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figf_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figf_HTML.gif)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[26](#CR26)] | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figg_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figg_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figh_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figh_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figi_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figi_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figj_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figj_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figk_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figk_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figl_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figl_HTML.gif)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[27](#CR27)] | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figm_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figm_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Fign_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Fign_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figo_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figo_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figp_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figp_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figq_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figq_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figr_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figr_HTML.gif)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[28](#CR28)] | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figs_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figs_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figt_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figt_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figu_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figu_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figv_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figv_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figw_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figw_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figx_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figx_HTML.gif)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[29](#CR29)] | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figy_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figy_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figz_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figz_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figaa_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figaa_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figab_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figab_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figac_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figac_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figad_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figad_HTML.gif)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[30](#CR30)] | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figae_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figae_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figaf_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figaf_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figag_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figag_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figah_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figah_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figai_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figai_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figaj_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figaj_HTML.gif)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[31](#CR31)] | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figak_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figak_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figal_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figal_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figam_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figam_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figan_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figan_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figao_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figao_HTML.gif)
    | ![../images/507793_1_En_13_Chapter/507793_1_En_13_Figap_HTML.gif](../images/507793_1_En_13_Chapter/507793_1_En_13_Figap_HTML.gif)
    |'
  prefs: []
  type: TYPE_TB
- en: High Level Architecture, Software Architecture, Sequence diagram, and State
    diagram of the system are shown in Fig. [2](#Fig2), [3](#Fig3), [4](#Fig4), and
    [5](#Fig5) respectively.![../images/507793_1_En_13_Chapter/507793_1_En_13_Fig2_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Fig2_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 2
  prefs: []
  type: TYPE_NORMAL
- en: High level architecture
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/507793_1_En_13_Chapter/507793_1_En_13_Fig3_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Fig3_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3
  prefs: []
  type: TYPE_NORMAL
- en: Software architecture
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/507793_1_En_13_Chapter/507793_1_En_13_Fig4_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Fig4_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4
  prefs: []
  type: TYPE_NORMAL
- en: Sequence diagram
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/507793_1_En_13_Chapter/507793_1_En_13_Fig5_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Fig5_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5
  prefs: []
  type: TYPE_NORMAL
- en: State diagram
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Transfer Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Innovation plays an important role in the utilization of a pre-trained model.
    For instance, a model can be used without making any changes into it, for example,
    it can be used in an application to categorize new photos. The pre-trained model
    can be used in coordination with other neural network model. In this case, the
    load of the pre-trained model can be frozen considering the fact that they are
    not updated based on the newly trained model. Similarly, the load can be refreshed
    based on the training of new model. However, there could be a lower learning rate.
    This allows pre-trained model to behave like a weight initialization program during
    the training of the new model. Some of its common usages are as classifier and
    standalone feature extractor. The pre-trained model can be directly used as a
    classifier to classify new photos. The pre-trained model or some segment of the
    model can also be used to pre-process new photos and to extract useful attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Model Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many models for image identification. Some of them includes VGG (e.g.
    VGG16 or VGG19), GoogLeNet (e.g. InceptionV2), and Residual Network (e.g. ResNet50).
    They are usually used for transfer learning becasue of their architectural innovations.
    The process of loading the InceptionV3 pre-trained model is described below.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 InceptionV3 Pre-trained Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The InceptionV3 is commonly known as the third iteration of the inception architecture.
    Initially, it was developed for GoogLeNet. The model requires the colour images
    should be ![$$299\times 299$$](../images/507793_1_En_13_Chapter/507793_1_En_13_Chapter_TeX_IEq1.png).
    The script described in this section was taken from [[32](#CR32)]. Run the following
    script to load the model.![../images/507793_1_En_13_Chapter/507793_1_En_13_Figaq_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Figaq_HTML.png)After
    executing the above code, the model will be loaded. It will also download the
    required weights. It will summarize the model architecture to ensure that the
    model got loaded correctly. Now download photos to train the model and save these
    photos in current working directory with some filename like “knife1.jpg”. A pre-trained
    model can be utilized to group new photos among the 1,000 known classes. The photo
    needs to be stacked and reshaped to a ![$$224\times 224$$](../images/507793_1_En_13_Chapter/507793_1_En_13_Chapter_TeX_IEq2.png)
    square. Moreover, the pixel esteems are scaled in the route required by the model.
    The model works on different tests. Along these lines the components of a stacked
    picture should be extended by 1 for one picture with ![$$224\times 224$$](../images/507793_1_En_13_Chapter/507793_1_En_13_Chapter_TeX_IEq3.png)
    pixels and three channels.![../images/507793_1_En_13_Chapter/507793_1_En_13_Figar_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Figar_HTML.png)Now
    the model is loaded and ready to make a prediction. This means that it will calculate
    the probability of the image belonging to each of the thousand classes.![../images/507793_1_En_13_Chapter/507793_1_En_13_Figas_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Figas_HTML.png)After
    combining all of this together, a new image is loaded and the prediction is made
    to its most likely class.![../images/507793_1_En_13_Chapter/507793_1_En_13_Figat_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Figat_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Object Tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For tracking the object we have used MIL Tracker that is designed in OpenCV.
    The reason for choosing the MIL tracker is its high accuracy. The script described
    in this section was taken from [[33](#CR33)]. For object tracking using OpenCV,
    create a new .py file and insert the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/507793_1_En_13_Chapter/507793_1_En_13_Figau_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Figau_HTML.png)![../images/507793_1_En_13_Chapter/507793_1_En_13_Figav_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Figav_HTML.png)![../images/507793_1_En_13_Chapter/507793_1_En_13_Figaw_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Figaw_HTML.png)![../images/507793_1_En_13_Chapter/507793_1_En_13_Figax_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Figax_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: 3.4 SMS Alert on Weapon Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.4.1 SMS API
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Twilio’s Programmable SMS API [[34](#CR34)] was used to send the SMS alert on
    weapon detection. Using this REST API, messages can be send and received. It can
    also keep track of the send messages. Moreover, it can recover and change the
    messages history.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 SMS API Authentication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hyper-text Transfer Protocol appeal to the API are ensured with HTTP Basic confirmation.
    Use the following script to do the basic authentication.![../images/507793_1_En_13_Chapter/507793_1_En_13_Figay_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Figay_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Send an SMS with Twilio’s API
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For sending a new message from a Twilio’s phone number to an outside number,
    execute the following script:![../images/507793_1_En_13_Chapter/507793_1_En_13_Figaz_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Figaz_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Evaluations and Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Evaluation Testbed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This system is based on the python model for the object, face feature and their
    response time. Therefore, the only professional way of testing the system is through
    checking the model usability and the response time for the particular model used
    in this system. Further system can capture the video via camera and makes its
    frame too slow to compare the object with the system’s database and gives a comparative
    result with a percentage of accuracy about the object. System can capture multiple
    frames means that multiple object can be detected at the same time as shown in
    Fig. [6](#Fig6), while Fig. [7](#Fig7) is showing the Facial Recognition using
    the machine learning model.![../images/507793_1_En_13_Chapter/507793_1_En_13_Fig6_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Fig6_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 6
  prefs: []
  type: TYPE_NORMAL
- en: Object detection
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/507793_1_En_13_Chapter/507793_1_En_13_Fig7_HTML.png](../images/507793_1_En_13_Chapter/507793_1_En_13_Fig7_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 7
  prefs: []
  type: TYPE_NORMAL
- en: Facial recognition
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Results and Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data Model training was done on about 1700+ pictures for object detection. Results
    were tested and found an accuracy of about 97.3% with the performance rate a bit
    low due to camera resolution and dataset unavailability. For facial recognition,
    training was done for at most 25 images per person with an accuracy rate of about
    90%. The notification test was done using the Twilio API and got the response
    time performance rate of about 5 s for a single message. The accuracy of existing
    work was found to be around 83 to 93% except for the two products i.e. the faceter
    which has an accuracy of 98.33% but limited to face only. Further, they use blockchain
    technology to gather and retrieve data. While the second one is Chinese surveillance
    which has claimed the accuracy of 98%. So far the proposed system has meanwhile
    improved and collectively highly functional by achieving the accuracy level up to
    97.33%.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An Intelligent Surveillance System proposed in this chapter. This system can
    be used to fulfill the concept of Smart Cities where surveillance is one of the
    fundamental building blocks. Yet there are things which can be added to make the
    system more appropriate like this system can be enhanced by adding sensors based
    detection and recognition, weapon integration with camera, enhancing accuracy
    up to maximum level, third-party control panel, and client’s database configuration.
  prefs: []
  type: TYPE_NORMAL
