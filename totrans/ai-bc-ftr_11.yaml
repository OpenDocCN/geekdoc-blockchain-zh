- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2021Y.
    Maleh et al. (eds.)Artificial Intelligence and Blockchain for Future Cybersecurity
    ApplicationsStudies in Big Data90[https://doi.org/10.1007/978-3-030-74575-2_9](https://doi.org/10.1007/978-3-030-74575-2_9)
  prefs: []
  type: TYPE_NORMAL
- en: 'AntiPhishTuner: Multi-level Approaches Focusing on Optimization by Parameters
    Tuning in Phishing URLs Detection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Md. Fahim Muntasir^([1](#Aff7), [2](#Aff8) [ ](#ContactOfAuthor1)), Sheikh Shah Mohammad Motiur Rahman^([1](#Aff7),
    [2](#Aff8) [ ](#ContactOfAuthor2)), Nusrat Jahan^([1](#Aff7) [ ](#ContactOfAuthor3)),
    Abu Bakkar Siddikk^([1](#Aff7), [2](#Aff8) [ ](#ContactOfAuthor4)) and Takia Islam^([1](#Aff7),
    [2](#Aff8) [ ](#ContactOfAuthor5))(1)Department of Software Engineering, Daffodil
    International University, Dhaka, Bangladesh(2)nFuture Research Lab, Dhaka, BangladeshMd. Fahim Muntasir (Corresponding
    author)Email: [fahim35-1900@diu.edu.bd](mailto:fahim35-1900@diu.edu.bd)Sheikh Shah Mohammad Motiur Rahman (Corresponding
    author)Email: [motiur.swe@diu.edu.bd](mailto:motiur.swe@diu.edu.bd)Nusrat JahanEmail:
    [nusrat.swe@diu.edu.bd](mailto:nusrat.swe@diu.edu.bd)Abu Bakkar SiddikkEmail:
    [abu35-1994@diu.edu.bd](mailto:abu35-1994@diu.edu.bd)Takia IslamEmail: [takia35-1014@diu.edu.bd](mailto:takia35-1014@diu.edu.bd)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Phishing is an alarming issue among the cybercriminals. In the last decade,
    online services have revolutionized the world. Due to the revolutionary transformations
    of web service, the reliance on the web has increased day by day. Security threats
    have emerged due to the increasing reliance on online orientation. There are many
    types of anti-phishing solutions available that have been proposed by many researchers.
    However, this chapter is to propose an intelligent framework to detect phishing
    URLs based on the optimized learning architecture scheme. Multi-layer based structures
    have been implemented to detect phishing URLs using Deep Neural Network (DNN),
    Neural Network (NN) and Stacking. These architectures are evaluated with various
    tuning hyper-parameters to obtain the optimized output named AntiPhishTuner. As
    a result, five-layer based DNN can provide accuracy of 0.95 with the minimum mean
    squared error (MSE) 0.30, and also a mean absolute error (MAE) 0.074 where the
    number of epochs was 50 and Adam optimizer as an optimizer. Using two-layer NN
    with AdaGard optimizer can provide accuracy of 0.95, with MSE 0.30 and MAE 0.074\.
    NN provides these results with 150 epochs. Stack generalization can reach maximum
    accuracy 0.97 in binary classification with MAE 2.1\. This chapter can provide
    a better lead to researchers and anti-phishing tools developers to make an initial
    decision about the approach that should be followed for further extension.
  prefs: []
  type: TYPE_NORMAL
- en: KeywordsUniform resource Locator (URL)PhishingDeep Neural Network (DNN)Neural
    Network (NN)Stacking
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Phishing is a fraudulent technique used by both social and technological engineering
    for the purpose of stealing user identities and personal account information and
    credentials from financial accounts Huang [[22](#CR22)]. There are a broad variety
    of phishing forms, including algorithms, link handling, email phishing, domain
    spoofing, phishing using HTTPS, SMS, pop-ups. prefix, suffix, subdomain, IP address,
    URL-length, ‘@’ symbol, spear phishing, dual-slash attributes, port, https token,
    request URL, URL-anchor, tag-links, domain age are phishing attributes Rahman
    [[13](#CR13)].
  prefs: []
  type: TYPE_NORMAL
- en: The elements of a phishing platform are typically equivalent to a few legitimate
    websites literally and externally. Today’s security concerns are increasingly
    rising due to phishing. According to an eminent Washington-based cyber security
    company F5 Systems, Inc., which stamped its target choice, sociology and technological
    infiltration, a phishers strategy combines three special tasks Pompon [[23](#CR23)].
    According to the Anti-Phishing Working organization, there were 18,480 momentous
    phishing attacks and 9666 curiously phishing regions in March2006\. It impacts
    billions of site clients and enormous costing boundaries to businesses Viktorov
    [[24](#CR24)]. The prospective expenditure of computerized offense to the around
    the global network could be a phenomenal 500 billion USD and a clue break will
    fetch the ordinary organization around 3.8 million USD expenditure, considering
    that evidence by Microsoft, in 2018.
  prefs: []
  type: TYPE_NORMAL
- en: There are several proposed solutions that researchers have provided. For example,
    detect phishing websites through a hierarchical clustering approach which bunches
    the vectors produced from DOMs together concurring to their corresponding distance
    Cui [[25](#CR25)]. A few considers centered on detecting phishing URLs by using
    the potential characteristics of URLs. One to two hidden layers are usually used
    for neural networks. In some cases of deep learning, the number of layers varies.
    But it requires nearly more than 150 layers Le [[11](#CR11)]. There are a few
    rules to decide the number of layers that incorporate two or less layers for basic
    data sets and for computer vision, time series, or with intricate datasets extra
    layers can give way better results Rahman [[13](#CR13)]. Mostly classification
    the data patterns are accessible in a structured way. But the URL information
    isn’t accessible in a settled pattern. Applying the classification methods or
    machine learning techniques in URL data. In this way additional approaches ought
    to be utilized for overseeing the URLs Woogue [[26](#CR26)]. Phishing could be
    a pivotal issue in web security. Phishing detection technique Enables URLs recognition
    through Various URLs evaluations. Apropos assess the URLs, a number of procedures
    are accessible. Among the accessible techniques the machine learning techniques
    are more compelling and precise. Such techniques the malicious URLs patterns become
    acquainted by classification algorithm and when requisite. It distinguishes the
    URLs sorts that are phishing or legitimate Dong [[6](#CR6)]. Phish tank database
    is a norm assortment that keeps track of phishing reported URLs by various web
    security organizations. This database stores a variety of features Mohammad [[27](#CR27)].
  prefs: []
  type: TYPE_NORMAL
- en: Phishing is the most known online security threat and it can be called fraudulent
    practice on criminal activities. Which is the main concern of phishing attackers.
    Usually, phishing attackers mimic legitimate websites for credential information
    such as online banking, e-commerce websites so that user’s expose their sensitive
    information such as name, password, login credential, credit card information,
    health-related information etc. refers to mimic sites. Attackers collect user’s
    information and carry out various fraudulent activities by phishing attacks Abutair
    [[1](#CR1)].
  prefs: []
  type: TYPE_NORMAL
- en: URLs play a significant role in phishing attacks, where attackers send malicious
    URLs to users through various communication channels such as emails, social media,
    etc., and sending URLs look like a valid URLs Shirazi [[17](#CR17)].
  prefs: []
  type: TYPE_NORMAL
- en: Typically, three ways are used to take advantage of phishing attacks Hutchinson
    [[9](#CR9)]. First of all, mimic the legitimate web interface which looks exactly
    like a legitimate interface is called web-based phishing. Considering it valid,
    Phishers fool user provides credential information. Secondly, attackers use web-based
    techniques to send phishing content via email. The third one is which phishing
    attack also occurred by malware-based where attackers inject malicious code to
    user’s system Dong [[6](#CR6)]. In any case, why machine learning-based anti-phishing
    framework is used for phishing detection? Because to detect those phishing attacks
    some traditional approaches like Blacklisting, regular expression, and signature
    matching are used, however those approach fail to detect unknown URLs Rahman [[13](#CR13)].
  prefs: []
  type: TYPE_NORMAL
- en: Detecting the unknown pattern of malicious URLs database signatures have always
    remains updated. However, by the expansion of research in the number of machine
    learning-based research for malicious URLs detection, it’s observed that deep
    learning-based architecture provides better performance than existing machine
    learning algorithms Harikrishnan [[10](#CR10)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The principal objectives of this chapter can be stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Assessment of AntiPhishTuner with tuning optimizer for Neural Network as well
    as Deep Learning (Deep Neural Network).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phishing URLs detection has been implemented to improve the accuracy by the
    stacking concept.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining all types of classification can perform phish stack, like machine
    learning, ensemble learning and neural network based approach as base classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expressing intelligent Anti-phishing architectures with optimization tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effect of learning rate in neural network-based technique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appraise of training accuracy with regard to mutate in learning rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting the optimized parameter that are suitable to develop the result for
    DNN and NN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting the combination of adaptive learning optimization algorithm with DNN
    and NN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The remainder of the paper is organized as follows: in Sect. [2](#Sec2), represents,
    Literary Review. In Sect. [3](#Sec3), represents the methodology of phishing URLs
    detection using multilayer approaches and the dataset information which are used
    to experiment and evaluate has been described. Experiments, evaluation parameters
    along with obtained results have been identified and analyzed in Sect. [4](#Sec9).
    Finally, Sect. [5](#Sec17) concludes the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Literature Review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adebowale [[2](#CR2)] proposed an ordinary technique that there are some users
    who steal confidential information from websites and call those users are phishing
    users. This activity commonly happens by fake websites or malicious URLs that
    are called fraudulent ventures. Cybercriminals use fraudulent activities to create
    a well-designed phishing attack. Gaining access to the victim’s systems the cybercriminals
    could install malware or inapt protected user systems.
  prefs: []
  type: TYPE_NORMAL
- en: Acquisti [[4](#CR4)] suggested that reduce the threat of phishing assaults,
    indicating at directing the hazard of phishing attacks, various strategies are
    recommended to get ready and instruct end-users to recognize phishing URLs.
  prefs: []
  type: TYPE_NORMAL
- en: Wang [[20](#CR20)] suggested ensemble classifiers for e-mail filtering that
    excluded five algorithms that are Support Vector Machines, K-Nearest Neighbor,
    Gaussian Naive Bayes, Bernoulli Naive Bayes, and Random Forest Classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately random forest was improved accuracy 94.09% to 98.02%. Gupta, S. and
    Singhal, A. [[8](#CR8)] proposed that approximately for minimum execution time
    random forest tree is an admirable strategy to detect malicious URLs.
  prefs: []
  type: TYPE_NORMAL
- en: Vrbančič [[19](#CR19)] recommended setting parameters of deep learning neural
    networks that are swarm intelligence-based techniques. After that the proposed
    technique applied to the classification of phishing website and capable of better
    detection by comparing to the existing algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: El-Alfy, E. S. M. [[7](#CR7)] recommended for training the nodes framework that
    connected unsupervised and supervised algorithms. Phishing sites depend on feasibility
    neural networks and clustering K medoids. Feature selection and module is used
    to reduce space capacity is used by K-medoid technique. Thirty features are achieved
    96.79% accuracy by the desired technology.
  prefs: []
  type: TYPE_NORMAL
- en: Le [[11](#CR11)], recommended to DNN, are trained with implied deep stacking.
    The evaluated covers of the past outlines are upgraded as they were at the conclusion
    of each DNN preparing epoch, and after that, the upgraded evaluated veils give
    extra inputs to train the DNN within the other epoch. At the test period, the
    DNN makes expectations successively in a repetitive manner. In expansion, we propose
    to utilize the L1 loss for training. Implicit.
  prefs: []
  type: TYPE_NORMAL
- en: Winterrose [[21](#CR21)] claimed that exploring distinctive properties of veritable
    oversees methodologies for recognizing phishing web goals. Phishing URLs utilizing
    significant learning strategies, for the case, profound Boltzmann machine (DBM),
    stacked auto-encoder (SAE), and profound neural organization (DNN). DBM and SAE
    are utilized for pre-preparing the show with a predominant depiction of information
    for attribute assurance. DNN is utilized for twofold gathering in recognizing
    darken URL as either a phishing URL or a genuine URL. The proposed system fulfills
    a higher area rate of 94% with an under most false-positive rate than other machine
    learning procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Rahman [[30](#CR30)] suggested that to detect phishing attack in several anti-phishing
    systems for that reason used six machine learning classifiers (KNN, DT, SVM, RF,
    ERT, and GBT) and three publicly accessible datasets with multidimensional attributes
    could be used due to a lack of proper selection of machine learning classifiers.
    Using confusion matrix, precision, recall, F1-score, accuracy and misclassification
    rate to evaluate the performance of the classifiers. Find better performance that
    obtained from Random Forest and Exceptionally Randomized Tree of 97% and 98% accuracy
    rate for detection of phishing URLs respectively. Gradient Boosting Tree offers
    the best performance with 92% accuracy for multiclass feature set.
  prefs: []
  type: TYPE_NORMAL
- en: Sahingoz [[31](#CR31)] proposed a real-time anti-phishing process that combines
    seven different classification algorithms also with different feature sets. Through
    using NLP-based Random Forest algorithm, 97.98% accuracy was observed.
  prefs: []
  type: TYPE_NORMAL
- en: '3 AntiPhishTuner: Proposed Approach'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The proposed approach has been depicted in Fig. [1](#Fig1) and described in
    details step by step in this section.Table 1
  prefs: []
  type: TYPE_NORMAL
- en: Dataset information
  prefs: []
  type: TYPE_NORMAL
- en: '| Total features of dataset | 30 features |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Total URLs | 11,055 URLs |'
  prefs: []
  type: TYPE_TB
- en: '| Phishing URLs | 4898 |'
  prefs: []
  type: TYPE_TB
- en: '| Legitimate URLs | 6157 |'
  prefs: []
  type: TYPE_TB
- en: 3.1 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A publicly accessible dataset has been used for training or creating the architecture.
    The initial part of this model is to collect data and analyze the datasets. This
    dataset was collected from the UCI repository. It has a total of 11055 different
    types of URLs. It has a total 30 features used to train the model Rahman [[14](#CR14)].
    Table [1](#Tab1) represents the various aspects of the used dataset.Table 2
  prefs: []
  type: TYPE_NORMAL
- en: Address bar-based features
  prefs: []
  type: TYPE_NORMAL
- en: '| Name of the features | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ip Address | In the event that IP address is utilized as an elective of a
    domain name within the URL that is a phishing website and client can almost be
    sure somebody is attempting to take his credential data . From this dataset, discover
    570 URLs having an IP address which add up to 22.8% of the dataset and proposed
    a rule IP address is in URL that called Phishing, otherwise its Legitimate |'
  prefs: []
  type: TYPE_TB
- en: '| Length of URLs | Long URLs are mostly utilized to cover up the dubious portion
    within the address bar because it contains malicious content. Deductively, no
    well-founded length that recognizes phishing URLs from legitimate ones. For that
    legitimate URLs proposed length of the URLs is 75\. In this study to guarantee
    the accuracy measured the length of URLS is suspicious, legitimate or a phishing
    site in this dataset and proposes an average length. From this proposed condition
    the URL length is less than or equal 54 and it is classified as legitimate, if
    the URL is larger than 74 then it is phishing. According to the dataset found
    1220 URLs that’s length greater than or equal 54 |'
  prefs: []
  type: TYPE_TB
- en: '| TinyURLs | For shortening the URL length tinyURL is used. It diverts to the
    most page to click the shorter URL. This interface is like a phishing site since
    rather than an authentic site it diverts the end client to fake sites |'
  prefs: []
  type: TYPE_TB
- en: '| Operate the @ Symbol | Web browsers mostly ignore the segment that is attached
    with @ symbol. Because it is kept away from real addresses. According to the dataset,
    finding 90 URLs that have the ‘@’ symbol will add up to only 3.6% |'
  prefs: []
  type: TYPE_TB
- en: '| Operate the “//” symbol | After HTTP or HTTPS the “//” symbol is used as
    legitimate URLs. On the off chance that after the initial protocol statement that’s
    considered phishing URLs. “//” symbol is utilized for diverting to other sites
    |'
  prefs: []
  type: TYPE_TB
- en: '| Domain names prefix or suffix separated by “-” symbol | If any URL contains
    the “-” symbol in its domain name then consider it’s a phishing URLs. Generally
    validated URLs don’t contain the “-” symbol |'
  prefs: []
  type: TYPE_TB
- en: '| Operate the “.” symbol in domain | Operate the “.” symbol in domain When
    a sub-domain with the domain name is added, it has to include dot. Considering
    suspicious in case drop out more than one subdomain and larger than that will
    point it like a phishing |'
  prefs: []
  type: TYPE_TB
- en: '| HTTPS with secure socket layer | Most of the legitimate site HTTPS protocol
    and the age of certificate is exceptionally vital for using HTTPS. For this that’s
    need a trusted certificate |'
  prefs: []
  type: TYPE_TB
- en: '| Expiry date of domain | Principally domain name have longer expiry date for
    legitimate sites |'
  prefs: []
  type: TYPE_TB
- en: '| Favicon | Favicon can divert clients to suspicious sites, when it is stacked
    from outside space. It’s by and large utilized in websites and it’s a graphic
    image |'
  prefs: []
  type: TYPE_TB
- en: '| Utilizing insignificant ports | Phishers continuously discover defenselessness
    and attempt to require an advantage on the off chance that any URLs has some open
    ports that’s superfluous |'
  prefs: []
  type: TYPE_TB
- en: '| HyperText Transfer Protocol in domain | The phishing websites are considered
    if any URLs of this website have HTTPS on domain name |'
  prefs: []
  type: TYPE_TB
- en: 3.2 Feature Description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before analyzing the features selection part, features and the ability to use
    these features need to be evaluated. Basically, there are four primary features
    and a total of 30 sub-features. Based on the details, each feature offers details
    as to whether the website may be phishing, legitimate or suspicious. This segment
    provides the planning to point up the features.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Address bar-based features:** The address bar that means URL bar or location
    bar could be a GUI gadget that appears in an ongoing URL. According to the dataset
    it has 12 sub-features. That is appeared on the Table [2](#Tab2) below.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Abnormal Based Features:** It for the most part centers on abnormal exercises
    on the site. According to the dataset it has 5 sub-features. That appears on the
    Table [3](#Tab3) below.Table 3'
  prefs: []
  type: TYPE_NORMAL
- en: Abnormal based features
  prefs: []
  type: TYPE_NORMAL
- en: '| Name of the features | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Request URL | From another domain on the off chance that a page contains
    larger amount of outside URLS that’s considered it suspicious or phishing |'
  prefs: []
  type: TYPE_TB
- en: '| Having URL of anchor | Comparable to the request URL features, the chance
    of phishing increases, more<a> tags utilized inside the site |'
  prefs: []
  type: TYPE_TB
- en: '| Link among (Meta, script, Link) tag | It is calculated as either suspicious
    or phishing formed on their proportion if the tag contains large number of outer
    links |'
  prefs: []
  type: TYPE_TB
- en: '| Server form handler | Phishing is considered in case the Server shape handler
    is blank or empty. Server frame handler diverts to a distinctive domain It’s checked
    as suspicious |'
  prefs: []
  type: TYPE_TB
- en: '| Having an email to submitting information | It is considered as phishing,
    rather than a server, web form coordinated to an individual email is submitted
    the information |'
  prefs: []
  type: TYPE_TB
- en: '| Abnormal URLs | It considered as phishing, In case the character isn’t included
    within the URLs |'
  prefs: []
  type: TYPE_TB
- en: '**3\. HTML and JavaScript based features:** According to the dataset it has
    5 sub-features. That appears on Table [4](#Tab4) below.Table 4'
  prefs: []
  type: TYPE_NORMAL
- en: HTML and JavaScript based features
  prefs: []
  type: TYPE_NORMAL
- en: '| Name of the features | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Forwarding website | It can be frightening, on the off chance that diverting
    is happened different times |'
  prefs: []
  type: TYPE_TB
- en: '| Customization of status bar | To alter the status bar of the URLs can be
    utilized on “Mouseover” occasion. It continuously appears off genuine URLs and
    stows away the fake URLs. at a time When it’s connected on the site that’s obliging
    as phishing |'
  prefs: []
  type: TYPE_TB
- en: '| Right click disabled | Users can’t check the source code; right-click functions
    are impaired mainly by Phishers. When the framework is debilitated within the
    site that’s obliging as phishing |'
  prefs: []
  type: TYPE_TB
- en: '| Having Pop-up Window | Pop-up window with a text field is consisted by a
    web page that’s obliging as phishing |'
  prefs: []
  type: TYPE_TB
- en: '| Custom IFrame | Stowing them away within the website phisher could be utilized
    IFrame. In for the most part Connect outside substance to appear in a domain utilized
    by IFrame |'
  prefs: []
  type: TYPE_TB
- en: '**4\. Domain based features:** Using domain names prepares effortlessly identifiable
    and unforgettable names numerically. According to the dataset it has 7 sub-features.
    That appears on Table [5](#Tab5) below.Table 5'
  prefs: []
  type: TYPE_NORMAL
- en: Domain based features
  prefs: []
  type: TYPE_NORMAL
- en: '| Name of the features | Explanation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Age of domain | Obliging a authentic site as phishing site tend to live for
    shorter period of time in the event that the age of domain is longer than six
    month |'
  prefs: []
  type: TYPE_TB
- en: '| Record of DNS | It is exceedingly recommended as phishing site within the
    event DNS record isn’t contained by website |'
  prefs: []
  type: TYPE_TB
- en: '| Traffic of website | Colossal amount of individuals visit websites for the
    most part because it would have higher positioning. Positioning can distinguish
    on the off chance that a location is phishing or not. A phishing site is being
    tends to have a lower chance by the next ranked site |'
  prefs: []
  type: TYPE_TB
- en: '| Ranking of page | In most time that phishing websites have no PageRank value
    since this value is allotted on its importance |'
  prefs: []
  type: TYPE_TB
- en: '| Indexing of Google | A legitimate site can be accepted by a site that has
    a title on the google index |'
  prefs: []
  type: TYPE_TB
- en: '| Reports statistical | Guessing it as phishing webpage within the event the
    have of the webpage has a place in any beat phishing IP’s or domains |'
  prefs: []
  type: TYPE_TB
- en: '| Joins indicating to page | Phishing site prohibiting have much links indicating
    apropos it since it has shorter lifetime |'
  prefs: []
  type: TYPE_TB
- en: 3.3 Deep Learning Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This study has considered five adaptive optimizer such as Stochastic Gradient
    Descent (SGD), ADAM, ADADELTA, ADAGARD, and RMSPROP used for evaluation of NN
    and DNN.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Machine Leraning Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This study has considered some machine learning algorithms for stacking such
    as Support Vector Machine (SVM), Decision Tree (DT), Naive Bayes (GNB), Linear
    Discriminant Analysis (LDA), Random Forest (RF), Multilayer Perceptron (MLP),
    Stochastic Gradient Descent (SGD), Logistic Regression (LR), k nearest neighbors
    (KNN) and Gaussian are used for Stacked Generalization as a base classifiers in
    first step, and 10-fold cross validation Adebowale [[3](#CR3)] has been used.
    Here, XGBoost classifier is being used as meta estimator for final prediction
    in second step.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Model Generation Phase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The above methodology indicates three types of multilayer approaches: NN, DNN
    and stack generalization respectively. The main purpose of this model is to determine
    the best output through evaluation by applying stacking technique and neural network
    and deep neural network on the processed data set and to propose an optimized
    model based on that output.'
  prefs: []
  type: TYPE_NORMAL
- en: Now an optimized output will be provided by applying neural network and deep
    neural network technique on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: After loading the features from the dataset, the data set is split into two
    parts, test and train. The train segment is applied to a two-layer neural network
    architecture and Somesha [[16](#CR16)] a five-layer deep neural network architecture,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Since the data set is of binary type, for binary classification problem non-linear
    activation function ReLU have been used for hidden layers of neurons and sigmoid
    function have been used for output layers of neurons Vrbančič [[19](#CR19)]. According
    to this architecture, five types of adaptive optimizer have been used here.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to compile the model using these adaptive optimizers. It is
    then divided into two parts, train and validation, by splitting the train set.
    The model is fitted using a number of epochs and early stopping techniques, to
    prevent overfitting. Now two outputs are available by evaluating the two models
    using the test set. After applying the approach, stack generalization technique
    has been applied in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation technique of stack generalization has been described in figure.
    It’s a multilevel approach. Stacking is usually done in two steps. In the first
    level stacking provides transitory prediction using based on classifiers with
    k-fold cross validation and output probability prediction are revealed. During
    the system formation the output prediction and transitory prediction of step one
    are used in second steps. The estimate theory of phish stack are described below
    Rahman [[14](#CR14)]:![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig1_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig1_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 1
  prefs: []
  type: TYPE_NORMAL
- en: Methodology for phishing URLs detection
  prefs: []
  type: TYPE_NORMAL
- en: In the first step of stacking by using base classifiers to predict train and
    test set according to the second step the desired predictions are being acquired
    then that are considered as features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking is a multilevel approach so any kind of algorithm can be used to predict
    it in two steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This proposed system used k-fold cross-validation so that it eluded overfitting
    for this training set and each fold of the train portion it may predict using
    out-of-fold. According to this proposed model the value three to ten is used for
    k-fold cross validation after all provides output using a test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first step at the end of training the data the output is predicted using
    the test set. This time it’s complete with all folds technique that’s needed to
    mean for estimating all values from all folds that are used.
  prefs: []
  type: TYPE_NORMAL
- en: In the second step connected to another classifier that’s called a meta-estimator
    on the train set, from the test set it performs terminal prediction. This approach
    takes extra time because it again adds a classifier for its performances. When
    the k-fold cross validation done in the first step then prediction is not completed
    these are completed on the second step.
  prefs: []
  type: TYPE_NORMAL
- en: Three outputs are obtained from the above multilayer techniques then a model
    is selected based on the decision, according to the value of the output. An optimized
    architecture is proposed based on that model.Table 6
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation parameters
  prefs: []
  type: TYPE_NORMAL
- en: '| Assessment parameter | Assessment parameters formula | Statement of the assessment
    parameter |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Mean Absolute Error (MAE) | ![$${\text {MAE = }}\frac{{\sum \nolimits _{{i
    = 1}}^{n} {\left&#124; {y_{i} - x_{i} } \right&#124; } }}{n}$$](../images/507793_1_En_9_Chapter/507793_1_En_9_Chapter_TeX_IEq1.png)
    | It is the average value of all absolute errors [[5](#CR5)] |'
  prefs: []
  type: TYPE_TB
- en: '| Mean Square Error (MSE) | ![$${\text {MSE = }}\frac{1}{n}\sum \limits _{{i
    = 1}}^{n} {\left( {Y_{i} - \hat{{Y_{i} }}} \right) ^{2} }$$](../images/507793_1_En_9_Chapter/507793_1_En_9_Chapter_TeX_IEq2.png)
    | It is the average value of all squares errors |'
  prefs: []
  type: TYPE_TB
- en: '| AUC-ROC curve | For Positive Recall TRP = TP/(TP + FN). For Negative Recall
    FPR = 1 − Specificity = 1 − TN/(TN+FP) = FP/TN+FP | AUC - ROC curve is intrigued
    with True Positive Rate that belongs on y-axis, in opposition to the False Positive
    Rate that belongs on x-axis [[1](#CR1)] |'
  prefs: []
  type: TYPE_TB
- en: '| Precision - Recall Curve | For Positive Precision P = TP/(TP + FP) For Negative
    Precision N = TN/(TN+FN) For Positive Recall PR = TP/(TP + FN) For Negative Recall
    NR = TN/(TN+FP) | According to the precision-recall curve for a single classifier,
    estimating and intrigued the precision in opposition to the recall [[12](#CR12)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | Accuracy = (TP + TN)/(TP + TN + FP + FN) | Accuracy means the
    rate of prediction that model executes [[15](#CR15)] |'
  prefs: []
  type: TYPE_TB
- en: '| Misclassification rate | Error Rate = 1 − Accuracy | The failings of identify
    value that is not appropriate for classification |'
  prefs: []
  type: TYPE_TB
- en: 4 Result and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Environment Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The experiment that has been conducted is Intel(R) Core(TM) i3-7100 CPU @2.40 GHz
    processor, 64-bit PC with 4 GB RAM. The operating system is Windows 10 pro Education
    and python has been used to implement the architecture To Detect Phishing URLs
    with the packages of python such that TensorFlow, scikit-learn, Keras, Pandas,
    and NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Evaluation Parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The system was mainly focused on evaluation based on data phishing or legitimate
    that’s identified by binary classification. Confusion matrix, Accuracy, Precision-Recall
    Curve, Classification report, AUC-ROC Curve, Mean Absolute Error (MAE), Mean square
    Error (MSE) used to evaluate the performance of this system. The evaluation parameters
    [[14](#CR14)] for assessment are described in the Table [6](#Tab6):'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Experiment Result
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The representation of the various optimizers of DNN and NN was shown in Tables
    [7](#Tab7) and [8](#Tab8). Five separate adaptive optimizers have been used for
    this experiment, the number of hidden layers, the learning rate and the epoch
    size are considered HL, LR and EPS respectively. According to this condition,
    HL5 means the number of hidden layers is 5 and the number of hidden layer 2 is
    HL2\. For this evaluation, 15 types of learning rate and 10 types of epoch size
    were used for 20 times iteration for these five optimizer’s. After 20-fold iteration,
    have chosen a better combination of epoch size and learning rate to achieve optimized
    performance so that this model is more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: '4.3.1 Case Study #1'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Evaluation rate of five Adaptive Optimizer with accuracy and loss for DNN.![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig2_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig2_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 2
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss DNN five optimizer
  prefs: []
  type: TYPE_NORMAL
- en: As illustrate in Fig. [2](#Fig2) have shown that used different deep learning
    adaptive optimizer to take the decision which optimizer would be the best for
    anti-phishing proposed model. In this case Adam optimizer given the highest accuracy
    among all the optimizer where SGD optimizer given slightly low performance. On
    the contrary, model optimizer loss their performance while tuning the model for
    the prediction of proposed model with the selected optimizer. Where every optimizer
    loss their performance based on their adaptive quality. In this case being understand
    to take the optimizer based on their performance and loss accuracy for the shake
    which optimizer will be the best fit for anti-phishing proposed model.![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig3_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig3_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3
  prefs: []
  type: TYPE_NORMAL
- en: Different optimizer for ROC curve and Precision-Recall curve for DNN
  prefs: []
  type: TYPE_NORMAL
- en: The ROC curve and precision-Recall curve the have been shown in Fig. [3](#Fig3).
    Maximum accuracy 0.955 attained from Adam individually. In case of precision-recall
    curve and the AUC-ROC curve SGD and AdaGard do better provides 0.96\. SGD and
    AdaGard perform better in ROC curve and precision-Recall curve than others.Table
    7
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation table for DNN
  prefs: []
  type: TYPE_NORMAL
- en: '| Serial | Optimizer | Label | Learning rate | Epochs | Accuracy | MSE | MAE
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Adam | HL5 | 0.01 | 50 | 0.955 | 0.030 | 0.074 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | SGD | HL5 | 0.001 | 100 | 0.951 | 0.021 | 0.049 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | RMSprop | HL5 | 0.0003 | 150 | 0.953 | 0.028 | 0.076 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | AdaDelta | HL5 | 0.0027570 | 250 | 0.954 | 0.023 | 0.078 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | AdaGard | HL5 | 0.0017470 | 150 | 0.953 | 0.018 | 0.049 |'
  prefs: []
  type: TYPE_TB
- en: The analysis shows clearly in Table [7](#Tab7) that the learning rate has an
    essential contribution to the success of profound neural systems among all the
    measurement or appraisal parameters. It was found that the maximum accuracy of
    0.955, MSE 0.030 and MAE 0.074 of the hidden five layers using Adam optimizer,
    along with the 50 epochs and 0.01 learning rate (HL5 EPs50). Observing all the
    outcomes from Table [7](#Tab7) from above, it can be observe that all the optimizer
    provides 95% accuracy of which Adam pays a little more, Adam is the top scorer
    Vrbančič [[19](#CR19)].
  prefs: []
  type: TYPE_NORMAL
- en: '4.3.2 Case Study #2'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Evaluation rate of five Adaptive Optimizer with accuracy and loss for NN![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig4_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig4_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss NN five optimizer
  prefs: []
  type: TYPE_NORMAL
- en: As discussed before in Fig. [2](#Fig2) similarly in this phase according to
    the illustration in Fig. [4](#Fig4) used different deep learning adaptive optimizer
    where AdaGard optimizer given the highest accuracy among all the optimizer for
    two layer NN where both Adam and RMSprop optimizer given slightly low performance
    for the NN two layer. According to their performance, model have been loss their
    performance while evaluated the model to find the best optimizer if two NN layer
    used for all the adaptive optimizer.![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig5_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig5_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 5
  prefs: []
  type: TYPE_NORMAL
- en: Different optimizer ROC curve and Precision-Recall curve for NN
  prefs: []
  type: TYPE_NORMAL
- en: The ROC curve and precision-Recall curve the have been shown in Fig. [5](#Fig5).
    Maximum accuracy 0.955 attained from AdaGard individually. In case of precision-recall
    curve and the AUC-ROC curve Adam, SGD, AdaDelta and AdaGard do better provides
    0.95, expect RMSprop.Table 8
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation table for NN
  prefs: []
  type: TYPE_NORMAL
- en: '| Serial | Optimizer | Label | Learning rate | Epochs | Accuracy | MSE | MAE
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Adam | HL2 | 0.0017470 | 150 | 0.948 | 0.014 | 0.058 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | SGD | HL2 | 0.001 | 128 | 0.945 | 0.026 | 0.086 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | RMSprop | HL2 | 0.0003 | 200 | 0.948 | 0.026 | 0.080 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | AdaDelta | HL2 | 0.0027570 | 250 | 0.949 | 0.016 | 0.067 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | AdaGard | HL2 | 0.0017470 | 150 | 0.955 | 0.030 | 0.074 |'
  prefs: []
  type: TYPE_TB
- en: From the experiment it’s clearly shown in Table [8](#Tab8) that the learning
    rate has an essential contribution to the success of profound neural systems among
    all the measurement or appraisal parameters. It was found that the maximum accuracy
    of 0.955, MSE 0.030 and MAE 0.074 of the hidden five layers using AdaGard optimizer,
    along with the 150 epochs and 0.0017470 learning rate (HL2, EPs150) which is slightly
    near with the DNN.
  prefs: []
  type: TYPE_NORMAL
- en: '4.3.3 Case Study #3'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main purpose of stacked generalization is used a higher grade model to combine
    low grade models to achieve higher predictive accuracy. Stacking combines multiple
    model and learns it up for classification task.Table 9
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy of machine learning classifier algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '| LR | LDA | KNN | DT | GNB | SVM |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.927 | 0.921 | 0.936 | 0.955 | 0.593 | 0.944 |'
  prefs: []
  type: TYPE_TB
- en: According to Table [9](#Tab9), first of all here 6 machine learning algorithms
    are used on the data of the desired dataset then some accuracy is found on the
    basis of that algorithm. These algorithm are used to build a stack model.Table
    10
  prefs: []
  type: TYPE_NORMAL
- en: Build model stack and the increased accuracy of Machine learning Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '| LR | LDA | KNN | DT | GNB | SVM |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.966 | 0.965 | 0.965 | 0.966 | 0.965 | 0.966 |'
  prefs: []
  type: TYPE_TB
- en: In this step a stack model is generated by applying these algorithm. According
    to Table [10](#Tab10) this algorithm have changed in their accuracy after generating
    a stack model. The stack stipulates that it combines multiple models and learns
    for classification task. So purpose of this step is to stack learn stack.Table
    11
  prefs: []
  type: TYPE_NORMAL
- en: Misclassification rate and accuracy of temporary prediction
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Accuracy | Misclassifation rate |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RF | 0.96 | 0.0047 |'
  prefs: []
  type: TYPE_TB
- en: '| DT | 0.95 | 0.0063 |'
  prefs: []
  type: TYPE_TB
- en: '| MLP | 0.96 | 0.0022 |'
  prefs: []
  type: TYPE_TB
- en: '| SVM | 0.94 | 0.0072 |'
  prefs: []
  type: TYPE_TB
- en: '| SGD | 0.91 | 0.0073 |'
  prefs: []
  type: TYPE_TB
- en: '| GNB | 0.59 | 0.012 |'
  prefs: []
  type: TYPE_TB
- en: The stack has already been learned, now it knows how to process a model. Table
    [11](#Tab11) represent the final accuracy and misclassification rate for first
    step. This work is done by two steps. So this table’s value indicates the first
    step’s prediction or temporary prediction because after second step prediction
    will find final prediction. The next step is to build a model, according to the
    study a model has been created XGBClassifier and through that model fitted the
    previous trained data and predict the final results.![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig6_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig6_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 6
  prefs: []
  type: TYPE_NORMAL
- en: Different algorithms ROC curve and Precision-Recall curve
  prefs: []
  type: TYPE_NORMAL
- en: The precision-Recall curve and the ROC curve have been shown in Fig. [6](#Fig6).
    The first step shows that the maximum accuracy 0.96 with minimum error rate. RF
    and MLP do better individually where precision-recall curve and the AUC-ROC curve,
    stacked generalization performs low. However in the time of final prediction stack
    generalization provides accuracy 0.97.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Discussion on the Difference Among the Three Multilayer Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Stack Generalization**Table 12'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation rate of stacking
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Accuracy | Accuracy of stack generalization |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LR | 0.927 | 0.966 |'
  prefs: []
  type: TYPE_TB
- en: '| LDA | 0.921 | 0.965 |'
  prefs: []
  type: TYPE_TB
- en: '| KNN | 0.936 | 0.965 |'
  prefs: []
  type: TYPE_TB
- en: '| DT | 0.955 | 0.966 |'
  prefs: []
  type: TYPE_TB
- en: '| GNB | 0.953 | 0.965 |'
  prefs: []
  type: TYPE_TB
- en: '| SVM | 0.944 | 0.966 |'
  prefs: []
  type: TYPE_TB
- en: In this study binary classification type dataset has been selected for evaluation
    of three multilayer approaches. It is a well-known fact that machine learning
    algorithms provide good results for binary classification type datasets. The main
    features of stack generalization is that it integrates with low grade models using
    high grade models and also known as ensemble algorithm that basically works in
    two layers. According to the stacking concept, it learns multiple machine learning
    algorithms in the first layer, and then gives the predictions as an output which
    is used as the learning of another algorithm in the second layer. The machine
    learning algorithm used as the final predictor, this learning is more error-free.
    The main target of stack generalization is to develop the result of low grade
    models Li [[28](#CR28)] New model is trained by other models that are already
    trained from a dataset. Most commonly stacking uses simple linear function (mean,
    median, average etc.) to assemble the prediction for other models. According to
    the stacking techniqe for binary classification type data sets will provide more
    accuracy than NN and DNN. So optimized output can be obtained using this stacking
    concept (Table [12](#Tab12)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Neural Network**Table 13'
  prefs: []
  type: TYPE_NORMAL
- en: Highest rate for NN
  prefs: []
  type: TYPE_NORMAL
- en: '| Optimizer | Label | Learning rate | Epochs | Accuracy | MSE | MAE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AdaGard | HL2 | 0.0017470 | 150 | 0.955 | 0.030 | 0.074 |'
  prefs: []
  type: TYPE_TB
- en: According to the determination rule, the number of hidden layer two is used
    for arbitrary decisions with rational activation functions Rahman [[13](#CR13)].
    Therefore two hidden layer neural network is the best approach for a given data
    dataset. The first layer is called the input layer according to the structure
    of the neuron. Previous layer outcome obtained to be the weighted input to the
    following layer, there is no correlation among each layer but NN shows craved
    conduct is made finding the correct weight by knowing NN Fister [[29](#CR29)].
    The structure of the neural network is similar to tree structure. There are units
    in each layer of the neural network. These units indicate how deep these layers
    can go. The value of unit basically indicates how depth the data will go and how
    many combinations will be tree based. A complete accurate outcome is obtained
    from multiple averages of a value. Stacking provides more accuracy than a neural
    network for a given data set, therefore neural networks provide more optimized
    results than stacking. Stacking is done in two layers whereas neural networks
    provide optimized output from many tree base combinations. Stacking is done in
    two layers so in case of complex data it will performed low whereas each hidden
    layer of neural network have units which indicates how deep it will go in tree
    base structures, so that it will provide an optimized output from multiple combinations
    (Table [13](#Tab13)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep Neural Network**Table 14'
  prefs: []
  type: TYPE_NORMAL
- en: Highest rate for DNN
  prefs: []
  type: TYPE_NORMAL
- en: '| Optimizer | Label | Learning rate | Epochs | Accuracy | MSE | MAE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Adam | HL5 | 0.01 | 50 | 0.955 | 0.030 | 0.074 |'
  prefs: []
  type: TYPE_TB
- en: This study ultimately dictates three multi-layer NN, DNN, and stacking strategies.
    Evaluating their output reveals that their outcomes among these stacks are virtually
    the same. Here two layers are used for NN, 5 for DNN, and two steps have been
    used for stack generalization. Stacking technique and NN have decent results for
    a basic dataset, such that the output of the dataset is lowered whether it has
    complicated or complex values. According to DNN, it works with a large number
    of layers and uses the value of the unit as needed. The DNN model-based architecture
    from this study offers good results for every form of dataset much of the time
    (Table [14](#Tab14)).
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Though phishing is a sensational phenomenon in today’s cyber space, it is a
    matter of concern to investigate for securing the future. In this study, anti-phishing
    techniques have been developed based on NN, DNN and stacking concept. Parameter
    adjustment plays a vital role for these techniques. Among those parameters, learning
    rate is one of them. This is an unimaginable footstep of increasing the performance
    of NN and DNN based on systems. Here is an assessment of the effect of parameters
    that will be an evidence in the development of NN and DNN based system. The amount
    of data in the data set affect the system learning base. In the case of stacking,
    Random Forest and Multilayer perception provides better results for precision
    and recall. However stack generalization helps better to enhance the overall accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, this chapter indicates three multilayer techniques that are NN, DNN,
    stacking along with the parameter tuning for neural network based architectures.
    Evaluating their performance shows that the results they provide almost same outcome.
    Apart from those, stacking provides better accuracy with less complex dataset.
    Here 2 layers are used for NN, 5 layers for DNN and stack generalization has used
    two layer. DNN and NN layers have units which indicate how deep these layers can
    go. Fundamental difference between NN and DNN is that NN works with two layers
    on the behalf of DNN works with more than two layers.
  prefs: []
  type: TYPE_NORMAL
- en: In recapitulate, the final outcome is obtained from the averages of multiple
    outputs. Stacking technique and NN provide better results for the dataset with
    simplicity, if the dataset holds complex or more complicated values then performance
    is getting decreased. According to DNN, it works with a large number of layers
    and uses the value of the unit as needed. From this study, DNN model based architecture
    provides better results on an average for any type of dataset.
  prefs: []
  type: TYPE_NORMAL
