- en: © Springer Nature Switzerland AG 2020A. M. LangerAnalysis and Design of Next-Generation
    Software Architectures[https://doi.org/10.1007/978-3-030-36899-9_10](https://doi.org/10.1007/978-3-030-36899-9_10)
  prefs: []
  type: TYPE_NORMAL
- en: 10. Transforming Legacy Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Arthur M. Langer^([1](#Aff2) [ ](#ContactOfAuthor2))(1)Center for Technology
    Management, Columbia University, New York, NY, USAArthur M. LangerEmail: [al261@columbia.edu](mailto:al261@columbia.edu)'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A Legacy system is an existing application system in operation. While this
    is an accurate definition, there is a perception that legacy systems are old and
    antiquated applications operating on mainframe computers. Indeed, Brodie and Stonebraker
    ([1995](#CR1)) state that, “a legacy information system is any information system
    that significantly resists modification and evolution” (p. 3). They define typical
    legacy systems as:'
  prefs: []
  type: TYPE_NORMAL
- en: Large application with millions of lines of code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually more than 10 years old.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Written in legacy languages like COBOL .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are built around a legacy database service (such as IBM’s IMS) and some
    do not use a database management system. Instead they use older flat-file systems
    such as ISAM and VSAM .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The applications are very autonomous. Legacy applications tend to operate independently
    from other applications, which means that there is very limited interface among
    application programs. When interfaces among applications are available, they are
    usually based on export and import models of data and these interfaces lack data
    consistency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While many legacy systems do fit these scenarios, many do not. Those that do
    not can; however, be considered legacies under my original definition, that is,
    any application in operation. What this simply means is that there are what I
    call “generations” of legacy systems that can exist in any organization. Thus,
    the definitions of what constitutes a legacy system are much broader than Brodie
    and Stonebraker’s descriptions. The more important issue to address is the relationship
    of legacy systems with packaged software systems especially with the explosion
    of independent APIs from the evolving IoT devices, blockchain products and Cloud
    computing. Packaged software systems that are typically supported by third-party
    vendors encompass both internal and external applications. Thus, existing internal
    production systems, including third-party outsourced products, must be part of
    any application strategy. Furthermore, there are many legacy systems that perform
    external functions as well, albeit not in directly in an internet interface.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter defines the type of legacy systems that exist and provides guidelines
    on how to approach their integration with packaged software applications and transformation
    in into the new architectures that support IoT. The project manager or analyst
    must determine whether a legacy system should be replaced, enhanced, or remain
    as is. This chapter also provides the procedures for dealing with each of these
    three choices and its effect on the overall architecture of the decision whether
    to make or buy a system. However, overall this chapter suggests that all traditional
    legacy at some point need to redeveloped to support mobility and maximized cyber
    protection.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Types of Legacy Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The types of legacy systems tend to mirror the life cycle of software development.
    Software development is usually defined within a framework called “generations.”
    Most professionals agree that there are five generations of programming languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*First*-*Generation*: the first generation was known as machine language. Machine
    language is considered a low-level language because there it uses binary symbols
    to communicate instructions to the hardware. These binary symbols form a one-to-one
    relationship between a machine language command and a machine activity, that is,
    one machine language command performs one machine instruction. It is rare that
    any legacy systems have first-generation software.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Second*-*Generation* : this generation was comprised of assembler programming
    languages. Assembler languages are proprietary software that translates a higher-level
    coding scheme into more than one machine language instruction. Therefore, it was
    necessary to design an assembler, which would translate the symbolic codes into
    machine instructions. Mainframe shops still may have a considerable amount of
    assembly code that exists, particularly with applications that perform intricate
    algorithms and calculations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Third*-*Generation* : these languages continued the growth of high-level symbolic
    languages that had translators into machine code. Examples of third-generation
    languages are COBOL , FORTRAN, BASIC, RPG, and C. These languages use more English-like
    commands and have a higher ratio of machine language produced from one instruction.
    Third-generation languages tend to be more specialized. For example, FORTRAN is
    better suited for mathematical and scientific calculations. Therefore, many insurance
    companies have FORTRAN because of the high concentration on actuarial mathematic
    calculations. COBOL , on the other hand, was designed as the business language
    and has special features to allow it to manipulate file and database information.
    There are more COBOL applications that exist than any other programming language.
    Most mainframe legacy systems still have COBOL applications. RPG is yet another
    specialized language that were designed for use on IBM’s mid-range machines. These
    machines include the System 36, System 38, and AS/400 computers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Fourth*-*Generation* (*4GL*): these programming languages are less procedural
    than third-generation languages. In addition, the symbols are more English-like
    and emphasize more about desired output results than how the programming statements
    need to be written. As a result of this feature, many less technical programmers
    have learned how to program using 4GLs. The most powerful features of 4GLs include
    query of databases , code-generation, and graphic screen generation abilities.
    Such languages include Visual Basic, C ++, Visual Basic, Powerbuilder, Delphi,
    and many others. Furthermore, 4GL languages also include what is known as *query
    languages* because they contain English-like questions that are used to directly
    produce results by directly accessing relational databases . The most popular
    4GL query language is Structured Query Language (SQL ).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Fifth*-*Generation*: these programming languages combine what is known as
    rule-based code generation, component management, and visual programming techniques.
    The rule-based code generation became popular in the late 1980s with the creation
    of artificial intelligence software. This software uses an approach called knowledge-based
    programming, which means that the developer does not tell the computer how to
    solve problems, but rather the problem (Stair and Reynolds [1999](#CR2)). The
    program figures out how to solve the problem. While knowledge-based programming
    has become popular in specialized applications, such as in the medical industry,
    it has not been as popular in business.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Most legacy applications will either be third- or fourth-generation language
    systems, therefore, analysts need to have a process and methodology to determine
    how to transform and re-architect these applications.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Third-Generation Language Legacy System Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As previously discussed, most third-generation language legacy systems were
    developed using COBOL . COBOL was developed to provide a method of forcing programmers
    to self-document their code so that other programmers could maintain it. Unfortunately,
    COBOL requires what is known as a File Description Table (FD). The FD defines
    the record layout for every file used by the COBOL program. In other words, every
    file is described within the program and must match the format of the actual physical
    data file. This means that any change to a file structure must be synchronized
    with every COBOL program that uses that data file. Thus, COBOL is somewhat eclectic:
    there is no real separation of the data description and the program logic. In
    COBOL programs then, a change in data format could necessitate a change in the
    program code. That is why COBOL programs suffer from large degrees of coupling
    of code. Coupling is defined as the reliance of one piece of the code on another.'
  prefs: []
  type: TYPE_NORMAL
- en: COBOL programs may or may not use a relational database as its source of data.
    I earlier defined two other common formats called ISAM and VSAM , which are flat-file
    formats, meaning that all data elements are contained in one record as opposed
    to multiple files as is the case in the relational database model. However, many
    COBOL legacy systems have been converted to work with relational databases such
    as IBM’s DB2\. In this situation the FD tables interface with the database’s file
    manager so that the two entities can communicate with each other. Figure [10.1](#Fig1)
    depicts the interface between program and database .![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig1_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig1_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.1
  prefs: []
  type: TYPE_NORMAL
- en: COBOL file description interface with database manager
  prefs: []
  type: TYPE_NORMAL
- en: Notwithstanding whether the COBOL legacy is using a database interface or flat-files
    the analyst needs to determine whether to replace the application, enhance it,
    or leave it as is.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Replacing Third-Generation Legacy Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When replacing third-generation legacies, analysts must focus on both the data
    and processes . Because of the age of these systems, it is likely that there will
    be little documentation available, and the amount available will most likely be
    outdated. Indeed, the lack of proper documentation is the major reason why legacy
    systems are slow to be replaced: rewriting code without documentation can be an
    overwhelming and time-consuming task. Unfortunately, all things must eventually
    be replaced. Delaying replacement leads to legacy systems that keep businesses
    from remaining competitive. The following sections provide a step-by-step approach
    to COBOL -based legacies.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Approaches to Logic Reconstruction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The best way to reconstruct the logic in COBOL applications is to separate
    the data from the processes . This can be accomplished by creating data flow diagrams
    (DFD) for each program. Having a DFD will result in defining all of the inputs
    and outputs of the application. This is accomplished by following the tasks below:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.Print the source code (actual COBOL written code) from each application. Each
    application will contain a “FD” section that defines all of the inputs and outputs
    of the program. These will represent the data stores of the data flow diagrams
    (Fig. [10.2](#Fig2)).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig2_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig2_HTML.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fig. 10.2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: COBOL file description tables
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DFDs should be decomposed so that they are at the functional primitive level
    (one-in and one-out, preferred). This provides functional decomposition for the
    old application and sets the framing for how it will be decomposed into an object-oriented
    solution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By reviewing the code, write the process specifications for each functional
    primitive .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the steps as outlined in Chap. [3](480347_1_En_3_Chapter.xhtml) to determine
    which functional primitive DFDs become methods of a particular class .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Capture all of the data elements or attributes required by each functional primitive
    DFD. These attributes are added to the data dictionary (DD).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take each major data store and create an entity . Do normalization and Logic
    Data Modeling in accordance with the procedures in Chap. [4](480347_1_En_4_Chapter.xhtml),
    combining these elements with the packaged software system as appropriate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data stores that represent reports should be compared against sample outputs.
    These reports will need to be redeveloped using a report-writer such as Crystal’s
    report writer or a data warehouse product.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examine all existing data files and/or databases in the legacy system . Compare
    these elements against those discovered during the logic reconstruction. In third-generation
    products there will be many data elements or fields that are redundant or used
    as logic “flags.” Logic flags consists of fields used to store a value that depicts
    a certain state of the data. For example, suppose a record has been updated by
    a particular program. One method of knowing that this occurred is to have the
    application program set a certain field with a code that identifies that it has
    been updated. This method would not be necessary in a relational database product
    because file managers automatically keep logs on the last time a record has been
    updated. This example illustrates how different third-generation legacy technology
    differs from more contemporary technologies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There is no question that replacing third-generation legacies is time-consuming.
    However, the procedures outlined above will prove to be accurate and effective.
    In many situations, users will decide that it makes sense to re-examine their
    legacy processes , especially when the decision has been made to rewrite applications
    for integration with IoT and blockchain systems. We call this business process
    reengineering . Business process reengineering is therefore synonymous with enhancing
    the legacy system .
  prefs: []
  type: TYPE_NORMAL
- en: 10.6 Enhancing Third-Generation Legacy Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Business process reengineering (BPR) is one of the more popular methodologies
    used to enhance third-generation applications. A more formal definition of BPR
    is “a requirement to study fundamental business processes , independent of organization
    units and information systems support, to determine if the underlying business
    processes can be significantly streamlined and improved.”^([1](#Fn1)) BPR is not
    just rebuilding the existing applications for the sake of applying new technology
    to older systems, but also an event that allows for the application of new procedures
    designed around the Object-Oriented systems paradigm. In this scenario, however,
    BPR is used to enhance existing applications without rewriting them in another
    generation language. Instead, the analyst needs to make changes to the system
    that will make it function more like an object component even though it is written
    in a third-generation language. In order to accomplish this task, it is necessary
    for the analyst to create the essential components of the legacy operation. Essential
    components represent the core business requirements of a unit. Another way of
    defining core business requirements is to view essential components as the reasons
    why the unit exists—what does it do—and for what reasons. For example, Fig. [10.3](#Fig3)
    depicts the essential components of a bank.![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig3_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig3_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.3
  prefs: []
  type: TYPE_NORMAL
- en: Essential components of a bank
  prefs: []
  type: TYPE_NORMAL
- en: Once the essential components have been created then the legacy applications
    need to be placed in the appropriate component so that it can be linked with its
    related packaged software applications or decomposed into primitive APIs.
  prefs: []
  type: TYPE_NORMAL
- en: The first step to applying successful BPR to legacy applications is to develop
    an approach to defining the existing system and extracting its data elements and
    applications. Once again, this is similar to the process described above when
    replacing third-generation legacy applications in that the data needs to be captured
    into the data repository and the applications need to be defined and compared
    to a new model based on essential components .
  prefs: []
  type: TYPE_NORMAL
- en: 10.7 Data Element Enhancements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The analyst will need to design conversion programs that will access the data
    files that are not in relational database format and place them in a data repository
    . The ultimate focus is to replace all of the existing data files in the legacy
    with relational databases that can be linked with packaged software databases
    . This methodology differs from replacing legacies. In replacement engineering,
    data files are integrated directly into a packaged software system. This means
    that the legacy data will often be used to enhance the packaged software databases
    or integrated with various cloud database products. However, the process of enhancing
    legacy systems means that the legacy data will remain separate but converted into
    the relational or object database model. For legacies that already have relational
    databases , there is no restructuring required beyond setting up links with the
    packaged software database . Figure [10.4](#Fig4) reflects the difference between
    replacing legacy data and enhancing it. Notwithstanding these steps, once these
    data elements are determined, the analyst should follow the steps to consider
    what elements should be considered for replication on IoT and blockchain architectures.![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig4_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig4_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.4
  prefs: []
  type: TYPE_NORMAL
- en: Replacing versus enhancing legacy data
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting difference between the two approaches is that enhanced legacies
    will likely have intentional data redundancy. This means that the same element
    may indeed exist in multiple databases ; necessary for the new distributed systems
    supported by IoT and blockchain . Duplicate elements may take on different formats.
    The most obvious is where a data element has alias’, meaning that an element has
    many different names, but the same attributes. Another type is the same element
    name, but with different attributes. The third type, and the most challenging,
    is the duplicate elements that have different names and different attributes.
    While duplicate data elements may exist in enhanced legacy applications that are
    integrated with the packaged software product, it is still important to identify
    duplicate data relationships. This can be accomplished by documenting data relationships
    in a CASE tool and in the database’s physical data dictionary where aliases can
    exist.
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.1 Application Enhancements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BPR typically involves a methodology called Business Area Analysis (BAA). The
    purpose of BAA is to:'
  prefs: []
  type: TYPE_NORMAL
- en: establish the various legacy business areas that will be linked with new architectures
    and/or packaged software systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: re-engineer the new and old requirements of each business area.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: develop requirements that provide an OO perspective of each legacy business
    area, meaning that there is no need to map its requirements to the existing physical
    organization structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: define the links that create relationships among all the legacy business areas
    and the packaged software business areas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is accomplished by mapping business areas to specific essential component
    . Applications designed for the packaged software system must also be mapped to
    an essential component . Once this has occurred, the legacy applications and packaged
    software applications must be designed to share common processes and databases
    as shown in Fig. [10.5](#Fig5).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig5_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig5_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.5
  prefs: []
  type: TYPE_NORMAL
- en: BPR legacy modeling using essential components
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the legacy and packaged software applications have been placed in their
    appropriate essential component they will need to be linked, that is communicate
    with each other to complete the integration of the internal IoT and external systems.
    Linking occurs in two ways: parameter messaging and database . Parameter messaging
    requires that the legacy programs be modified to receive data in the form of parameters.
    This allows the application system to deliver information directly to the legacy
    program. Conversely, the legacy program may need to return information back to
    the packaged software system. Therefore, legacy applications need to be enhanced
    so they can actually format and send a data message to the packaged software system.
    A database interface is essentially the same concept except that it occurs differently.
    Instead of the application sending the data directly to another program, it forwards
    it as a record in a database file. The legacy program that returns the data also
    needs to be modified to forward messages to a likely cloud database .'
  prefs: []
  type: TYPE_NORMAL
- en: There are advantages and disadvantages of using either method. First, parameters
    use little overhead and are easy to program. They do not provide reusable data,
    that is, once a message has been received it is no longer available to another
    program. Parameters are also limited in size. Databases , on the other hand, allow
    programs to send the information to multiple destinations because it can be read
    many times. Unfortunately, it is difficult to control what applications or queries
    can access the data, which does raise questions about how secure the data is.
    Furthermore, applications must remember to delete a record in the database if
    it is no longer required. Figure [10.6](#Fig6) reflects the two methods of transferring
    data between legacy systems and packaged software applications.![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig6_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig6_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.6
  prefs: []
  type: TYPE_NORMAL
- en: Linking data between legacy and packaged software systems
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the legacy and the new applications have all been mapped to the essential
    components , analysts can use a CRUD diagram to assist them in reconciling whether
    all of the data and processes have been found. The importance of the CRUD diagram
    is that it ensures:'
  prefs: []
  type: TYPE_NORMAL
- en: that an essential component has complete control over its data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: that all of the entities are accessible by at least one process , and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: that processes are accessing data (Fig. [10.7](#Fig7)).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig7_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig7_HTML.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fig. 10.7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sample CRUD diagram
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While CRUD is not 100% accurate, it certainly uncovers potential problems as
    shown above. Even if BPR is not used, the CRUD diagram is an excellent tool to
    use to determine the processes and data needed for an essential component or an
    object. Once the CRUD diagram is finalized, the objects and classes would then
    be created as shown in Fig. [10.8](#Fig8); some of these objects are still in
    the form of a third-generation COBOL program while others might be in an API-based
    packaged software format. It is important to note; however, the “U” and the “D”
    are not allowed in a blockchain application, because ledger systems do not allow
    modification of existing transactions!![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig8_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig8_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.8
  prefs: []
  type: TYPE_NORMAL
- en: Essential component object diagrams
  prefs: []
  type: TYPE_NORMAL
- en: 10.8 “Leaving As Is”—Third-Generation Legacy Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Moving to an object-oriented and API paradigm from a third-generation product
    like COBOL may not be feasible. The language design of third-generation procedural
    programs may result in conceptual gaps between procedural and object-oriented
    philosophies. For example, distributed object-oriented programs require more detailed
    technical infrastructure knowledge and graphics manipulation then was required
    in older legacy systems . Native object-oriented features such as inheritance,
    polymorphism, and encapsulation do not apply in traditional third-generation procedural
    design. It is difficult, if not impossible to introduce new object concepts and
    philosophies during a direct COBOL to JAVA API migration. If the translation is
    attempted without significant restructuring (as discussed earlier in this chapter)
    then the resulting product will likely contain slower code that is more difficult
    to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: There can also be a cultural divide that occurs. Veteran COBOL programmers and
    newer JAVA API developers do not understand each other’s technologies. This scenario
    will often create bias during any conversion effort. Furthermore, COBOL programmers
    learning new technology can present them with self-specified threats to their
    cultural status. Furthermore, COBOL and RPG applications have benefited from more
    lengthy testing, debugging, and overall refinement than newer programming generations.
    While JAVA is more dynamic, it is less stable, and the procedure of debugging
    and fixing problems are very different than for COBOL or RPG. Therefore, the analyst
    will leave the legacy “as is,” and create only packaged software links for passing
    information that is required between the two systems. While this is similar to
    the “linking” proposed for enhancing legacy systems , it is different because
    legacy programs are not enhanced, except for the external links needed to pass
    information. This is graphically shown in Fig. [10.9](#Fig9).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig9_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig9_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.9
  prefs: []
  type: TYPE_NORMAL
- en: “As Is” legacy links
  prefs: []
  type: TYPE_NORMAL
- en: Using parameters or databases to link connecting information is still relevant,
    but analysts must be cognizant that legacy data formats will not be changed. This
    means that the legacy applications will continue to use their original file formats.
    Another concept used to describe “links” is called “bridges.” The word suggests
    that the link serves to connect a gap between the packaged software system and
    the legacy applications. Bridging can also imply temporary link. Very often “as
    is” can be seen as a temporary condition because legacy conversions cannot occur
    all at once, so it is typically planned in phases. However, while parts of the
    system are being converted, some portions may need temporary bridges until it
    is time to actually enhance them. One can visualize this as a temporary “road
    block” or detour that occurs when there is construction on a highway.
  prefs: []
  type: TYPE_NORMAL
- en: 10.9 Fourth-Generation Language Legacy System Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Integrating fourth-generation legacy systems with packaged software technology
    is much easier than third-generation languages. The reasons are two-fold. First,
    most fourth-generation implementations are already using a relational database
    , so conversion of data to the packaged software system is less complex. Second,
    fourth-generation language applications typically use SQL -based code, so conversion
    to an object-oriented system is also less involved.
  prefs: []
  type: TYPE_NORMAL
- en: 10.10 Replacing Fourth-Generation Legacy Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As stated above, replacement of fourth-generation language systems is less complex
    than third-generation languages with respect to the packaged software conversion.
    As with any system replacement, separating data and process is the suggested approach.
    Fortunately, in fourth-generation language systems, process and data are likely
    to already be separate, because of the nature of its architecture. Specifically,
    fourth-generation languages typically use relational databases , which architecturally
    separate data and process . Therefore, replacing the legacy is more about examining
    the existing processes and determining where the applications need to be reengineered.
  prefs: []
  type: TYPE_NORMAL
- en: 10.11 Approaches to Logic Reconstruction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best approach to logic analysis is to print out the source code of the programs.
    If the source is written in SQL , then the analyst should search for all SELECT
    statements. SQL SELECT FROM statements define the databases that the program is
    using as shown in Fig. [10.10](#Fig10).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig10_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig10_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.10
  prefs: []
  type: TYPE_NORMAL
- en: SELECT statements in a fourth-generation language application
  prefs: []
  type: TYPE_NORMAL
- en: 'As in third-generation languages logic reconstruction, the analyst should produce
    a DFD for every program as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SELECT statements define all inputs and outputs that a program uses. Each SELECT
    statement file will be represented by a DFD data store. Reviewing the logic of
    the application program will reveal whether the data is being created, read, updated,
    or deleted (CRUD ).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DFD’s should be decomposed to the functional primitive level so that the framework
    to an object-oriented system is established.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.For each DFD copy the relevant SQL code, making modifications where necessary
    to provide more object-oriented functionality to the program. This means that
    the decomposition of the code will likely require that some new logic be added
    to transform it to a method. This is shown in Fig. [10.11](#Fig11).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig11_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig11_HTML.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fig. 10.11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SQL code transition to object method
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examine existing system objects and determine if functional primitive DFDs belong
    to an existing class as a new method, or whether it truly represents a new object
    in the packaged software system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Capture all of the data elements required by the new methods and add them to
    their respective object. Ensure that the packaged software DD is updated appropriately.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine whether any new objects need to become a reusable component in the
    TP monitor (middleware), a reusable component in the client application, or as
    a stored procedure at the database level.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examine the legacy databases and do logic data modeling to place the entities
    in third-normal form (3NF).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine and integrate data elements with packaged software databases , ensuring
    that each data field from the legacy system is properly matched with packaged
    software data elements . New elements must be added to the appropriate entity
    or require that new entities be created for them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Link new entities with existing models using third-normal form referential integrity
    rules.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 10.Determine which data elements are redundant, such as calculations. These
    data elements will be removed; however, logic to their calculations may need to
    be added as a method as shown in Fig. [10.12](#Fig12).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig12_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig12_HTML.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fig. 10.12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transition of redundant data elements to process specifications
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 10.12 Enhancing Fourth-Generation Legacy Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enhancing fourth-generation language legacy systems is really the process of
    converting it to an object-oriented client/server system. BPR (Business Process
    Reengineering ) is also used on fourth-generation language legacy systems to accomplish
    this transition. The process , as one might expect, is much easier than for third-generation
    languages; however, the process of determining essential components is the same
    in both type of systems. Once essential components are established, the existing
    applications need to be decomposed and realigned as appropriate. This is accomplished
    by using BAA, as it was used for third-generation legacy applications. The fact
    that fourth-generation languages are less procedural than third-generation languages
    greatly assists this transition. Fourth-generation language systems, by simply
    looking at the SQL SELECT statements can identify which data files are used by
    the application. Using logic modularity rules, an analyst can establish cohesive
    classes based on applications that use the same data. This can be accomplished
    without using DFDs, although reengineering using DFDs is always a more thorough
    method for analysts to follow.
  prefs: []
  type: TYPE_NORMAL
- en: Linkage of fourth-generation language legacy and packaged software applications
    needs to be accomplished after application reengineering is completed. As with
    third-generation language systems, this can either be accomplished using a data
    parameter or the creation of a special database . However, with fourth-generation
    languages it is likely that application integration will occur using databases
    , since both systems use them in their native architectures. An analyst will most
    likely find that application communication with fourth-generation languages will
    not always require separate databases to be designed solely for the purpose of
    system linkage. The more attractive solution to integration is to identify the
    data elements that are common between the two systems so it can be shared in a
    central database available to all applications as shown in Fig. [10.13](#Fig13).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig13_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig13_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.13
  prefs: []
  type: TYPE_NORMAL
- en: Fourth-generation language legacy shared database architecture
  prefs: []
  type: TYPE_NORMAL
- en: The use of CRUD in fourth-generation languages is used less, but is certainly
    applicable and should be implemented by the analyst if he/she feels that the code
    is too procedural. In other words, the code architecture resembles third-generation
    as opposed to fourth-generation .
  prefs: []
  type: TYPE_NORMAL
- en: 10.13 “Leaving As Is”—Fourth-Generation Legacy Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of limiting integration to just the sharing of data is similar to
    the design architecture that I used for third-generation language systems. Indeed,
    the architecture of linking separate and distinct software systems can only be
    accomplished by sharing common data. Once again, this data can be shared either
    using a data parameter or data file.
  prefs: []
  type: TYPE_NORMAL
- en: Because many fourth-generation language systems utilize the same architecture
    as a packaged software system (three-tier client/server using Windows NTor UNIX/LINUX),
    it is sometimes advantageous to make use of certain operating system level communication
    facilities. For example, UNIX allows applications to pass data using an operating
    system facility called a “pipe.” A pipe resembles a parameter, in that it allows
    an application to pass a message or data to another application without creating
    an actual new data structure, like a database . Furthermore, a pipe uses an access
    method called “FIFO” (first-in, first-out), which is the same access criteria
    used by parameters. FIFO also requires that once the data is read, it cannot be
    read again. The major advantage of using a pipe is that the message/data can be
    stored long after the application that created the message has terminated in memory.
    Thus, linkage of information among IoT, packaged software, and fourth-generation
    language applications can be accomplished in RAM at execution time, which is called
    “intra-application communication.” This capability reduces overhead as well as
    the need for separate modules to be designed that would just handle data communication
    as shown in Fig. [10.14](#Fig14).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig14_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig14_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.14
  prefs: []
  type: TYPE_NORMAL
- en: Intra-application communication using a UNIX pipe
  prefs: []
  type: TYPE_NORMAL
- en: '10.14 Hybrid Methods: The Gateway Approach'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Thus far in this chapter I have focused on the interface between a specific
    type of legacy and IoT, blockchain and/or packaged software systems. Each type
    was defined with respect to its “generation” type. In reality, however, legacy
    systems are not that self-defined. Many large organizations have “legacy layers,”
    meaning that multiple generations exist throughout the enterprise. In this case
    , attempting to integrate each generation with a central packaged software system
    is difficult and time consuming. Indeed, migrating and integrating legacy systems
    is difficult enough. In these complex models, another method used for migration
    of legacy applications is a “hybrid” approach called “Gateway .” The gateway approach
    means that there will be a software module that mediates requests between the
    packaged software system and the legacy applications. In many ways, a gateway
    performs similar tasks as a TP system. Thus, the gateway acts as a broker between
    applications. Specifically, gateways :'
  prefs: []
  type: TYPE_NORMAL
- en: Separate yet integrate components from different generation languages. It allows
    for the linkages among multiple generation language systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translates requests and data between multiple components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coordinates between multiple components to ensure update consistency. This means
    that the gateway will ensure that redundant data elements are synchronized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typical gateway architectures would be designed as shown in Fig. [10.15](#Fig15).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig15_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig15_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.15
  prefs: []
  type: TYPE_NORMAL
- en: Gateway architecture for legacy integration
  prefs: []
  type: TYPE_NORMAL
- en: The most beneficial role of the gateway is that it allows for the phasing of
    legacy components. The infrastructure provides for an incremental approach to
    conversion by establishing a consistent update process for both data and applications.
  prefs: []
  type: TYPE_NORMAL
- en: 10.15 Incremental Application Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A gateway establishes a transparency for graphical user interfaces (GUI), character-based
    interfaces, and automated interfaces (batch updates) to appear the same to the
    packaged software system. Hence, the gateway insulates the legacy system so that
    its interface with the packaged software systems seems seamless. This is accomplished
    through an interface that translates requests for process functions and routes
    them to their appropriate application, regardless of the generation of the software
    and its particular phase in the packaged software migration. Figure [10.16](#Fig16)
    depicts the process functions of the gateway system.![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig16_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig16_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.16
  prefs: []
  type: TYPE_NORMAL
- en: Application functions of legacy gateway s
  prefs: []
  type: TYPE_NORMAL
- en: The most salient benefit of the gateway approach is its consistency with the
    object-oriented paradigm and the concept of application reusability. Specifically,
    it allows any module to behave “like” a reusable component notwithstanding its
    technical design. Under this architectural philosophy, a particular program, let’s
    say, a third-generation language system, may eventually be replaced and placed
    into the gateway , with temporary bridges built until the overall migration is
    completed. This procedure also supports a more “global” view of the enterprise
    as opposed to just focusing on a particular subsystem. Figure [10.17](#Fig17)
    depicts the concept of process integration using the gateway architecture.![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig17_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig17_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.17
  prefs: []
  type: TYPE_NORMAL
- en: Process integration and migration using gateway architecture
  prefs: []
  type: TYPE_NORMAL
- en: 10.16 Incremental Data Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incremental data integration focuses on the challenge of keeping multiple sets
    of data coordinated throughout a packaged software system.
  prefs: []
  type: TYPE_NORMAL
- en: The two primary issues relating to data integration focus on queries and updates.
    Queries involve the access of complete information about a dataset (collection
    of related data elements ) across multiple systems. Much of the query challenges
    can be addressed by using a data warehouse or data mining architecture. The gateway
    would serve as the infrastructure that would determine how many copies of the
    data exist and its location.
  prefs: []
  type: TYPE_NORMAL
- en: 'The more difficult and more important concept of data integration is the ability
    of the gateway to coordinate multiple updates across databases and flat-file systems.
    This means that the changing of a data element in one component would “trigger”
    an automatic update to the other components. There are four scenarios that could
    exist regarding the different definitions of data elements :'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data elements in each system have the same name. This at least allows analysts
    to identify how many copies of the element exist in the system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data elements do not match-up by name. This requires that the analyst design
    a “mapping” algorithm that tracks the corresponding name of each alias.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.Data elements match by name but not by attribute. In this case the analyst
    must propagate updates to the data element by tracking the different attribute
    definitions it has across systems. These differences can vary dramatically. The
    most obvious is element length. If the length of the data element is shorter than
    the one that has been updated then there is the problem of field truncation. This
    means that either the beginning value of ending value of the string will be lost
    when the value is propagated to the system with the shorter length definition.
    On the other hand, if the target is longer, then the process must populate either
    the beginning or end of the string so that the element has a complete value. This
    is graphically depicted in Fig. [10.18](#Fig18).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig18_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig18_HTML.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fig. 10.18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Propagating data elements with different field lengths
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Furthermore, the same data element might have different data types, meaning
    that one is alphanumeric and the other numeric. In this case , analysts need to
    know that certain values (e.g., a leading zero) will not be stored in the same
    way depending on its data type classification.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.There is not a one-to-one relationship among data elements . This suggests
    that a data element in one system may be based on the results of a calculation
    (a derived data element ). This would require a more in-depth analysis and mapping
    often solved by creating a stored-procedure that replicates the business rule
    to calculate the data element’s value. So in this case there might be simple copies
    of the element moved from one system to another, as well as one data element value
    that needs to first be calculated and then propagated across multiple systems.
    For example, if the data element “Total-Amount” is entered in one system but calculated
    as Quantity times Price in another, the propagation of the values is very complex.
    First, the analyst must know whether the calculated value is performed first before
    the resultant value, in this case , Total-Amount is reentered in another system.
    If this is true, then the propagation is much easier; once the calculation is
    made then the result is copied to the “entered” element. The converse is much
    more complex. If the Total-Amount was entered, but the values of Quantity and
    Price were not, then it would be very difficult to propagate until both Quantity
    and Price were entered. The example is further complicated if adjustments are
    made to the Quantity, Price, or Total-Amount. For any change, the systems would
    need to automatically be “triggered” to recalculate the values to ensure they
    are in synchronization. Figure [10.19](#Fig19) graphically shows this process
    .![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig19_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig19_HTML.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fig. 10.19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Propagation of calculated data elements
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 10.17 Converting Legacy Character-Based Screens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It would be naïve to assume that most legacy systems do not have character-based
    screens. Character-based screens are those that do not make use of the GUI . While
    most character-based screens in existence emanate from third-generation language
    mainframe implementations there are also many early fourth-generation language
    systems that preceded the GUI paradigm. Unfortunately, character-based screens
    often do not map easily to its GUI counterparts. The analyst must be especially
    careful not to attempt to simply duplicate the screens in the legacy software.
    Figure [10.20](#Fig20) shows a typical character-based legacy screen. Note that
    there can be up to four Contract/PO’s as shown in the upper right-hand corner.
    The user is required to enter each Contract/PO on a separate screen.![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig20_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig20_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.20
  prefs: []
  type: TYPE_NORMAL
- en: Character-based user screen
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the replacement GUI screen in Fig. [10.21](#Fig21) takes
    advantage of the view bar that allows for scrolling in a window. Therefore, the
    GUI version requires only one physical screen, as opposed to four.![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig21_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig21_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.21
  prefs: []
  type: TYPE_NORMAL
- en: Transformed character-based to GUI screen
  prefs: []
  type: TYPE_NORMAL
- en: 10.18 The Challenge with Encoded Legacy Screen Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most legacy character-based screens, a common practice was used to create
    codes that represented another, more meaningful data value. For example, numeric
    codes (1, 2, 3, 4, etc.) might be used to represent product colors such as blue,
    green, dark red, etc. Legacy applications used codes because they reduced the
    number of characters needed to type in the value on a screen. The technology to
    implement common GUI features such as drop-down menus and pop-up windows were
    not available. Indeed, many people used codes just from habit, or had to use them
    in order to implement computer systems. When transitioning to a GUI system, especially
    on the Web, it is wise to phase-out any data elements that are in an encoded form
    unless the codes are user-defined and are meaningful within the industry or business
    area. This essentially means that certain codes, like State (NY, CT, CA, etc.)
    are industry standards that are required by the business, as opposed to those
    created to aid in the implementation of software—like color codes. In the later
    case , the color name itself is unique and would be stored in an entity with just
    its descriptive name, as opposed to a code, which then identifies the actual description.
    Figure [10.22](#Fig22) shows the character-based and GUI screen transition.![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig22_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig22_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.22
  prefs: []
  type: TYPE_NORMAL
- en: Encoded value GUI screen transition
  prefs: []
  type: TYPE_NORMAL
- en: Changing character-based screens that contain encoded values has a trickle-down
    effect on the data dictionary and then on logic data modeling . First, the elimination
    of a coded value inevitably deletes a data element from the data dictionary. Second,
    codes are often key attributes, which become primary keys of entities. The elimination
    of the code, therefore, will eliminate the primary key of the entity . The new
    primary-key will likely be the element name. These changes must then be made to
    the entity relational diagram (ERD) and placed in production (see Fig. [10.23](#Fig23)).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig23_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig23_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.23
  prefs: []
  type: TYPE_NORMAL
- en: Transition to encoded databases
  prefs: []
  type: TYPE_NORMAL
- en: Third, the elimination of codes affects previous stored-procedures that use
    queries against the coded vales. Therefore, analysts must be sure to reengineer
    all queries that use the codes. This transition will add tremendous value since
    encoded elements typically add unnecessary overhead and time delays to queries.
    Finally, the elimination of encoded values will free up considerable space and
    index overhead. This will result in an increase in performance of the legacy system
    .
  prefs: []
  type: TYPE_NORMAL
- en: 10.19 Legacy Migration Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As stated earlier, all legacy systems inevitably must reach the end of their
    original life cycle. Therefore, notwithstanding whether certain components will
    remain “as is” or enhanced, eventually IT management must plan for migration to
    another system. The issue that this section addresses is how to establish a migration
    life cycle that takes into consideration an incremental approach to replacement
    of various legacy components within an enterprise computer system. The previous
    sections provided a framework of what can be done with legacy systems and their
    integration with other systems. This section provides a step-by-step template
    of procedures to follow that can assist the analysts on the schedule of legacy
    migration including temporary and permanent integration. This approach is an incremental
    one, so analysts can use it as a checklist of the progression they have made in
    the legacy migration life cycle. In all there are 12 steps as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze the existing legacy systems .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decompose legacy systems to determine schedules of migration and linkage strategies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design “As Is” links.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design legacy enhancements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design legacy replacements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design and integrate new databases .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine new infrastructure and environment, including gateways .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement enhancements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement links.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Migrate legacy databases .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '11.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Migrate replacement legacy applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '12.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Incrementally cutover to new systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The above steps are graphically depicted in Fig. [10.24](#Fig24).![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig24_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig24_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.24
  prefs: []
  type: TYPE_NORMAL
- en: Legacy migration life cycle
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that there are two streams of steps, that is, steps 3–5 and 8–10 that
    can occur in parallel. These steps encompass the three types of legacy migration
    choices that can be made: replacement, enhancement, and “as is.” While this life
    cycle seems simple, in reality it is a significant challenge for most migrations
    to plan, manage, and modify these steps and their interactions. Indeed, creating
    a migration plan and adequately coordinating the incremental and parallel steps
    is a difficult project. The subsequent sections will provide more details for
    each of these 12 steps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Analyze the Existing Legacy Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is obviously important that analysts fully understand all of the existing
    legacy components that exist in the system. The objective is to provide the requirements
    of each system and how it relates to the system. Analysts must remember that little
    to no documentation will be available to fully represent the architecture of the
    legacy system . However, analysts should compile as much information that is available
    including but not limited to:'
  prefs: []
  type: TYPE_NORMAL
- en: User and programming documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Existing users, software developers and managers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Required inputs and outputs and known services of the legacy system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any historical perspective on the history of the system itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of what existing information and documentation is available, certain
    aspects of reverse engineering must be used. Various CASE (Computer Aided Software
    Engineering) tools should be used that allows analysts to create a repository
    of data and certain levels of code analysis, particularly for third-generation
    language migrations. The analyst should create DFDs and PFDs for process analysis,
    and logic data modeling (LDM) and entity relational diagramming (ERD) for representation
    of the data. The analyst should also determine which legacy components are decomposable
    and non-decomposable. Inevitably, regardless of whether the decision is to replace
    immediately, enhance, or leave “as is,” ultimately little of the existing code
    will survive the ultimate migration of a legacy system (Brodie and Stonebraker
    [1995](#CR1)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Decompose Legacy Systems to Determine Schedules of Migration and Linkage
    Strategies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gradual migration of legacy systems is easiest accomplished when analysts
    utilize decomposition engineering. Previous chapters have outlined the process
    of functional decomposition , which is based on the breaking down of a system
    into its functional components. Decomposition , which results in components also
    allows for reusability of code. Since the fundamental premises of a package system
    are reusability, the process of decomposition is a mandatory part of the life
    cycle of any legacy migration. Thus, analysts should decompose all DFDs to functional
    primitives . Process analysis continues with the writing of process specifications
    for each functional primitive . These functional primitives will either be rewritten
    from the existing code or documentation or recreated from analyzing the functionality
    of the program. Analysts need to remove all dependency logic that links modules
    from the legacy code because it represents coupling among the programs. Module
    dependencies can typically be identified by finding *procedure calls* from within
    the legacy code. Ultimately, each of these process specifications will become
    methods. Eventually all methods will be mapped to classes and identify the attributes
    that each class needs. While this sounds simple, there will be a number of processes
    for which decomposition will be problematic. This will typically occur for legacy
    code that is too eclectic and needs to simply be reengineered. In these cases
    , analysts may want to interview users to understand the functionality that is
    required, as opposed to relying solely on the written legacy code.
  prefs: []
  type: TYPE_NORMAL
- en: From a data perspective, entity relational diagrams (ERD) need to undergo normalization
    . Remember that the DD and ERD produced in Step 1 is a mirror of the existing
    system, which most likely will not be in third-normal form. Thus this step will
    result in the propagation of new entities and even new data elements . Furthermore,
    data redundancies will be discovered as well as derived elements that should be
    removed from the logic model. However, while these steps are taking place, analysts
    need to be cognizant that the process of normalization represents the total new
    blueprint of how the data should have been originally engineered. The actual removal
    of elements or reconstruction of the physical database needs to be a phased-plan
    in accordance with the overall legacy migration effort. Thus Step 2 provides the
    decomposed framework, but not the schedule of implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, analysts must always be aware of the challenges of decomposition ,
    particularly over-decomposition , meaning that too many classes have been formed
    that will ultimately hurt system performance. There needs to be a mix of decomposed
    levels, which will serve as the basis of the new migration architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Design “As Is” Links'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This step involves determining and designing what components will remain untouched
    except for linkages that are necessary with other package software components.
    These modules are determined not to be part of the initial migration plans; however,
    they need to function *within* the packaged software system infrastructure and
    are therefore part of its architecture. Part of the decision needs to include
    how data will be migrated into the packaged software framework. In most cases
    , “as is” components continue to use their legacy data sources. Consideration
    must be given to how legacy data will be communicated to other components in the
    packaged software system. Analysts need to consider either a parameter-based communication
    system, or a centrally shared database repository as outlined earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Design Legacy Enhancements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This step determines which modules will be enhanced. BPR (business process reengineering
    ) will be used to design new features and functions in each business area. Analysts
    should identify essential components and determine what changes need to be employed
    to make the existing system behave more like an object-oriented system. Common
    processes and databases also need to be mapped so that shared resources can be
    designed between legacy and packaged software systems. New linkages will also
    be needed and the analyst must determine whether to use parameters or databases
    , or both to implement the communication among application systems.
  prefs: []
  type: TYPE_NORMAL
- en: User screens may also need to be updated as necessary, especially to remove
    encoded values or moving certain character-based screens to GUI. Many of the enhancements
    to a legacy application are implemented based on the analysis performed in Step
    1\. Any modifications will need to eventually operate on newer platforms once
    the total migration of legacy systems is completed. Analysts need to be cognizant
    that additional requirements mean increasing risk, which should be avoided when
    possible. However, during enhancement consideration it is almost impossible to
    ignore new requirements. Therefore, analysts need to focus on risk assessment
    as part of the life cycle of legacy migration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Design Legacy Replacements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Analysts must focus on how to reconstruct logic in a later generation of software
    architecture. Therefore, it is important to understand the differences in generation
    language design. This chapter provided two types of legacy software systems: third-
    and fourth-generation languages. Third-generation languages were depicted as being
    more procedural and more difficult to convert.'
  prefs: []
  type: TYPE_NORMAL
- en: Analysts must design the target applications so they will operate in accordance
    with the business rules and processes that will be supported in the new systems
    environment. Because of this integration, most replacement legacy migrations require
    significant reengineering activities. These activities necessitate the inclusion
    of new business rules that may have evolved since the legacy system was placed
    in production. Furthermore, new business rules can be created simply by the requirement
    of being an packaged software system.
  prefs: []
  type: TYPE_NORMAL
- en: Another important component of legacy migration is screen design. When replacing
    legacy systems , analysts must view the migration as assimilation, that is, the
    old system is becoming part of the new one. As a result, all existing packaged
    software screens need to be reviewed so that designers can determine whether they
    need to be modified to adopt some of the legacy functionality. This is not to
    suggest that all of the legacy systems screens will be absorbed into the existing
    packaged software system. Rather, there will be a combination of new and absorbed
    functionality added to the target environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Design and Integrate New Databases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From an enterprise perspective, analysts must gather all of the permutations
    of legacy systems that exist and seek to provide a plan on how to integrate the
    data into one source. For “as is” solutions, legacy data files will most likely
    remain separate from the packaged software system until the complete legacy is
    migrated. However, the process of enhancing and replacing legacy data should have
    the objective of creating one central database source that will serve the entire
    packaged software enterprise system. Indeed, a central data source reduces data
    redundancy and significantly increases data integrity .
  prefs: []
  type: TYPE_NORMAL
- en: The process of data integration can only be accomplished by “combining user
    views” which is the process of matching up the multiple data element definitions
    that overlap and identifying data redundancies and alternative definitions. This
    can only be completed by creating a repository of data elements and by representing
    the data graphically using an ERD. Once each system is represented in this fashion
    then the analyst must perform logic data modeling as prescribed in Chap. [9](480347_1_En_9_Chapter.xhtml).
    The result will be the creation of one central database system cluster that can
    provide the type of integration necessary for successful packaged software implementations.
  prefs: []
  type: TYPE_NORMAL
- en: While this is the goal for all analysts, the road to successful implementation
    is challenging. First, the process of normalization will require that some data
    elements be deleted (e.g., encoded values) while others will need to be added.
    Second, full data integration cannot be attained until all replacement screens
    are complete. Third, applications must be redesigned so that the centrality of
    the data source is assumed to exist. Since this process may be very time-consuming,
    it may not be feasible to attempt a full database migration at one time. Therefore,
    the legacy data may need to be logically partitioned to facilitate incremental
    database migration. Thus, there will be legacy data subsets that are created to
    remain independent of the central database until some later migration phase is
    deemed feasible. Of course, this strategy requires the design of temporary “bridges”
    that allows the entire packaged software system to appear cohesive to users.
  prefs: []
  type: TYPE_NORMAL
- en: Another important factor in planning data migration is to determine how much
    is really known about the legacy data itself. The less knowledge available, the
    longer the period where legacy data and packaged software data need to run separately
    and in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 7: Determine New Infrastructure and Environment, including Gateway s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prior to the migration of any system, the necessary hardware and software infrastructure
    must be planned and installed. A common error in legacy migration is not factoring
    the time and effort to provide this infrastructure. Furthermore, this process
    , which may create a new network environment, needs to determine the placement
    of software in a three-tier client/server platform. This means that further decomposition
    may be required of all processes , especially object classes . You may recall
    in Chap. [8](480347_1_En_8_Chapter.xhtml) that classes may undergo further decomposition
    depending on the need to distribute application across the network.
  prefs: []
  type: TYPE_NORMAL
- en: Another important factor is performance. In many instances it is difficult for
    network engineers to predict performance in large application system environments.
    It may be necessary to plan for several benchmarks in performance early in the
    design phase. Benchmarking is the process of setting up an environment that replicates
    a production network so that modifications can be made, if necessary, to the design
    of the system to increase performance.
  prefs: []
  type: TYPE_NORMAL
- en: Another decision that must be made at this juncture is whether a gateway infrastructure
    will be created to mediate legacy layers. This decision, of course is highly dependent
    on the migration life cycle; the more legacy layers that will be phased in over
    time, the higher the chances that a gateway processor for data and applications
    will be necessary. The decision to go with a gateway is significant, not only
    from the perspective of software design, but network infrastructure as well. Constructing
    a gateway can be very costly. It involves writing the system from scratch or tailoring
    a commercial product to meet the migration requirements. It is also costly because
    of the amount of additional hardware necessary to optimize the performance of
    the gateway servers. However, the benefits of a gateway are real, as it could
    provide a dependable structure to slowly migrate all components in the new systems
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 8: Implement Enhancements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This step requires a schedule of when legacy enhancements will be implemented
    and become part of the production system. Many analysts suggest that the simplest
    modules go into production first so that any unexpected problems can be dealt
    with quickly and efficiently. Furthermore, simple modules tend to have small consequences
    should there be a problem in processing or performance. There are some other aspects
    of coordination, however. For example enhancements that feed off of the same data
    or use the same subsystems should obviously be implemented at one time.
  prefs: []
  type: TYPE_NORMAL
- en: Another factor in the decision of which enhanced components go first relates
    to the affects it has on other subsystems. This means that the priority may indeed
    be influenced by what other systems need or are dependent upon from other systems.
    Another issue could be the nature of the legacy links. Should a link be very complex
    or dependent on other subsystem enhancements, its schedule could be affected.
    Finally, the nature of the enhancements has much to do with the decision as opposed
    to just the application’s complexity. There may be simple enhancements that are
    crucial for the packaged software system and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 9: Implement Links'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As I alluded to in Step 8, the determination of legacy links greatly affects
    the scheduling of the migration cycle. Once the determination is made in Step
    8, the related legacy links must be put in place. This could also mean that the
    gateway , if designed, must also be in operation since many links might be filtered
    through the gateway infrastructure. So, the implementation of legacy links relates
    to both hardware and software. Notwithstanding whether a gateway is in place,
    database links often require separate servers. In many cases because these “servers”
    interface with the internet , there is a need to install firewalls to ensure security
    protection.
  prefs: []
  type: TYPE_NORMAL
- en: From a software perspective, legacy links can almost be treated like conversion
    programs. There needs to be substantial testing done to ensure they work properly.
    Once legacy links are in production, like data conversions, they tend to keep
    working. It is also important to ensure that legacy links are documented. Indeed,
    any link will eventually be changed based on the incremental migration schedule.
    Remember that most legacy links are accomplished by building temporary “bridges.”
    The concept of temporary can be dangerous, in that many of these links, over time,
    tend to be more permanent. That is, their temporary life can sometimes extend
    beyond the predicted life of a permanent component. The message here should be
    that legacy links, while they are a temporary solution, should be designed under
    the same intensity and adherence to quality as any other software development
    component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 10: Migrate Legacy Databases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The migration of data is so complex that it should be handled as a separate
    and distinct step in the migration life cycle. Data affects everything in the
    system, and often if it is not migrated properly it can cause immense problems.
    First, the analyst must decide on the phasing of data based on the schedule of
    application migration. Hopefully, the process of data migration should be done
    in parallel to Steps 8 and 9.
  prefs: []
  type: TYPE_NORMAL
- en: The most challenging aspect of data migration is the physical steps in the process
    . Migrating new entities and schema changes are complex. For example, changes
    to databases require that the tables be “dropped” meaning that they are taken
    off-line. Data dictionaries need to be updated, and changes to stored procedures
    and triggers are extremely time-consuming. Most problematic is the process f quality
    assurance . While some testing can be done in a controlled environment, most of
    the final testing must be done once the system is actually in production. Therefore,
    the coordination with users to test the system early is critical. Furthermore,
    there must be back-up procedures in case the database migration does not work
    properly. This means that there is an alternate fail-safe plan to reinstall the
    old system should major problems arise. Finally, a programming team should be
    ready to deal with any problems that arise that do not warrant reinstalling the
    old version. This might include the discovery of application “bugs” that can be
    fixed within a reasonable period and are not deemed critical to operations (which
    means there is usually a “work-around” for the problem). Analysts must understand
    that this process must be followed each time a new database migration takes place!
  prefs: []
  type: TYPE_NORMAL
- en: Database migration is even more complex when there is a gateway . The reason
    is that the gateway , from an incremental perspective, contains more and more
    database responsibilities each time there is a migration. Therefore, for every
    migration, the amount of data that can be potentially affected grows larger. In
    addition, the number of data that becomes integrated usually grows exponentially,
    so the planning and conversion process becomes a critical path to successful migration
    life cycles. Since the migration of legacy databases becomes so much more difficult
    as the project progresses, the end of the life cycle becomes even more challenging
    to reach. That is why many migrations have never been completed!
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 11: Migrate Replacement Legacy Applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the database migration is completed, then the remainder of the legacy applications
    can be migrated to the new system. These applications are usually the replacement
    components, which have been reengineered in the object-oriented paradigm. These
    programs, then, have been designed to operate against the target databases with
    the new functionality required for the packaged software system. Since replacement
    applications usually do not create links, there is typically little effect on
    gateway operations. What is more challenging is the quality assurance process
    . Users need to be aware that the code is relatively new and will contain problems
    regardless of the amount of pre-production testing that has been performed. In
    any event, programmers, database administrators, and quality assurance personnel
    should be on-call for weeks after system cutover.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 12: Incrementally Cutover to New Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As discussed above, testing and application turnover are two areas that frequently
    are overlooked. Because projects typically run over budget and schedule, the final
    procedures like testing and verification are usually shortened. The results of
    this decision can be devastating to successful legacy migrations. Because of the
    size and complexity of many packaged software systems, to go “cold turkey” is
    unrealistic and irresponsible. Therefore, an analyst should consider providing
    test scenarios that provide more confidence that the system is ready to be cutover.
    This approach is called “acceptance testing” and requires that users be involved
    in the determination of what tests must be performed before the system is ready
    to go live. Thus, acceptance test plans can be defined as the set of tests that
    if passed will establish that the software can be used in production. Acceptance
    tests need to be established early in the product life cycle and should begin
    during the analysis phase. It is only logical then that the development of acceptance
    test plans should involve analysts. As with requirements development, the analyst
    must participate with the user community. Only users can make the final decision
    about the content and scope of the test plans. The design and development of acceptance
    test plans should not be confused with the testing phase of the software development
    life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Another perspective on acceptance testing is that it becomes a formal checklist
    that defines the minimal criteria for incrementally migrating systems. However,
    one must work with the understanding that no new product will ever be fault-free.
    The permutations of testing everything would make the timetable for completion
    unacceptable and cost prohibitive. Therefore, the acceptance test plan is a strategy
    to get the most important components tested completely enough for production.
    Figure [10.25](#Fig25) represents a sample acceptance test plan.![../images/480347_1_En_10_Chapter/480347_1_En_10_Fig25_HTML.png](../images/480347_1_En_10_Chapter/480347_1_En_10_Fig25_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10.25
  prefs: []
  type: TYPE_NORMAL
- en: Acceptance test plan
  prefs: []
  type: TYPE_NORMAL
- en: 10.20 Problems and Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a Legacy? Explain.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the Five generation languages. What increases with each generation?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are Essential Components?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the Object-Oriented paradigm mean?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does Object Orientation relate to Business Area Analysis?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a Legacy Link?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the notion of Logic Reconstruction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a UNIX pipe?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain how Legacy Integration operates through Gateway Architecture.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Propagation?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '11.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the essential differences between character-based screens and GUI?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '12.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an encoded value?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '13.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the relationship between an object and an API?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '14.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the restrictions of CRUD in blockchain architecture? Why is this difference
    so important in a ledger-based system?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
