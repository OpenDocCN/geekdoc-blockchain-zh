- en: © Nishith Pathak and Anurag Bhandari 2018Nishith Pathak and Anurag BhandariIoT,
    AI, and Blockchain for .NET[https://doi.org/10.1007/978-1-4842-3709-0_1](A458845_1_En_1_Chapter.html)
  prefs: []
  type: TYPE_NORMAL
- en: 1. The Artificial Intelligence 2.0 Revolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nishith Pathak^(1 ) and Anurag Bhandari²(1)Kotdwara, Dist. Pauri Garhwal, India(2)Jalandhar,
    Punjab, IndiaOnce upon a time, computers were as big as rooms. They were capable
    of complex mathematical calculations. They were not, though, meant to be operated
    by people like me and you. Nor were they designed for creating documents and presentations,
    playing games, or surfing the web. Early computers were powered by vacuum tubes—just
    like most other sophisticated electronic devices of the time—and were used in
    scientific research. Then, the semiconductor revolution happened, and the transistor
    was born.NoteA vacuum tube is an electronic device that was a common component
    in old radio and television sets, amplifiers, and even computers. The tube is
    a glass enclosure that houses an anode and a cathode inside a vacuum (no air or
    gas). It’s based on the principle that electric current can move through vacuum
    and does not need solid material for the purpose. The first vacuum tube was a
    diode that, unlike semi-conductor diodes of today, was large and fragile.Transistors
    gave birth to microprocessors, and microprocessors eventually brought computers
    into our homes and allowed them to do much more than just record scientific data,
    crunch numbers, or break codes. The first IBM personal computer was powered by
    an Intel 8088 chip, which ran at a “blazing” speed of 4.77MHz. Processors soon
    went through a revolution of their own, one dictated by the famous Moore’s Law.
    The processing power of computers roughly doubled every 18 months, allowing them
    to do tasks that could not be efficiently done on previous generation processors.What
    is the common pattern here? All these revolutions have affected not just the performance
    or software development methods but also computing in general, in a way unimaginable
    before.There have been other such historic revolutions—in parallel and subsequent—that
    have changed computing forever. Take a recent phenomenon for instance: the Cloud
    revolution. Back in 2010, when “cloud” was just a buzzword in newspapers and magazines,
    there was widespread confusion about the true meaning of cloud. Everyone talked
    about the disruptive potential and lasting benefits of Cloud, but only pockets
    of technology-savvy people actually understood it. A few years later everyone
    had adopted it. Cloud has affected not just businesses by offering entirely new
    business models to run their companies on but also affected our personal lives.
    Today, we cannot begin to imagine a world without Cloud: a world without online
    storage, unlimited music and video streaming, photo sharing, collaborative document
    editing, and social networking at the speed of light. Businesses have saved millions
    of dollars by basing the better part of their infrastructure on Cloud services
    rather than bearing steep costs of managing in-house networks of servers.Cloud
    went on to become a key enabler of the Big Data revolution. Cloud computing gave
    us enough power to analyze billions of records, worth terabytes of data, exponentially
    quicker, and at considerably lower costs.Let’s try to understand the role of Cloud
    through an example. Consider an e-commerce website, say Amazon.com. At a high
    level, it stores two types of data about its users—transactional and non-transactional.
    Non-transactional data is information about customers (name, email, or address),
    items (name, price, discount, or seller), etc. Transactional data, on the other
    hand, is information about a particular transaction on the website, e.g., buying
    an item, submitting a product review, adding an item to wishlist or cart, etc.
    This type of data grows at a rapid pace on sites like Amazon. On Prime Day 2016,
    Amazon recorded sales of close to 600 items per second. That is 2.1 million items
    in one hour alone!Storing such gigantic data was once prohibitively expensive,
    forcing companies to archive or remove transactional data after a set retention
    period (a few weeks to a couple of years). The potent combination of Cloud and
    Big Data technologies has not only enabled us to store (rather than throwing away)
    huge amounts of historic data at dirt cheap prices, but it has also allowed us
    to leverage the archived data to perform complex data analytics tasks over years’
    worth of data to derive meaningful statistics and graphs for customer behavior
    and buying trends—what were the top selling hundred items in each category during
    peak hours on sale day; which items were popular among users in terms of viewing
    but not in terms of buying?Almost simultaneously came the IoT revolution. As with
    Big Data, Cloud is a key enabler of this revolution. Powered by a variety of sensors,
    IoT devices generate so much data that Big Data technologies usually go hand-in-hand
    with IoT. Cloud provides both storage and computing power to the otherwise lightweight
    devices on an IoT network. After helping spark two big revolutions, Cloud did
    it again with Artificial Intelligence (AI).AI has surfaced and resurfaced in several
    waves, but it’s only recently that it has become commonplace. The widespread affordability
    and use of AI in software development has been seen as a revolution. The first
    AI revolution, the one we are currently witnessing, is about AI-as-a-Service.
    The book Artificial Intelligence for .NET: Speech, Language, and Search (Apress,
    2017) gives an in-depth of creating AI-enabled software applications. With advancements
    in IoT and the emergence of Blockchain, AI is on brink of a second revolution,
    one that involves creating complete product offerings with intelligent software
    and custom hardware.NoteThe “as a service” model is generally associated with
    Cloud infrastructure, and that is true here as well. World’s top tech companies—Google,
    IBM, Microsoft, and Amazon—are offering AI services, in the form of SDKs and RESTful
    APIs, that help developers add intelligence to their software applications. Let’s
    understand this in a little more detail.Let’s explore each component of the upcoming
    AI 2.0 revolution in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The meaning of artificial intelligence (AI) has evolved over generations of
    research. The basic concepts of AI have not changed, but its applications have.
    How AI was perceived in the 1950s is very different from how it’s actually being
    put to use today. And it’s still evolving.Artificial intelligence is a hot topic
    these days. It has come a long way from the pages of popular science fiction books
    to becoming a commodity. And, no, AI has nothing to do with superior robots taking
    over the world and enslaving us humans. At least, not yet. Anything intelligent
    enough, from your phone’s virtual assistant (Siri and Cortana) to your trusty
    search engine (Google and Bing) to your favorite mobile app or video game, is
    powered by AI. Figure [1-1](#Fig1) shows an AI-powered intelligent chatbot.![A458845_1_En_1_Fig1_HTML.jpg](A458845_1_En_1_Fig1_HTML.jpg)Figure
    1-1An intelligent chatbot that can place pizza orders by humanly understanding
    its usersInterest in AI peaked during the 2000s, especially at the start of 2010s.
    Huge investments were made in AI research by both academia and corporations, investments
    that have not only affected these institutions but also its affiliates and users.
    For software developers, this has been nothing short of a boon. Advances made
    by companies such as Microsoft, Google, Facebook, and Amazon in various fields
    of AI, and the subsequent open-sourcing and commercialization of their products,
    has enabled software developers to create human-like experiences in their apps
    with unprecedented ease. This has resulted in an explosion of smart, intelligent
    apps that can understand their users just as a normal human would.Have you, as
    a developer, ever thought about how you can use AI to create insanely smart software?
    You probably have, but did not know where to start.Like all humans, developers
    have pre-conceived notions about products and technologies. In our experience
    with software developers at top IT companies, a common perception that we’ve found
    among both developers and project managers is that adding AI capabilities, such
    as natural language understanding, speech recognition, machine learning, etc.,
    to their software would require a deep understanding of neural networks, fuzzy
    logic, and other mind-bending computer science theories. Well, let us tell you
    the good news. That is not the case anymore.The intelligence that powers your
    favorite applications, like Google search, Bing, Cortana, and Facebook, is slowly
    being made available to developers outside of these companies: some parts for
    free and the others as SaaS-based paid commercial offerings.'
  prefs: []
  type: TYPE_NORMAL
- en: AI in the Old Days
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The term “artificial intelligence” was coined at a conference on the campus
    of Dartmouth College in the summer of 1956\. The proposal for the conference included
    this assertion: “Every aspect of learning or any other feature of intelligence
    can be so precisely described that a machine can be made to simulate it.” It was
    during this conference that the field of AI research was established, and the
    people who attended it became the pioneers of AI research.During the decades that
    followed, there were major breakthroughs in the field of AI. Computer programs
    were developed to solve algebra problems, prove theorems, and speak English. Government
    agencies and private organizations poured in funds to fuel the research. But the
    road to modern AI was not easy.The first setback to AI research came in 1974\.
    The time between that year and 1980 is known as the first “AI Winter.” During
    this time, a lot of promised results of the research failed to materialize. This
    was due to a combination of factors, the foremost one being the failure of scientists
    to anticipate the difficulty of the problems that AI posed. The limited computing
    power of the time was another major reason. As a result, a lack of progress led
    the major British and American agencies that were earlier supporting the research
    to cut off their funding.The next seven years, 1980-87, saw a renewed interest
    in AI research. The development of expert systems fueled the boom. Expert systems
    are rule engines designed for specific domains; they are fed with a set of logical
    rules derived from the knowledge of experts; decisions are calculated for each
    input using the fed rules. They were getting developed across organizations, and
    soon all big giants started investing huge amount of money in artificial intelligence.
    Work on neural networks laid the foundation for the development of optical character
    recognition and speech recognition techniques. The following years formed the
    second AI Winter, which lasted from 1987 to 1993\. Like the previous winter, AI
    again suffered financial setbacks.1993-2001 marked the return of AI, propelled
    in part by faster and cheaper computers. Finally, older promises of AI research
    were realized because of access to faster computing power, the lack of which had
    started the first winter. Specialized computers were created using advanced AI
    techniques to beat humans. Who can forget the iconic match between IBM’s Deep
    Blue computer and the then reigning chess champion Garry Kasparov in 1997?AI was
    extensively used in the field of robotics. The Japanese built robots that looked
    like humans and even understood and spoke human languages. The western world wasn’t
    far behind, and soon there was a race to build the most human-like mechanical
    assistant to man. Honda’s ASIMO is a brilliant example of what could be achieved
    by combining robotics with AI: a 4’3” tall humanoid that can walk, dance, make
    coffee, and even conduct orchestras.'
  prefs: []
  type: TYPE_NORMAL
- en: Status Quo
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'AI started off as a pursuit to build human-like robots that could understand
    us, do our chores, and remove our loneliness. But today, the field of AI has broadened
    to encompass various techniques that help in creating smart, functional, and dependable
    software applications.With the emergence of a new breed of technology companies,
    the 21st Century has seen tremendous advances in artificial intelligence, sometimes
    behind the scenes in the research labs of Microsoft, IBM, Google, Facebook, Apple,
    Amazon, and more. Perhaps one of the best examples of contemporary AI is IBM’s
    Watson, which started as a computer system designed to compete with humans on
    the popular American TV show “Jeopardy!”. In an exhibition match in 2011, Watson
    beat two former winners to clinch the $1 million prize money. Propelled by Watson’s
    success, IBM soon released the AI technologies that powered its computer system
    as individual commercial offerings. AI became a buzzword in the industry, and
    other large tech companies entered the market with commercial offerings of their
    own. Today, there are startups offering highly specialized but accurate AI-as-a-Service
    offerings.AI has not been limited to popular and enterprise software applications.
    Your favorite video games, both on TV and mobile, have had AI baked for a long
    time. For example, when playing single player games, where you compete against
    the computer, your opponents make their own decisions based on your moves. It
    is even possible to change the difficulty level of the opponents: the harder the
    difficulty level, the more sophisticated the “AI” of the game, and the more human-like
    your opponents will be.'
  prefs: []
  type: TYPE_NORMAL
- en: The Buildup to AI 1.0 Revolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Commoditization of AI, as highlighted in the previous section, was made possible
    because of the development of key enabling technologies such as machine learning.
    This term is best explained by the help of an example.Microsoft Bing, the popular
    search engine from Microsoft, can not only perform keyword-based searches but
    also search the Web based on the intended meaning of your search phrase. So doing
    a simple keyword search like “Taylor Swift” will give you the official website,
    Wikipedia page, social media accounts, recent news stories, and some photos of
    the popular American singer-songwriter. Doing a more complex search like “Who
    is the president of Uganda?” will give you the exact name in a large font and
    top web results for that person. It’s like asking a question of another human,
    who knows you do not mean to get all web pages that contain the phrase “Who is
    the president of Uganda,” just the name of the person in question.In both examples
    (Taylor Swift and President of Uganda), Bing will also show, on the left, some
    quick facts about the person: date of birth, spouse, children, etc. And depending
    on the type of person searched, Bing will also show other relevant details, such
    as education, timeline, and quotes for a politician, and net worth, compositions,
    and romances for a singer. How is Bing able to show you so much about a person?
    Have Bing’s developers created a mega-database of quick facts for all the famous
    people in the world (current and past)? Not quite.Although it is not humanly impossible
    to create such a database, the cost of maintaining it would be huge. Our big,
    big world, with so many countries and territories, will keep on producing famous
    people. So there’s a definite scalability problem with this database.The technique
    Microsoft used to solve this problem is called machine learning. We look at machine
    learning in a bit. Similarly, the thing that enables Bing to understand the meaning
    of a search phrase is natural language understanding (NLU). You can ask the same
    question of Bing in a dozen different ways and Bing will still arrive at the same
    meaning every time. NLU makes it smart enough to interpret human languages in
    ways humans do subconsciously.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The term machine learning was coined by Arthur Samuel in his 1959 paper “Some
    Studies in Machine Learning.” As per Samuel, machine learning is what “gives computers
    the ability to learn without being explicitly programmed”. We all know a computer
    as a machine that performs certain operations by following instructions supplied
    by humans in the form of programs. So how is it possible for a machine to learn
    something by itself?Machine learning is the very fundamental concept of artificial
    intelligence. ML explores the study and construction of algorithms that can learn
    from data and make predictions based on their learning. ML is what powers an intelligent
    machine; it is what generates artificial intelligence.A regular, non-ML language
    translation algorithm would have static program instructions to detect which language
    a sentence is written in: words used, grammatical structure, etc. Similarly, a
    non-ML face detection algorithm would have a hard-coded definition of a face:
    something round, skin colored, having two small dark regions near the top (eyes),
    etc. An ML algorithm, on the other hand, doesn’t have such hard-coding; it learns
    by examples. If you train it with lots of sentences that are written in French
    and some more that are not written in French, it will learn to identify French
    sentences when it sees them. Figure [1-2](#Fig2) shows how an ML algorithm iteratively
    uses sample data to create a trained model that can make predictions based on
    new inputs.![A458845_1_En_1_Fig2_HTML.jpg](A458845_1_En_1_Fig2_HTML.jpg)Figure
    1-2Large amounts of training data are supplied to an ML algorithm  , such as a
    neural network, to create what is called a “trained model”. A trained model can
    then be used to quickly make predictions against the specified input data.A lot
    of real-world problems are nonlinear, such as language translation, weather prediction,
    email spam filtering, predicting the next president of the United States, classification
    problems (such as telling apart species of birds through images), and so on. ML  is
    an ideal solution for such nonlinear problems where designing and programming
    explicit algorithms using static program instructions is simply not feasible.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating AI-Enabled Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know now how precious ML is for solving nonlinear problems. Technically,
    AI is seen as a wide research area whose goal is to explore fields of computer
    science and to create technologies to lend machines human-like intelligence. One
    major field of AI—and the one used the most—is natural language processing or
    NLP. It is an umbrella term that is used to represent various related and independent
    tasks related to processing natural languages, languages that humans speak. Understanding
    the basic communication medium of humans is the foundation of intelligence in
    our world, is it not? Among the various NLP tasks are language understanding (NLU),
    machine translation (language translation), speech recognition (speech to text),
    etc.Another major field of AI is computer vision (CV), the field that deals with
    making machines intelligent enough to interpret visuals (images and videos) the
    way humans do. Some common CV tasks include face detection and recognition, object
    detection and recognition, optical character recognition (OCR), image classification,
    etc.Each of the NLP and CV  tasks is a nonlinear problem that can be solved through
    machine learning. In technical terms, we can create machine learning models for
    each of them using sufficiently large amounts of training data (usually in gigabytes).
    Creating one ML model, let alone all, is intellectually challenging and expensive.
    Even if you have a team of computer science and math people armed with ML skills,
    there’s the cost of extremely expensive hardware to take care of to develop those
    models. It may take up to 2-3 months to accurately train an ML model on high-end
    machines running 24x7, using the currently available deep learning toolkits such
    as Microsoft CNTK, Google Tensorflow, and Torch. Although specialized ML-friendly
    microprocessors are being developed (iPhone’s A11 Bionic and Google’s TPU) and
    hardware costs are coming down further, it’s still exponentially easier and cheaper
    to use commercially available AI services.AI services—such as Microsoft’s Cognitive
    Services APIs—can be used in virtually any code to incrementally add intelligence.
    This task has been reduced to a matter of a few REST API calls. All of the following
    use cases are not cumbersome to implement anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding face detection in a security system’s software to detect intruders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding face recognition in social apps to identify friends
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding language understanding in chatbots for automated support, ticket bookings,
    etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting text from images using OCR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AI 1.0 revolution is literally changing the way software is written. AI
    is no longer a dominion of enthusiasts. The practicality and accuracy of contemporary
    AI solutions have compelled even the most traditional IT and software companies
    to adopt it. IT companies are using AI to create innovative new solutions and
    to redesign existing software for their clients. And an increasing number of clients
    are themselves demanding AI-enabled software.
  prefs: []
  type: TYPE_NORMAL
- en: What Is AI 2.0?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next generation of AI technologies will take software solutions several
    steps ahead. AI 2.0 is something bigger than AI alone. It’s no longer about just
    creating intelligent software.This has been made possible by recent advancements
    in cognitive technologies (AI), the Internet of Things (IoT), and Blockchain.
    Figure [1-3](#Fig3) highlights this equation. IoT and Blockchain are relatively
    recent developments but have co-existed with AI for some time now.![A458845_1_En_1_Fig3_HTML.jpg](A458845_1_En_1_Fig3_HTML.jpg)Figure
    1-3AI 2.0 is a combination of AI (intelligent software), IoT (intelligent devices),
    and Blockchain (intelligent network)Improvements in AI technologies have made
    it increasingly easier to develop “complete” product offerings using AI, IoT,
    and Blockchain. To reiterate, it’s not about just software anymore but it’s about
    software and hardware on top of a highly secure and flexible network.The following
    sections list three possibilities opened by AI 2.0\. Try to imagine more.
  prefs: []
  type: TYPE_NORMAL
- en: Early Warning Systems for Wildlife
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some rural areas across the globe are hotbeds of human-animal conflict, one
    of the most common being humans versus elephants. Humans are known to illegally
    poach elephants for their tusks. In retaliation, elephants often trample humans’
    crop fields. As per WWF, current intrusion prevention systems, such as electric
    fencing, are expensive, difficult to maintain, and sometimes life-threatening
    for elephants. Intelligent early warning systems that work based on real-time
    data collected from IoT monitoring devices can be a cost-effective and highly
    scalable solution for this sensitive problem.
  prefs: []
  type: TYPE_NORMAL
- en: Smart Lean Manufacturing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Manufacturing practices have become highly optimized. But there is still room
    for improvement. IoT devices can be used to monitor machines and environment.
    Intelligent analytics can then be performed over the collected monitoring data
    to generate insights to help further optimize manufacturing processes. Blockchain
    can help in securely and reliably distribute optimized parameters across a string
    of connected manufacturing plants. Supply chain  can also be effectively managed
    using Blockchain.
  prefs: []
  type: TYPE_NORMAL
- en: Connected Homes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mark Zuckerberg, founder and CEO of Facebook, conducted and published a famous
    AI experiment in 2016\. Jarvis was a custom home automation solution built using
    several open source and in-house AI libraries. AI 2.0 will allow us to easily
    create not just one such smart home but an ultra-secure network of connected smart
    homes. You can read more about his experiment at [https://www.facebook.com/notes/mark-zuckerberg/building-jarvis/10154361492931634/](https://www.facebook.com/notes/mark-zuckerberg/building-jarvis/10154361492931634/)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Azure Cognitive Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cognitive Services is a set of software-as-a-service (SaaS) commercial offerings
    from Microsoft related to artificial intelligence. Cognitive Services is the product
    of Microsoft’s years of research into cognitive computing and artificial intelligence,
    and many of these services are being used by some of Microsoft’s own popular products,
    such as Bing (search, maps), Translator, Bot Framework, etc.Microsoft has made
    these services available as easy-to-use REST APIs, directly consumable in a web
    or a mobile application. As of writing this book, there are 30 available cognitive
    services, broadly divided into five categories—Vision, Speech, Language, Search,
    and Knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: The Internet of Things
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Internet of Things is a routinely misunderstood term. In fact, Internet
    itself is highly misunderstood, let alone “things?” So, what is the Internet and
    what are the things? And more importantly, how are things connected to the Internet
    (and each other)?The Internet was invented to connect two computers, sitting in
    two different parts of the world, with each other. It was not invented, as is
    a common misconception, to help you browse websites. Sometimes, the terms Internet
    and WWW (world wide web or just Web) are used interchangeably. This is a dangerous
    error. They are not the same. It’s like saying that Facebook and status updates
    are the same thing, which they are not because updating status is just one part
    of Facebook among dozens of other features, such as photos, apps, games, check-ins,
    etc.Using the same analogy, Web is just one aspect of the Internet. It is Internet,
    and not Web, that allows you to check emails, watch videos, send IM messages,
    make video calls, and a hundred other things. Each of the Internet’s “functionalities”
    is supported by a different protocol. Web uses HTTP, email uses IMAP/SMTP, video
    streaming uses RTSP, instant messaging uses Jabber, video calling uses VoIP, and
    so on. So, the next time you hear somebody say, “check the latest prices for this
    product on the Internet,” interrupt them politely by pointing out that they actually
    meant Web and not Internet.During its initial days, the Internet was composed
    only of computers, which were large and bulky (although not big-as-room bulky).
    Back then, the Internet connected computers with servers or with each other. There
    were few or no other devices that could be connected to the Internet. This changed
    with the introduction of smartphones. They were tiny, pocket-sized computers that
    could be connected to the Internet. While on Internet, they could do most things
    a regular computer could—browse websites, check email, send IM, etc. Then came
    tablets. And then, smart TVs. After that, smartwatches. You have probably witnessed
    the trend yourself. We even have smart thermostats, fridges, printers, cameras,
    music systems, cars, toasters (!), and so on. In other words, our Internet now
    has millions of these non-computer items connected to it (see Figure [1-4](#Fig4)).
    These non-computer items are the “things” in IoT. Each thing is individually called
    an IoT device.![A458845_1_En_1_Fig4_HTML.jpg](A458845_1_En_1_Fig4_HTML.jpg)Figure
    1-4This illustration shows many different things connected to the Internet
  prefs: []
  type: TYPE_NORMAL
- en: A More Technical Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IoT is the global network of “smart” versions of regular physical objects. What
    makes an object smart? Its ability to do its tasks automatically and more efficiently
    through the help of embedded computing hardware, sensors, actuators, and software.
    But the defining feature that makes an object smart is its ability to connect
    to the Internet, which opens a bunch of possibilities. For instance, a regular
    thermostat installed at home must be manually set with a desired temperature to
    control air conditioning. A smart thermostat can be remotely controlled via a
    mobile app (because it’s connected to the Internet) to make the house warmer when
    it starts snowing so that you get a cozy feeling once you reach home from the
    office.We’ll explore IoT devices in more detail in Chapter [2](A458845_1_En_2_Chapter.html),
    including the technologies that help in converting regular electrical and electronic
    items into IoT devices.
  prefs: []
  type: TYPE_NORMAL
- en: What’s the Use of IoT?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The smart thermostat  example illustrates an IoT device’s remotely controlling
    use case. The same use case can be extended to smart bulbs, smart fans, smart
    CCTVs, smart washing machines, etc., to collectively build a smart home . The
    devices that make a smart home are also called home automation systems/solutions.However,
    the most common use case of IoT devices is that of data collection. Most IoT devices
    have one or more sensors for measuring certain parameters. Smartwatches  come
    with a heart-rate sensor, a pedometer (to measure steps), barometer (to measure
    pressure), etc. Sensors continuously record and store data in devices. This data
    is then analyzed on a device or saved in the Cloud for later analysis. The result
    of analysis is a lot of meaningful information—average vitals during the day,
    the hour of maximum activity, total number of steps walked, total calories burned
    (as a result of walking/running), etc. Specialized wearables are sometimes used
    to keep track of patients’ health. So, another useful application of IoT is that
    of health monitors and remote patient monitoring systems.A smartwatch  is usually
    a complex, expensive IoT device. We may have simpler IoT devices that have only
    one purpose and, thus, one sensor. In a typical manufacturing plant, these could
    be a device to monitor room temperature, another device to monitor humidity, another
    one to record video, and yet another one to record the rotations of a running
    machine. The result is a connected factory, where the collected data can be analyzed
    to extract results to optimize the efficiency of machines and processes.You will
    learn more about capturing real-time data in Chapter [9](A458845_1_En_9_Chapter.html).
    Analyzing the captured data to generate insights is covered in Chapter [10](A458845_1_En_10_Chapter.html).Try
    to imagine more applications of IoT in the fields of agriculture, transportation,
    environment, and public infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Azure IoT Suite
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'IoT suite is a collection of services, from Microsoft, to manage small to large
    networks of IoT devices. The services are especially useful in large setups, where
    managing hundreds or thousands or even millions of IoT devices manually just isn’t
    possible. A typical setup, aka IoT solution, is composed of two major components:
    IoT devices and a solution backend. The Cloud-based solution backend is where
    data collected by devices is stored and analyzed. Azure IoT Suite provides end-to-end
    implementations for pre-configured or custom IoT solutions, such as remote monitoring
    (monitoring status of devices), predictive maintenance (anticipating maintenance
    needs of devices to avoid downtime), and connected factory (manage industrial
    devices).The suite usually goes with the following five Azure services:'
  prefs: []
  type: TYPE_NORMAL
- en: IoT Hub —Enables secure, bi-directional communication between IoT devices and
    solution backend. Azure IoT device SDKs for various languages and platforms are
    provided to enable devices to reliably connect with their solution backend. IoT
    Hub provides solutions for problems such as device identity management, device
    twins, per-device authentication, routing device-to-Cloud messages to Azure services,
    etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning —A fully-managed Cloud service that enables you to easily build,
    deploy, and share predictive analytics solutions. Machine Learning Studio is a
    browser-based, drag-and-drop authoring environment that allows you to create ML
    models for your analytics needs. It comes with support for R and Python, languages
    commonly used for creating statistical, predictive solutions. ML Studio also provides
    a fully managed service you can use to deploy your predictive models as ready-to-consume
    web services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stream analytics —Develop and run real-time analytics on data streams captured
    by IoT devices. Analytics programs are written in an SQL-like declarative language,
    with support for JavaScript user-defined functions for temporal logic. Parallel
    real-time analytics on multiple IoT streams is supported. It is also possible
    to call Azure Machine Learning models for predictive scoring on streaming data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notification hubs —A mobile push notification engine to send out notifications
    at scale about various IoT events. It’s a common requirement to receive notifications
    on mobile about job completion, regular monitoring updates, impending device failure,
    etc. Notification hubs can send out notifications to millions of mobiles at once.
    All popular mobile platforms are supported, including Android, iOS, and Windows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Power BI —A suite of business analytics and intelligence tools to create dashboards
    and charts using the results of analytics performed on data collected by IoT devices.
    Integrates well with stream analytics to create dashboards with real-time insights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will learn more the IoT suite and its services in Chapter [2](A458845_1_En_2_Chapter.html).
    In Chapter [3](A458845_1_En_3_Chapter.html), you will use Azure IoT Hub and other
    services with your own devices to create real-world connected networks.
  prefs: []
  type: TYPE_NORMAL
- en: Blockchain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you bought a new car. If, like me, you live in India, it is compulsory
    to buy car insurance before taking your new car out on the road. This is not just
    to protect your car against damages but also to provide for medical costs for
    a person who may get injured in an accident because of your driving. Anyway, let’s
    say you are a careful driver and even after a year of driving your new car you
    haven’t injured anyone. Then one fateful day, you were driving carefully as usual
    when another car comes speeding from nowhere and hits your car from the side.
    You are devastated. You had kept your car like new for all that time. And now,
    because of no fault of your own, your precious car has a dent as deep as a black
    hole.To make things worse, when you submit your claim, get bogged down by a complicated
    claims process and are eventually paid unfairly. The insurer says that they weren’t
    able to access whether the damage was the other guy’s fault or your own.Cases
    like this happen by the thousands on a daily basis around the world. Blockchain
    is the revolution that can change this forever. Not only do customers benefit
    but so do the insurers. Before you see how, let’s first understand what Blockchain
    is.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Blockchain?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the simplest of words, Blockchain is a document full of entries that are
    shared with a group of people and organizations. The entries can be anything related
    to people/organizations within the sharing group: items owned, money sent/received,
    items sold/bought, etc. For security purposes, the shared document is encrypted
    and verified to ensure the data it stores is always correct (true to everyone).
    As everyone in the group has access to the exact same entries, there is no need
    for a central authority—a bank, the police, a judge, or any other arbitrator—to
    validate a transaction between two people in the group. Everyone knows about the
    said transaction that occurred between those two people. Also, the entries are
    immutable: once created, they cannot be updated or deleted. They cannot be tampered
    with!In slightly more technical terms, Blockchain is a decentralized network—as
    seen in Figure [1-5](#Fig5)—where each transaction involving two or more members
    on the network is recorded on a block in a giant write-once ledger. Think of a
    block as a page in the ledger. In this sense, a ledger can be thought of as a
    chain of blocks. Everyone in the network has the same copy of the ledger. Each
    time a transaction takes place, it is added to the ledgers of the members involved.
    Then, quickly and securely, all other copies of the ledger on the network are
    updated to ensure that all copies are kept in sync. There is no “master” ledger.
    There are as many ledgers as there are members on the network.![A458845_1_En_1_Fig5_HTML.jpg](A458845_1_En_1_Fig5_HTML.jpg)Figure
    1-5A decentralized network of computers and mobile devices (collectively called
    “nodes”). Each node has a copy of the ledger.For the sake of emphasis, let us
    recap what we have learned so far about Blockchain:'
  prefs: []
  type: TYPE_NORMAL
- en: Blockchain is decentralized. There is no central authority to provide guarantees
    or validations for transactions that happen on the network. It’s the direct opposite
    of a bank that acts as a middleman for all financial transactions between two
    or more entities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nobody on a Blockchain network owns the ledger. Because of its decentralized
    nature, everyone on the network has a copy of the ledger.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Records in the ledger are immutable. This is to disallow fraudulent tampering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All ledger copies are always kept in sync. If everyone has the same information
    about each other, the risk of a fraud is nullified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blockchain provides a mechanism to verify each transaction through strong encryption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Can Blockchain Help?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may have heard about Blockchain in the context of the popular cryptocurrency
    Bitcoin  . The precursor to Blockchain was first described by Stuart Haber and
    W. Scott Stornetta, as early as 1991, in their paper “How to Time-Stamp a Digital
    Document,” published in the Journal of Cryptography. It was only in 2008 that
    the modern idea of Blockchain—or block chain, as it was originally called—was
    presented in the famous paper “Bitcoin: A Peer-to-Peer Electronic Cash System”
    by Satoshi Nakamoto published at [metzdowd.com](http://metzdowd.com) . Blockchain
    was described as the backend infrastructure on which the Bitcoin would work.NoteNo
    one really knows who invented Bitcoin and wrote the paper on it. Satoshi Nakamoto
    is a pseudonym used by an unknown person or a group of persons that invented Bitcoin,
    implemented the first Blockchain, and released the open source Bitcoin software.
    There have been multiple attempts to find the real identity of Nakamoto, including
    involvements from American agencies such as Department of Homeland Security and
    NSA. While some sources claim to have found the real person(s) behind the mysterious
    pseudonym, there is no consensus. To this day, Nakamoto remains a hidden figure
    potentially worth billions of U.S. dollars because of his or her initial stock
    of Bitcoins.Bitcoin  is a virtual, electronic currency. It is not issued by a
    bank, and no government or authority in the world endorses it. There are no official
    Bitcoin currency notes and physical coins. So, how does it work? Why does it have
    credibility? Let’s see. We learned in Blockchain’s simpler definition that it
    can be thought of as a big shared database of transactions. In the case of Bitcoin,
    its Blockchain network started off with a small amount of coins (money). These
    coins were then distributed among the network’s members. New coins can be introduced
    into the chain through a time- and resource-intensive process, aka Bitcoin mining.
    Each time coins are redistributed or added, a transaction is securely recorded
    in the Blockchain’s ledgers. So, if Person A gives Person B a certain amount of
    money, the transaction is replicated at each and every ledger. In effect, everyone
    on the network knows that Person A has given x money worth coins to Person B.
    As everyone knows about this transaction, there is no dispute. Person B cannot
    lie about getting money less than x, and Person A cannot lie about giving money
    more than x. The economics of demand and supply decide the real-world worth of
    a Bitcoin. As of writing this book, 1 Bitcoin (BTC) was valued over U.S. $4000!Now
    let’s apply the same logic to the car insurance situation we saw earlier. If there
    were a Blockchain network consisting of customers, insurers, repair shops, and
    hospitals, everyone on the network would know of your insurance contract with
    an insurer. In the case of a medical claim for the person you injured, the hospital
    can instantly check your insurance contract to quickly facilitate the claim. In
    the case of a car damage claim, chances of getting unfairly duped by the insurer
    because of a complicated process would reduce. With the help of an IoT device
    installed in your car, the insurer can find out various parameters during the
    accident such as speed, location, photo, etc. This would help the customer get
    fair claims. For the insurer, this would mean no exaggerated claims. As per a
    Deloitte report ( [https://www2.deloitte.com/content/dam/Deloitte/ch/Documents/innovation/ch-en-innovation-deloitte-blockchain-app-in-insurance.pdf](https://www2.deloitte.com/content/dam/Deloitte/ch/Documents/innovation/ch-en-innovation-deloitte-blockchain-app-in-insurance.pdf)
    ), some customers are known to indulge in a practice called “crash for cash,”
    in which deliberately cause an accident to make claims. IoT devices with accurate
    sensors would prevent such scenarios. Another thing an insurer is protected from
    is multiple claims fraud. Scammers buy insurance policies from multiple insurers
    under made-up identities. Through a staged crash, they make multiple claims against
    the same accident. Such frauds are currently difficult to detect, as data is not
    shared by different insurers. A Blockchain network where multiple insurers are
    present and where fraudulent identity management is enforced will make it easy
    to detect a multiple claims fraud.Blockchain is explained in further detail in
    Chapter [7](A458845_1_En_7_Chapter.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Azure Blockchain Solutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deploying a Blockchain network is not easy. Microsoft Azure makes it easy to
    develop, test, and deploy Blockchain applications through something called Blockchain
    as a Service (BaaS)  . Blockchain is an open concept, with several open source
    and commercial implementations. Some implementations are existing deployments
    that run a digital currency while others are offered as platforms that can be
    installed on one’s own infrastructure. Azure offers the latter implementations—Blockchain
    platforms—as part of BaaS. Each Blockchain platform or solution must be installed
    on one or more Linux or Windows virtual machine (VM) for your Blockchain network
    to come into effect. BaaS provides pre-configured VM templates to automate the
    task for installation and deployment. The tools and libraries required to develop
    applications for these Blockchain solutions are also installed along with the
    network.Some Blockchain solutions offered as part of BaaS  are:'
  prefs: []
  type: TYPE_NORMAL
- en: Ethereum Consortium
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: STRATO Blockchain LTS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chain Core Developer Edition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethereum Studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emercoin Blockchain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will learn about implementing one of these BaaS solutions in Chapter [8](A458845_1_En_8_Chapter.html).
  prefs: []
  type: TYPE_NORMAL
- en: It Is All About Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the revolutions we have talked about, the real hero is not Cloud, nor it
    is AI or IoT or Blockchain. These are only the enablers. What do they enable?
    That’s easy—data, which will always be the real hero in all important revolutions.
    Think about it. Data is what’s collected (IoT). Data is what’s analyzed (AI).
    Data (results) is what’s stored and propagated (Blockchain).
  prefs: []
  type: TYPE_NORMAL
- en: Why Is Data So Important?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first computer programs were written specifically to collect and analyze
    data—from recording weather data to making predictions to intercepting encrypted
    messages and using them to recognize patterns for code breaking, you may also
    be able to think of numerous other such examples.Most of the software that we
    create today also depends on data in one form or another. While some apps come
    preloaded with their data, a sizable number of them depend on data collected from
    users. And, usually, there is an end goal to be met or insights to be obtained
    using the collected data, which may vary from a few kilobytes to gigabytes to
    terabytes in size. Through each revolution, it’s always been our intention to
    put the data to use as intelligently as possible.Take for instance a simple TODO
    application. The whole point of its existence is to provide a means to store user’s
    notes (data) to help them remember things. A simple, static TODO app will do just
    that. A more sophisticated TODO app will continuously analyze your notes (and
    probably your emails, SMS, calendar, etc.) to offer you suggestions on what you
    may need reminding.A social network is built totally on user data. Your status
    updates, comments, likes, photos, and so on, make Facebook the lively place that
    it is. Why is Facebook—the nicest thing on planet for some—offered absolutely
    free of cost? It’s because of data: the data it collects from me, you, and millions
    of other people. Based on your activity (likes, location, friends, and apps) on
    the social network, it cleverly shows your promotions and advertisements, things
    that Facebook actually gets paid for by publishers and companies. The same goes
    for Twitter, Instagram, YouTube, Gmail, and more.A search engine may use search
    trends to offer customized ads. A travel app may share (at a cost) users’ booking
    and search data with airlines and hotels to allow them to offer better prices
    based on demand and supply. A maps app may collect users’ location data to offer
    real-time traffic predictions.Can you think of the importance of data in other
    domains, such as academia, research, health, stock markets, banking, education,
    and entertainment?'
  prefs: []
  type: TYPE_NORMAL
- en: How Data Collection Has Evolved
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two aspects of data collections—how it is collected, and where is
    it stored. Both these aspects have evolved immensely between the revolutions.
  prefs: []
  type: TYPE_NORMAL
- en: How Data Is Collected
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Early computer programs were not user-facing. They were used by academicians
    and researchers to solve scientific problems. When software became accessible
    to commercial and non-commercial organizations, data collection was largely a
    paper-and-pen based process. Thousands of paper-based applications and forms filled
    up by customers had to be manually fed into the system by following a manual,
    labor-intensive process.The Internet revolution brought about an explosion of
    user-facing applications. Through websites and desktop software, users and customers
    could themselves submit data. The smartphone revolution took this to an unimaginably
    new level. These smart pocket devices not only allowed users to explicitly submit
    data whenever and wherever they wanted, they also allowed collection of implicit
    user data—data collected without the intervention of the user: location, audio,
    images, etc.We are currently in the midst of IoT revolution that has taken implicit
    user data collection further. Smartwatches automatically collect data about various
    health parameters, smart thermostats automatically collect temperature data, and
    custom IoT devices can be built with sensors to collect the desired data from
    a user’s surroundings.'
  prefs: []
  type: TYPE_NORMAL
- en: Where Data Is Stored
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We started with files, the sort of text files that you create in Notepad. Logs,
    configuration data, and even user data was stored in text files. Writing to text
    files was fast and simple. Large amounts of data could be split among several
    files for convenience. Reading data from files was a challenge, though. As we
    know, data stored in unstructured or partially structured formats is difficult
    to search and use for analysis. This gave rise to spreadsheets and relational
    databases, where data could be stored in orderly formats made up of rows and columns.
    Storing and looking up large amounts of data became easy. They also made it easy
    to generate reports and insights. Major RDBMSs, such as SQL Server, offer reporting
    and intelligence functionalities such as SSRS and SSIS. The problem with RDBMSs
    is that they weren’t made to handle billions of records. That’s where NoSQL came
    into picture.NoSQL databases are non-relational and do not enforce a set schema.
    Popular options, such as MongoDB and Cassandra, can store massive amounts of data
    (billions of records) in a way that is exponentially faster to save and retrieve.
    They are especially useful for storing semi-structured or unstructured data, and
    are used in environments where performance and speed are of utmost priority. Unstructured
    data is the one that grows at a rapid pace, as opposed to structured (relational)
    data, and analyzed to derive useful insights. And then there is Hadoop, a key
    driver of the Big Data revolution.Hadoop is capable of storing massive amounts
    of unstructured and structured data. Additionally, it provides the means to perform
    data analytics on the stored massive amounts of data to generate reports and dashboards.Now
    we have Blockchain, a mechanism to openly and securely store large amounts of
    data that is available to each member on the network. Blockchain should not be
    thought of as a replacement to other data storage mechanisms discussed here. Its
    use is only with transactional data, and it is often used in conjunction with
    NoSQL and relational databases.
  prefs: []
  type: TYPE_NORMAL
- en: Smart Hospitals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this book, you will learn and use the technologies that make up AI
    2.0\. You will see how AI 2.0 can be used to build real-world solutions. For the
    purpose of this book, we’ll be focusing on the smart hospitals use case. Asclepius
    (pronounced ess-clip-ee-us) Consortium is our fictitious global network of 3+
    star-rated hospitals that want to work together to save more lives and make easier
    the lives of their patients through data sharing.We will build an AI 2.0 offering
    for the Asclepius Consortium that works in the following ways. All member hospitals
    and a bunch of insurance companies will be part of a dedicated Blockchain network.
    Medical records of patients across hospitals will be published on the Blockchain,
    including real-time patient monitoring data recorded through IoT devices. This
    will facilitate transparent sharing of patient data across hospitals and insurers.AI
    will facilitate real-time analytics on patient monitoring data, leading to better
    diagnoses. Patient data collected from across hospitals can also be centrally
    analyzed to generate results that can be later used to make life-saving predictions
    in the case of other patients.A network of such hospitals can keep track of all
    of medical inventory (equipment, machines, medicines, etc.) across hospitals.
    This will be helpful in the event of shortages of medical supplies.Along the same
    lines, it will be possible to create highly-accessible organ banks, including
    a blood bank and an eye bank.Transactions in which customers buy medical or life
    insurance from one of the insurers on this network will also be recorded. Availability
    of insurance records to hospitals will automate the claim process, making it simpler
    and time-saving. Availability of health records of patients to insurers will prevent
    potential frauds.
  prefs: []
  type: TYPE_NORMAL
- en: Recap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter introduced you to the concepts and technologies that you will
    use to create the next generation of software solutions. You learned about:'
  prefs: []
  type: TYPE_NORMAL
- en: The upcoming revolution in software development called AI 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various use cases involving AI 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basics and evolution of Artificial Intelligence (AI)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of the Internet of Things (IoT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simplistic understanding of Blockchain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of data in ongoing and upcoming revolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, you learn in detail about IoT in general and Azure’s enterprise
    IoT offerings.
  prefs: []
  type: TYPE_NORMAL
