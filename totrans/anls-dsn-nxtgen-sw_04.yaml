- en: © Springer Nature Switzerland AG 2020A. M. LangerAnalysis and Design of Next-Generation
    Software Architectures[https://doi.org/10.1007/978-3-030-36899-9_4](https://doi.org/10.1007/978-3-030-36899-9_4)
  prefs: []
  type: TYPE_NORMAL
- en: 4. Distributed Client/Server and Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Arthur M. Langer^([1](#Aff2) [ ](#ContactOfAuthor2))(1)Center for Technology
    Management, Columbia University, New York, NY, USAArthur M. LangerEmail: [al261@columbia.edu](mailto:al261@columbia.edu)'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Client/Server and Object-Oriented Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Client/Server provides another level of sophistication in the implementation
    of systems. The concept of Client/Server is based on distributed processing, where
    programs and data are placed in the most efficient places. Client/server systems
    are typically installed on Local Area Networks (LANs) or Wide Area Networks (WANs).
    LANs can be defined as multiple computers linked together to share processing
    and data. WANs are linked LANs. For purposes of this book, we will restrict our
    discussion about Client/Server within the concepts of application development
    moving to cloud and mobile environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can design effective Client/Server applications for mobility, the
    organization should commit to the object paradigm. Based on an OO implementation,
    Client/Server essentially requires one more step: the determination of what portions
    of an object or class should be moved to client only activities, server only activities,
    or both across vast mobile networks. Many existing client/server applications
    need to be expanded to operate in a much more distributed design and one that
    is not hierarchical.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Definition of Client/Server Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have already stated that Client/Server is a form of distributed processing.
    Client/Server applications have three components: a client, a server and a network.
    Setting aside the implications of the network for a moment, let us understand
    what clients and servers do. Although Client/Server applications tend to be seen
    as either permanent client or permanent server programs, we will see that this
    is not true in the object paradigm.'
  prefs: []
  type: TYPE_NORMAL
- en: A “server” is something that provides information to a requester. There are
    many Client/Server configurations that have permanent hardware servers. These
    hardware servers typically contain databases and application programs that provide
    services to requesting network computers (as well as other LANs). This configuration
    is called “back-end” processing. On the other hand, we have network computers
    that request the information from servers. We call these computers “clients” and
    categorize this type of processing as “front-end.” When we expand these definitions
    to applications only, we look at the behavior of an object or class and categorize
    it as client (requesting services), server (providing services), or both (providing
    and requesting services).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how objects become either permanent servers or clients is fairly
    straightforward. For example, the Cars object in the Car Transmission Types class
    is categorized as a server. If this were the only use of cars, then it would be
    called a “dedicated” server object. On the same basis, the Cars object in the
    Transportation Vehicles class is categorized as a client object. In turn, if it
    were the only use of the object in a class, it would be defined as a “permanent”
    client. However; because it exists in more than one class and is polymorphic,
    the Cars object is really both a client and a server, depending on the placement
    and behavior of the object. Therefore, when we talk about an object’s Client/Server
    behavior we must first understand the “instance” it is in and the class it is
    operating within.
  prefs: []
  type: TYPE_NORMAL
- en: The difficulty in Client/Server is in the further separation of attributes and
    services for purposes of performance across a network. This means that the server
    services and attributes components of the Cars object might need to be separated
    from the client ones and permanently placed on a physical server machine.
  prefs: []
  type: TYPE_NORMAL
- en: The client services and attributes will be then be stored on a different physical
    client machine(s). To put this point into perspective, an object may be further
    functionally decomposed based on processing categorization. Therefore, the analyst
    must be involved in the design of the network and must understand how the processing
    will be distributed across the network. Client/Server analysis should employ Rapid
    Application Development (RAD)^([1](#Fn1)) because both analysis and design are
    needed during the requirements phase of the system. Once the analyst understands
    the layout of the network, then further decomposition must be done to produce
    hybrid objects. These hybrid objects break out into dedicated server and object
    functions as shown below (Fig. [4.1](#Fig1)).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig1_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig1_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.1
  prefs: []
  type: TYPE_NORMAL
- en: Decomposition of Client/Server objects to dedicated client and server objects
  prefs: []
  type: TYPE_NORMAL
- en: Moving to Client/Server is much easier if OO has been completed. Getting the
    analysis team involved in network design early in the process is much more difficult.
    The role of the analyst in Client/Server will continue to expand as the distribution
    of objects in these environments continues to grow and mature.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Introduction to Databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chapter [3](480347_1_En_3_Chapter.xhtml) focused on application specifications
    as they relate to process. Using DFDs, PDFs, ERDs, etc., I showed how data elements
    are defined in the DD. However, the process of completing the DD and building
    complex relational databases has further steps. This chapter focuses on how to
    design databases for use with ecommerce Web applications. The completion of the
    DD and the creation of the database schematic, called the Entity Relational Diagram,
    provide developers with the data architecture component of the system. We call
    the process of creating this architecture Logic Data Modeling. The process of
    logic data modeling not only defines the architecture, it also provides the construct
    for the actual database, often called the physical database. The physical database
    differs from its logical counterpart in that it is subject to the rules and formats
    of the database product that will be used to implement the system. This means
    that if Oracle is used to implement the logical schema, the database must conform
    to the specific proprietary formats that Oracle requires. Thus, the logical model
    provides the first step in planning for the physical implementation. First, I
    will examine the process of building the appropriate schematic. Even if a packaged
    software product is selected, the chances are that it will need to use a database
    product like Oracle. Thus, many of the analysis and design below will be extremely
    important in determining the best fit for a package.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Logic Data Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logic Data Modeling (LDM) is a method that examines a particular data entity
    and determines what data elements need to be associated with it. There are a number
    of procedures, some mathematically based, to determine how and what the analyst
    needs to do. Therefore, LDM only focuses on the stored data with the intent to
    design what can be defined as the “engine” of the system. Often this “engine”
    is called the “backend.” The design of the engine must be independent from the
    process and must be based on the rules of data definition theory. Listed below
    are the eight suggested steps to build the database blueprint. This blueprint
    is typically called the schema, which is defined as a logical view of the database.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: identify data entities
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: select primary and alternate keys
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: determine key business rules
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: apply normalization to 3rd normal form
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: combine user views
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: integrate with existing data models (e.g., legacy interfaces)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: determine domains and triggering operations
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: de-normalize as appropriate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Prior to providing concrete examples, it is necessary to define the database
    terms used in this chapter. Below are the key concepts and definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Entity: an object of interest about which data can be collected. Larson and
    Larson ([2000](#CR2)) define an entity as “a representation of a real-world person,
    event, or concept.” For example, in an ecommerce application, customers, products,
    and suppliers might be entities. The chapter will provide a method of determining
    entities from the DFD. An entity can have many data elements associated with it,
    called attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attribute: data elements are typically called attributes when they are associated
    with an entity. These attributes, or cells of an entity, belong to or “depend
    on” the entity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Key: a key is an attribute of an entity that uniquely identifies a row. A row
    is defined as a specific record in the database. Therefore, a key is an attribute
    that has a unique value that no other row or record can have. Typical key attributes
    are “Social Security Number,” “Order Number,” etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Business Rule: this is a rule that is assumed to be true as defined by the
    business. Business rules govern the way keys and other processes behave within
    the database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Normalization: a process that eliminates data redundancy and ensures data integrity
    in a database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'User View: the definition of the data from the perspective of the user. This
    means that how a data element is used, what is its business rules, and whether
    it is a key or not, depends largely on the user’s definition. It is important
    that analysts understand that data definitions are not universal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Domains: this relates to a set of values or limits of occurrences within a
    data element or attribute of an entity. An example of a domain would be STATE,
    where there is a domain of 50 acceptable values (i.e., NY, NJ, CA, etc.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Triggers: these are stored procedures or programs that are activated or triggered
    as a result of an event at the database level. In other words, an event (insert,
    delete, update) may require that other elements or records be changed. This change
    would occur by having a program stored by the database product (such as Oracle)
    automatically execute and update the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cardinality: this concept defines the relationship between two entities. This
    relationship is constructed based on the number of occurrences or associations
    that one entity has with another. For example, one customer record could have
    many order records. In this example, both customer and orders are separate entities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Legacy Systems: these are existing applications that are in operation. Legacy
    applications sometimes refer to older and less sophisticated applications that
    need to be interfaced with newer systems or replaced completely (see Chap. [10](480347_1_En_10_Chapter.xhtml)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Entity Relational Diagram: a schematic of all the entities and their relationships
    using cardinal format. An entity relational diagram provides the blueprint of
    the data, or the diagram of the data engine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.5 Logic Data Modeling Procedures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in LDM is to select the entities that will be used to start the
    normalization process. If DFDs have been completed in accordance with the procedures
    outlined in Chap. [3](480347_1_En_3_Chapter.xhtml), then all data stores that
    represent data files become transformed into data entities. This approach offers
    the major advantage of modeling process before data. If DFDs or some comparable
    process tool is not used, then analysts must rely on the information they can
    obtain from the legacy systems, such as existing data files, screens, and reports.
    The following example depicts how a data store from a DFD becomes an entity. The
    data contained in the data store called “Orders” is represented as an actual form
    containing many data elements (Fig. [4.2](#Fig2)). Thus, this example represents
    a physical form translated into an LE called a data store, which then is transformed
    again into an entity (Fig. [4.3](#Fig3)).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig2_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig2_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.2
  prefs: []
  type: TYPE_NORMAL
- en: Sample customer order form
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig3_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig3_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4.3
  prefs: []
  type: TYPE_NORMAL
- en: Transition of the order data store into an Entity
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Key Attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step in LDM is to select the primary and alternate keys. A primary
    key is defined as an attribute that will be used to identify a record or occurrence
    in an entity. The primary key, like any key attribute contains a unique value.
    Often there is more than one attribute in an entity that contains unique values.
    We call an attribute that can be a primary key a candidate key attribute. This
    simply means that this attribute can serve in the role of the primary key. If
    there is only one candidate, then there is no issue: that candidate becomes the
    primary key. In the event that there is more than one candidate attribute, then
    one must be selected the primary key, and the others will be called alternate
    or secondary key attributes. These alternate key attributes provide benefit in
    the physical database only. This means that they can be used to identify records
    in the database as an alternative should the primary key not be known. Take the
    following example. Suppose that an employee entity has two candidate keys: Social-Security-Number
    and Employee-ID. Employee-ID is selected as the primary key, so Social-Security-Number
    becomes an alternate key. In the logical entity, Social-Security-Number is treated
    as any other non-key attribute; however, in the physical database, it can be used
    (or indexed) to find a record. This could occur when an employee calls to ask
    someone in Human Resources about accrued vacation time. The Human Resource staff
    would ask the employee for their Employee-ID. If the employee did not know his/her
    Employee-ID, the Human Resource staff could ask them for their Social Security
    Number, and use that information as an alternative way to locate that individual’s
    information. It is important to note that the search on the primary key will be
    substantially faster, because primary key searches use a method called direct
    access, as opposed to index methods, which are significantly slower. This raises
    the question: When there are multiple candidate-key attributes, which key attribute
    should be selected as the primary key? The answer is the attribute that will be
    used most often to find the record. This means that Employee-ID was selected the
    primary key attribute because the users determined that it was the field most
    often used to locate employee information. Therefore, ecommerce analysts must
    ensure that they ask users this question during the interview process. Figure [4.4](#Fig4)
    provides a graphic depiction of the employee entity showing Employee-ID as the
    primary key attribute and Social-Security-Number as a non-key attribute.![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig4_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig4_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.4
  prefs: []
  type: TYPE_NORMAL
- en: Primary key and alternate key attributes
  prefs: []
  type: TYPE_NORMAL
- en: There is another type of key attribute called Foreign keys. Foreign keys provide
    a way to link tables and create relationships between them. Since foreign keys
    are created during the process of Normalization, I will defer discussion about
    them to the section on Normalization in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the next step in LDM is to determine key business rules, it is easier
    to explain the process of Normalization first. That is, Normalization occurs after
    Defining Key Business Rules in practice, but not when introducing the topic for
    educational purposes. Therefore, Key Business Rules will be discussed after Normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization, without question, is the most important aspect of LDM. As mentioned
    above, Normalization is defined as the elimination of redundancies in an entity
    and ensures data integrity. It is the latter point that is critical in understanding
    the value of Normalization in the design of ecommerce database systems. Understanding
    of the LDM process depends largely on understanding how to implement the Normalization
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization is constructed in a number of “Normal Forms.” While there are
    five published Normal Forms, Normal Forms 4 and 5 are difficult to implement and
    most professionals avoid them. Therefore, this book omits Normal Forms 4 and 5\.
    The three Normal Forms of Normalization are listed below. Note that a Normal Form
    is notated as “NF.”
  prefs: []
  type: TYPE_NORMAL
- en: '1st NF: No repeating non-key attributes or group of non-key attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2nd NF: No partial dependencies on a part of a concatenated key attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3rd NF: No dependencies of a non-key attribute on another non-key attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each Normal Form is dependent on the one before it, that is, the process of
    completing Normalization is predicated on the sequential satisfaction of the Normal
    Form preceding it. Normalization can be best explained by providing a detailed
    example. Using the Order form provided in Fig. [4.1](#Fig1), we can start the
    process of Normalization. Figure [4.5](#Fig5) shows the Logical Equivalent of
    the Order form in entity format. In this example, the primary key is Order-Number
    (signified by the “PK” notation), which requires that every order have a unique
    Order-Number associated with it. It should also be noted that a repeating group
    made up of five attributes is shown in a separate box. This repeating group of
    attributes correlates to an area on the Order form, which often is referred to
    as an order line item. This means that each item associated with the order appears
    in its own group, namely the item identification, its name, unit price, quantity,
    and amount. The customer order in Fig. [4.1](#Fig1) shows two items associated
    with the Order-Number 12345.![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig5_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig5_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.5
  prefs: []
  type: TYPE_NORMAL
- en: Orders entity and its associated attributes
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of determining compliance with Normalization is to evaluate whether
    each normal form or NF has been satisfied. This can be accomplished by testing
    each NF in turn. Thus, the first question to ask is: Are we in 1st NF? The answer
    is no because of the existence of the repeating attributes: Item-ID, Item-Name,
    Quantity, Unit-Price, and Amount, or as specified above an “order line item.”
    In showing this box, the example exposes the repeating group of items that can
    be associated with a customer order. Another way of looking at this phenomenon
    is to see that within the Order, there really is another entity, which has its
    own key identification. Since there is a repeating group of attributes, there
    is a 1st NF failure. Anytime an NF fails or is violated, it results in the creation
    of another entity. Whenever there is a 1st NF failure, the new entity will always
    have as its primary key a concatenated “group” of attributes. This concatenation,
    or joining of multiple attributes to form a specific value, is composed of the
    primary key from the original entity (Orders) attached with a new key attribute
    from the repeating group of elements. The new key must be an attribute that controls
    the other group of attributes. In this example, the controlling attribute is Item-ID.
    After the new “key attribute” is determined, it is concatenated with the original
    key attribute from the Orders entity. The remaining non-key attributes will be
    removed from the original entity to become non-key attributes of the new entity.
    This new entity is shown in Fig. [4.6](#Fig6).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig6_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig6_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.6
  prefs: []
  type: TYPE_NORMAL
- en: Orders in 1st NF
  prefs: []
  type: TYPE_NORMAL
- en: The new entity, called Order Items, has a primary key that reflects the concatenation
    of the original key Order-Number from the entity Orders, combined with Item-ID,
    which represents the controlling attribute for the repeating group. All of the
    other repeating attributes have now been transferred to the new entity. The new
    entity Order Items allows the system to store multiple order line items as required.
    The original entity left without this modification would have limited the number
    of occurrences of items artificially. For example, if the analyst/designer had
    defaulted to five groups of order line items, the database would always have five
    occurrences of the five attributes. If most orders, in reality, had fewer than
    five items, then significant space would be wasted. More significant is the case
    where the order has more than five items. In this case, a user would need to split
    the order into two physical orders so that the extra items could be captured.
    These two issues are the salient benefits of attaining entities in their 1st NF.
    Therefore, leaving the entity Order as is would in effect create an integrity
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Once the changes to the entity Orders has been completed, and the new entity
    Order Item has been completed, the system is said to be a database in 1st NF.
    It is important to note that the new primary key of the entity Order Items is
    the combination of two attributes. While the two attributes maintain their independence
    as separate fields of data, they are utilized as one combined value for purposes
    of their role as a key attribute. For example, based on the data in the Order
    form from Fig. [4.1](#Fig1), the entity Order Items would have two records. The
    first record would have the primary key of 1234531, which would be the concatenation
    of Order-Number (12345) with Item-ID (31). The second record would be 1234527,
    which is the same Order-Number, but concatenated with the second Item-ID (27).
    From an SQL feature perspective, while the key attribute concatenates each attribute
    into one address, it can be searched as separate fields. So, a user could search
    for all the items associated with Order 12345, by simply searching on Order Items
    that contain an Order-Number = “12345.” This exemplifies the power of versatility
    in the relational model. Once 1st NF has been reached, the next test must ensue,
    that is, testing for compliance with 2nd NF.
  prefs: []
  type: TYPE_NORMAL
- en: Second NF testing applies only to entities that have concatenated keys. Therefore,
    any entity that is in 1st NF and does not have a concatenated primary key must
    already be in 2nd NF. In our example, then, the entity “Orders” is already in
    2nd NF because it is in 1st NF and does not have a concatenated primary key. The
    entity Order Items, however, is in a different category. Order Items has a concatenated
    primary key attribute and must be tested for compliance with 2nd NF. Second NF
    requires the analyst to ensure that every non-key attribute in the entity is totally
    dependent on all components of the primary key, or all of its concatenated attributes.
    When we apply the test, we find that the attribute “Item-Name” is dependent only
    on the key attribute “Item-ID.” That is, the Order-Number has no effect or control
    over the name of the item. This condition is considered a 2nd NF failure. Once
    again, a new entity must be created. The primary key of the new entity is the
    portion of the concatenated key that controlled the attribute that caused the
    failure. In other words, Item-ID is the primary key of the new entity, because
    “Item-Name” was wholly dependent on the attribute “Item-ID.” It is worthwhile
    at this time to explain further the concept of attribute dependency. For one attribute
    to be dependent on another infers that the controlling attribute’s value can change
    the value of the dependent attribute. Another way of explaining this is to say
    that the controlling attribute, which must be a key, controls the record. That
    is, if the Item-ID changes, then we are looking at a different Item Name, because
    we are looking at a different Item record.
  prefs: []
  type: TYPE_NORMAL
- en: To complete the creation of the new entity, Items, each non-key attribute in
    the original entity Order Items must be tested for 2nd NF violation. Note that
    as a result of this testing, “Quantity” and “Amount” stay in the Order Items entity
    because they are dependent on both Order-Number and Item-ID. That is, the quantity
    associated with any given Order Items occurrence is dependent not only on the
    Item itself, but also the particular order it is associated with. We call this
    being wholly dependent on the concatenated primary key attribute. Thus, the movement
    of non-key attributes is predicated on the testing of each non-key attribute against
    the concatenated primary key. The result of this test establishes the three entities
    shown in Fig. [4.7](#Fig7).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig7_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig7_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.7
  prefs: []
  type: TYPE_NORMAL
- en: Orders in 2nd NF
  prefs: []
  type: TYPE_NORMAL
- en: The results of implementing 2nd NF reflect that without it, a new Item (or Item-ID)
    could not have been added to the database without an order. This obviously would
    have caused major problems. Indeed, the addition of a new Item would have to precede
    the creation of that Item with a new Order. Therefore, the new entity represents
    the creation of a separate Item master file as shown in Fig. [4.7](#Fig7).
  prefs: []
  type: TYPE_NORMAL
- en: Figure [4.7](#Fig7) represents Orders in 2nd NF. Once again, we must apply the
    next test-3rd NF to complete Normalization. Third NF tests the relationship between
    two non-key attributes to ensure that there are no dependencies between them.
    Indeed, if this dependency were to exist, it would mean that one of the non-key
    attributes would, in effect, be a key attribute. Should this occur, the controlling
    non-key attribute would become the primary key of the new entity. Testing this
    against the sample entity reflects that Customer-Name and Customer-Address^([2](#Fn2))
    are dependent on Customer-ID. Therefore, the entity Orders fails 3rd NF and a
    new entity must be created. The primary key of the new entity is the non-key attribute
    that controlled the other non-key attributes; in this case Customer-ID. The new
    entity is called Customers, and all of the non-key attributes that depend on Customer-ID
    are moved to that entity as shown in Fig. [4.8](#Fig8).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig8_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig8_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.8
  prefs: []
  type: TYPE_NORMAL
- en: Orders in preliminary 3rd NF
  prefs: []
  type: TYPE_NORMAL
- en: 'What is unique about 3rd NF failures is that the new key attribute remains
    as a non-key attribute in the original entity (in this case: Orders). The copy
    of the non-key attribute Customer-ID is called a foreign key and is created to
    allow the Order entity and the new Customer entity to have a relationship. A relationship
    between two entities can exist only if there is at least one common keyed attribute
    between them. Understanding this concept is crucial to what Normalization is intended
    to accomplish. Looking at Fig. [4.8](#Fig8), one can see that the entity Order
    and Order Items have a relationship because both entities have a common keyed
    attribute: Order-ID. The same is true in the creation of the Item entity, which
    resulted from a 2nd NF failure. The relationship here is between the Order Item
    entity and the Item entity, where both entities contain the common key attribute
    Item-ID. Both of these relationships resulted from the propagation of a key attribute
    from the original entity to the newly formed entity during the normalization process.
    By propagation, we mean that a pointer, or copy of the key attribute is placed
    in the new entity. Propagation is implemented using foreign keys and is a natural
    result of the process. Note that the “PK” is followed by an “FK” signifying that
    the keyed attribute is the result of a propagation of the original key attribute.
    Such is not the case in 3rd NF. If Customer-ID were to be removed from the Orders
    entity, then the relationship between Orders and Customers would not exist because
    there would be no common keyed attribute between the two entities. Therefore,
    in 3rd NF, it is necessary to force the relationship because a natural propagation
    has not occurred. This is accomplished by creating a pointer from a non-keyed
    attribute to the primary keyed copy, in this case Customer-ID. The concept of
    a pointer is important. Foreign key structures are typically implemented internally
    in physical databases using indexes. Indexes, or indirect addresses, are a way
    of maintaining database integrity by ensuring that only one copy of an attribute
    value is stored. If two copies of Customer-ID were stored, changing one of them
    could create an integrity problem between Orders and Customers. The solution is
    to have the Customer-ID in Orders “point” indirectly to the Customer-ID key attribute
    in the Customer entity. This ensures that a Customer-ID cannot be added to the
    Orders entity that does not exist in the Customer master entity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The question now is whether the entities are in 3rd NF. Upon further review,
    we see the answer is no! Although it is not intuitively obvious, there are three
    non-key attributes that are dependent on other non-key attributes. This occurs
    first in the Order Items entity. The non-key attribute “Amount” is dependent on
    the non-key attribute “Quantity.” Amount represents the total calculated for each
    item in the order. It is not only dependent on “Quantity,” but also dependent
    on “Unit-Price.” This occurs frequently in attributes that are calculations. Such
    attributes are called derived elements, and are eliminated from the database.
    Indeed, if we store Quantity and Unit-Price, “Amount” can be calculated separately
    as opposed to being stored as a separate attribute. Storing the calculation would
    also cause integrity problems. For example, what would happen if the quantity
    or unit price would change? The database would have to recalculate the change
    and update the Amount attribute. While this can be accomplished, and will be discussed
    later in this chapter, it can be problematic to maintain in the database and cause
    performance problems in production ecommerce systems. The Orders entity also contains
    two derived attributes: Subtotal and Total-Due. Again, both of these attributes
    are removed. The issue is whether the removal of derived attributes should be
    seen as a 3rd NF failure. Date ([2000](#CR1)) views these failures as outside
    of 3rd NF, but in my view, they represent indirect dependencies on other non-key
    attributes and should be included as part of the 3rd NF test. In any case, we
    all agree that derived elements should be removed in the process of LDM. The 3rd
    NF LDM is modified to reflect the removal of these three attributes as shown in
    Fig. [4.9](#Fig9).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig9_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig9_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.9
  prefs: []
  type: TYPE_NORMAL
- en: Orders in second phase of 3rd NF
  prefs: []
  type: TYPE_NORMAL
- en: Once 3rd NF is reached the analyst should create the Entity Relational Diagram
    (ERD), which will show the relationships or connections among the entities. The
    relationship between entities is established through associations. Associations
    define the cardinality of the relationship using what is known as the Crow’s Foot
    Method as shown in Fig. [4.10](#Fig10).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig10_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig10_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.10
  prefs: []
  type: TYPE_NORMAL
- en: The entities in ERD format using crow’s Feet
  prefs: []
  type: TYPE_NORMAL
- en: 'The Crow’s Foot Method is only one of many formats. The method contains three
    key symbols:'
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/480347_1_En_4_Chapter/480347_1_En_4_Figa_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Figa_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: denotes the cardinality of many occurrences
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/480347_1_En_4_Chapter/480347_1_En_4_Figb_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Figb_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: denotes zero occurrences
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/480347_1_En_4_Chapter/480347_1_En_4_Figc_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Figc_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: denotes one occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the ERD in Fig. [4.10](#Fig10) depicts the relationships of all
    the entities as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One, and only one (signified by the double lines) Order record can have one
    to many Order Item records. It also shows that any Order in the Order Items entity
    must exist in the Order entity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One and only one Item record can have zero to many Order Item records. The difference
    in this relationship and the one established between Orders and Order Items is
    that Items may not have a relationship with Order items, signified by the zero
    in the Crow’s Foot. This would often occur when there is a new item that has not
    yet received any orders.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Order Items entity has a primary key, which is a concatenation of two other
    primary keys: Order-ID from the Orders entity, and Item-ID from the Items entity.
    This type of relationship is said to be an “associative” relationship because
    the entity has been created as a result of a relational problem. This relational
    problem exists because the Order entity has a “many-to-many” relationship with
    the Items entity. Thus the 1st NF failure, which created the associative entity
    Order Items, is really the result of a “many-to-many” situation. A many-to-many
    relationship violates Normalization because it causes significant problems with
    SQL coding. Therefore, whenever a many-to-many relationship occurs between two
    entities, an associative entity is created which will have as its primary key
    the concatenation of the two primary keys from each entity. Thus, associative
    entities make many-to-many relationships into two one-to-many relationships so
    that SQL can work properly during search routines. Associative entities are usually
    represented with a diamond box.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One and only one Customer can have zero-to-many Orders, also showing that a
    Customer may exist who has never placed an order. As an example, this would be
    critical if the business were credit cards, where consumers can obtain a credit
    card even though they have not made a purchase. Note that the Customer-ID is linked
    with Orders through the use of a non-key foreign key attribute.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.8 Limitations of Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although 3rd NF has been attained, there is a major problem with the model.
    The problem relates to the attribute Unit-Price in the Items entity. Should the
    Unit-Price of any Item change, then the calculation of historical Order Item purchases
    would be incorrect. Remember that the attribute “Amount” was eliminated because
    it was a derived element. This might suggest that Normalization does not work
    properly! Such is not the case. First, we need to evaluate whether putting “Amount”
    back in the ERD would solve the problem. If the Unit-Price were to change, then
    Amount would need to be recalculated before it was done. While this might seem
    reasonable, it really does not offer a solution to the problem, just a way around
    it. The actual problem has little to do with the attribute “Amount”, but more
    to do with a missing attribute. The missing attribute is Order-Item-Unit-Price,
    which would represent the price at the time of the order. Order-Item-Unit-Price
    is dependent on both the Order and the Item and therefore would become a non-key
    attribute in the Order Items entity (i.e., it is wholly dependent on the entire
    concatenated primary key). The only relationship between Unit-Price and Order-Item-Unit-Price
    is at the time the order is entered into the system. In this situation, an application
    program would move the value or amount of the Unit-Price attribute into the Order-Item-Unit-Price
    attribute. Thereafter, there is no relationship between the two attributes. Because
    this is a new data element that has been discovered during Normalization, it must
    be entered into the Data Dictionary. Thus, a limitation of Normalization is that
    it cannot normalize what it does not have; it can normalize only the attributes
    that are presented to the formula. However, the limitation of Normalization is
    also an advantage: the process can help the analyst recognize that a data element
    is missing. Therefore, Normalization is a “data-based” tool that the analyst can
    use to reach the Logical Equivalent. Figure [4.11](#Fig11) shows the final ERD
    with the addition of Order-Item-Unit-Price.![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig11_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig11_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.11
  prefs: []
  type: TYPE_NORMAL
- en: Final ERD with Order-Item-Unit-Price
  prefs: []
  type: TYPE_NORMAL
- en: 4.9 The Supertype/Subtype Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A troublesome database issue occurs in the LDM when there are records within
    an entity that can take on different characteristics or have many “types” of attributes.
    “Type” means that a portion of the attributes in a specific record can vary depending
    on the characteristic or identification of the row within that entity. Another
    way of defining type is to describe it as a group of attributes within a given
    record that are different from other records of the same entity depending on the
    type of record it represents. This type is referred to as a “subtype” of the record.
    A subtype, therefore, is the portion of the record that deviates from the standard
    or “supertype” part of the record. The “supertype” portion is always the same
    among all the records in the entity. In other words, the “supertype” represents
    the global part of the attributes in an entity. The diagram below in Fig. [4.12](#Fig12)
    depicts the supertype/subtype relationship.![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig12_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig12_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.12
  prefs: []
  type: TYPE_NORMAL
- en: Supertype/subtype relationship
  prefs: []
  type: TYPE_NORMAL
- en: The difference between a subtype and an ordinary type identifier (using a foreign
    key) is the occurrence of at least one non-key attribute that exists only in that
    subtype record. The major reason to create a supertype/subtype relationship is
    the occurrence of multiple permutations of these unique attributes that exist
    in just certain subtype records. Limiting these permutations of attributes within
    one record format can be problematic. First, it can waste storage, especially
    if each subtype has significant numbers of unique attributes. Second, it can create
    significant performance problems particularly with the querying of data. Using
    Fig. [4.12](#Fig12), we can see two ways to store this data. The first (Fig. [4.13](#Fig13))
    is a basic representation where all the permutations exist in one entity called
    “Educators.” The “type” of row is identified by using a foreign key pointer to
    a validation entity called “Educator Type.”![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig13_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig13_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.13
  prefs: []
  type: TYPE_NORMAL
- en: Educator ERD using Foreign Key Identifier
  prefs: []
  type: TYPE_NORMAL
- en: Although this representation of the data uses only one entity, it wastes storage
    space because all of the attributes of the entity are never needed by any one
    “type” of record. Furthermore, a user must know which attributes need to be entered
    for a particular type of record. This method of logic data modeling violates the
    concepts of normalization, and entrusts the integrity of values of elements in
    an entity to either an application program’s control (stored procedure), or to
    the memory of the user. Neither of these choices is particularly dependable or
    has proven to be a reliable method of data integrity.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Fig. [4.14](#Fig14) provides a different solution using the
    Supertype/Subtype model.![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig14_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig14_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.14
  prefs: []
  type: TYPE_NORMAL
- en: Educator entity supertype/subtype model
  prefs: []
  type: TYPE_NORMAL
- en: This model constructs a separate entity for each type of educator, linked via
    a special notation in the relational model, known as the supertype/subtype relation.
    The relationship is mutually exclusive, meaning that the supertype entity Educator
    can have only one of the three subtypes for any given supertype occurrence. Therefore,
    the relationship of one record in a supertype must be one-to-one with only one
    subtype. The supertype/subtype model creates a separate subtype entity to carry
    only the specific attributes unique to its subtype.
  prefs: []
  type: TYPE_NORMAL
- en: There are two major benefits to this entity structure. First, the construct
    saves storage because it stores only the attributes it needs in each entity. Second,
    the subtype information can be directly addressed without accessing its related
    supertype. This is possible because each subtype entity contains the same primary
    key as its parent. This capability is significant because a user can automatically
    obtain the unique information from any subtype without having to search first
    through the supertype entity. This is particularly beneficial when the number
    of records in each subtype varies significantly. Suppose, for example, there are
    6 million educators in the database. The Educator database would therefore contain
    6 million rows. Let’s say that 5 million of the educators are high school teachers,
    and as such, the High School subtype entity has 5 million records. Eight hundred
    thousand educators are professors, and the remaining 200,000 educators are deans;
    therefore, the Professor database and Dean database have 800,000 and 200,000 records,
    respectively. Using the supertype/subtype model applications could access each
    subtype without searching through every record in the database. Furthermore, because
    access to one subtype does not affect the other, performance is greatly improved.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the Supertype/Subtype model is not limited to mutual
    exclusivity, that is, it can support multiple subtype permutations. For example,
    suppose an educator could be a high school teacher, college professor, and a dean
    at the same time, or any permutation of the three types. The sample model would
    then be modified to show separate one-to-one relationships as opposed to the “T”
    relationship shown in Fig. [4.14](#Fig14). The alternative model is represented
    in Fig. [4.15](#Fig15).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig15_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig15_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.15
  prefs: []
  type: TYPE_NORMAL
- en: Supertype/subtype model without mutual exclusivity
  prefs: []
  type: TYPE_NORMAL
- en: Supertype/Subtypes can cascade, that is, they can continue to iterate or decompose
    within each subtype. This is represented in Fig. [4.16](#Fig16).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig16_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig16_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.16
  prefs: []
  type: TYPE_NORMAL
- en: Cascading subtypes
  prefs: []
  type: TYPE_NORMAL
- en: Notice that in the above example the same primary key continues to link the
    “one-to-one” relationships between the entities. In addition, Fig. [4.16](#Fig16)
    also shows another possibility in the supertype/subtype model. This possibility
    reflects that a subtype can exist without containing any non-key attributes. This
    occurs in the example in the subtype entity Adjunct Prof. The “empty” entity serves
    only to identify the existence of the subtype, without having a dedicated non-key
    attribute associated with it. The Adjunct Prof entity, therefore, is created only
    to allow the other two subtypes (Tenured Prof and Contract Prof) to store their
    unique attributes. This example shows how supertype/subtype models can be constructed,
    and how they often have subtypes that are created for the sole purpose of identification.
  prefs: []
  type: TYPE_NORMAL
- en: Cascading subtypes can mix methods, that is, some levels may not be mutually
    exclusive, while other cascade levels can be mutually exclusive as shown in Fig. [4.17](#Fig17).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig17_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig17_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.17
  prefs: []
  type: TYPE_NORMAL
- en: Cascading subtypes with alternating exclusivities
  prefs: []
  type: TYPE_NORMAL
- en: There is a controversial issue among database developers. The controversy relates
    to whether it is necessary to create a special attribute that identifies which
    entity contains the subtype entry for any given supertype. In other words, how
    does the database know which subtype has the continuation information? This dilemma
    is especially relevant when mutually exclusive relationships exist in the supertype/subtype.
    The question is ultimately whether the supertype/subtype model needs to contain
    an identifier attribute that knows which subtype holds the continuation record,
    or is the issue resolved by the physical database product? Fleming and von Halle
    addressed this issue in the Handbook of Database Design, where they suggest that
    the “attribute is at least partially redundant because its meaning already is
    conveyed by the existence of category or subtype relationships” (p. 162). Still,
    the issue of redundancy may vary among physical database products. Therefore,
    I suggest that the logical model contain a subtype identifier for mutually exclusive
    supertype/subtype relationships as shown in Fig. [4.18](#Fig18).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig18_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig18_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.18
  prefs: []
  type: TYPE_NORMAL
- en: Supertype/subtype with subtype identifier element
  prefs: []
  type: TYPE_NORMAL
- en: Note that the above example has the subtype identifier, Professor Types as a
    validation entity in 3rd normal form.
  prefs: []
  type: TYPE_NORMAL
- en: Supertype/subtypes must also be normalized following the rules of Normalization.
    For example, the subtype Educator Types contains elements that are not in 3rd
    NF. Attributes Grade_Level and Subject in the subtype entity High School Teacher
    can be validated using a look-up table. Department, School, and PhD_Subject can
    also be validated. The resulting 3rd NF ERD is shown below in Fig. [4.19](#Fig19).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig19_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig19_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.19
  prefs: []
  type: TYPE_NORMAL
- en: Supertype/subtype in 3rd NF
  prefs: []
  type: TYPE_NORMAL
- en: 4.10 Key Business Rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Key business rules are the rules that govern the behavior between entities
    when a row is inserted or deleted. These business rules are programmed at the
    database level using stored procedures and triggers (see Step 7: determine domains
    and triggering operations). These procedures are typically notated as constraints.
    Constraints enforce the key business rules that will be defined by the analysts
    and are the basis of what is meant by referential integrity, that is, the integrity
    based on the relations between tables. The process of insertion and deletion focuses
    on the relationship between the parent entity and the child entity. A child entity
    is always the entity that has the Crow’s Foot pointing to it. Based on the ERD
    in Fig. [4.10](#Fig10) the parent-child entity relationships are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Orders entity is the parent of Order Items entity (child).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer entity is the parent of Orders entity (child).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Items entity is the parent of Order Items entity (child).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When discussing insertion of a row, it is always from the perspective of the
    child entity. That is, key business rules governing the insertion of a child record
    concern what should be done when attempting to insert a child record that does
    not have a corresponding parent record. There are six alternatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Not Allowed: this means that the constraint is to disallow the transaction.
    For example, in Fig. [4.11](#Fig11), a user could not insert an Order Item (child)
    for an Order-Number that did not exist in the Orders entity (parent). Essentially,
    the integrity of the reference would be upheld, until the Order-Number in the
    Orders entity was inserted first.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add Parent: this means that if the parent key does not exist, it will be added
    at the same time. Using Fig. [4.11](#Fig11), this would mean that the user would
    be prompted to add the Order-Number to the Orders entity before the child Item
    would be inserted. The difference between Not Allowed and Add Parent is that the
    user can enter the parent information during the insertion of the child transaction.
    Using this rule still enforces referential integrity.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Default Value: the use of a default value allows a “dummy” row to be inserted
    into the parent. An example of the use of a default occurs when collection agencies
    receive payments from an unknown person. The parent entity is “Account” and the
    child entity is “Payments.” The Account entity would have a key value called “Unapplied”
    which would be used whenever an unidentified payment was collected. In this scenario,
    it is appropriate to have the dummy record because the child transaction is really
    unknown, but at the same time needs to be recorded in the database. It is also
    useful because the user can quickly get a list of “unapplied” payments and it
    upholds referential integrity.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Algorithm: an algorithm is an “intelligent” default value. Using the same example
    as (3), suppose the user wanted to track unapplied payments by State. For example,
    if an unapplied payment were received in New York, the parent (Account entity)
    would have a record inserted with a value “Unapplied-New York.” Therefore, each
    State would have its own default. There are also default keys that are based on
    sophisticated algorithms to ensure that there is an understanding to the selection
    of the parent’s key attribute value. Again, this selection ensures referential
    integrity because a record is inserted at both the parent and child entities.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Null: assigning a null means that the parent does not exist. Most database
    products such as Oracle allow such selection, and while it is maintained within
    the product, it violates referential integrity because the parent is unknown.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Don’t Care: this essentially says that the user is willing to accept that referential
    integrity does not exist in the database. The user will tell you that they never
    wish to balance the records in the child with those in the parent. While this
    happens, it should be avoided, because it creates a system without integrity.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When discussing deletion of a row, it is always from the perspective of the
    parent entity. That is, key business rules governing the deletion of a parent
    record concern what should be done when attempting to delete a parent record that
    has corresponding child records. There are similarly six alternatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Not Allowed: this means that the constraint is to disallow the deletion of
    the parent record. In other words, if there are children records, the user cannot
    delete the parent. For example, if in Fig. [4.11](#Fig11), a user could not delete
    an Order (parent) if there were corresponding records in the Order Items entity
    (child). This action would require the user first to delete all of the Order Items
    or children records before allowing the parent Order to be deleted.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Delete All: this is also known as cascading, because the system would automatically
    delete all child associations with the parent entity. Using the same example as
    (1), the children records in Order Items would automatically be deleted. While
    this option ensures referential integrity, it can be dangerous because it might
    delete records that are otherwise important to keep.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Default Value: the use of a default value is the same as in insertion, that
    is, it allows a “dummy” row to be inserted into the parent. This means that the
    original parent is deleted, and the child records are redirected to some default
    value row in the parent entity. This is sometimes useful when there are many old
    parent records, such as old part-numbers, that are cluttering up the parent database.
    If keeping the child records is still important, they can be redirected to a default
    parent row, such as “Old Part-Number.”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Algorithm: the use of the algorithm is the same as with an insertion. As in
    the case of (3) above, the default value might be based on the type of product
    or year it became obsolete.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Null: as in the case of insertion, the assigning of a null means that the parent
    does not exist. This creates a situation where the child records become “orphans.”
    Referential integrity is lost.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Don’t Care: same as in insertion. The database allows parent records to be
    deleted without checking to see if there are corresponding child records in another
    entity. This also results in losing referential integrity and creates “orphans.”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In summary, key business rules are concerned with the behavior of primary keys
    during insert and delete operations. There are six alternative options within
    each operation (insert and delete). Four of the options uphold referential integrity,
    which is defined as the dependability of the relationships between items of data.
    Data integrity is an issue any time there is change to data, which in ecommerce
    systems will be frequent. Thus, the ecommerce analyst must ensure that once primary
    keys have been determined, it is of vital importance that users are interviewed
    regarding their referential integrity needs. Analysts should not make these decisions
    in a vacuum and need to present the advantages of referential integrity appropriately
    to users so that they can make intelligent and well-informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: This discussion of key business rules was predicated on using examples derived
    from the discussion on Normalization. As discussed earlier in this section, the
    application of normalization occurs after the determination of key business rules,
    especially since it may indeed affect the design of the ERD, and in the programming
    of stored procedures. This will be discussed further in the Determine Domains
    and Triggering Operations section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 4.11 Combining User Views
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The application of Normalization focused on breaking up or decomposing entities
    to include the correct placement of data. Each NF failure resulted in creating
    a new entity; however, there are situations where certain entities may need to
    be combined. This section is labeled “Combining User Views” because the meaning
    of data is strongly dependent on how the user defines a data element. Unfortunately,
    there are circumstances where data elements are called different things and defined
    differently by different users in different departments. The word “different”
    is critical to the example. In cases where we think we have two entities, we may,
    in fact have only one. Therefore, the process of combining user views typically
    results in joining two or more entities as opposed to decomposing them as done
    with Normalization. The best way to understand this concept is to recall the earlier
    discussion on Logical Equivalents. This interpretation of the Logical Equivalent
    will focus on the data rather than the process. Suppose there are two entities
    created from two different departments. The first department defines the elements
    for an entity called “Clients” as shown in Fig. [4.20](#Fig20).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig20_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig20_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.20
  prefs: []
  type: TYPE_NORMAL
- en: The client entity
  prefs: []
  type: TYPE_NORMAL
- en: The second department defines an entity called “Customers” as shown in Fig. [4.21](#Fig21).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig21_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig21_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.21
  prefs: []
  type: TYPE_NORMAL
- en: The customer entity
  prefs: []
  type: TYPE_NORMAL
- en: Upon a closer analysis and review of the data element definitions, it becomes
    apparent that the two departments are looking at the same object. Notwithstanding
    whether the entity is named Client or Customer, these entities must be combined.
    The process of combining two or more entities is not as simple as it might sound.
    In the two examples, there are data elements that are the same with different
    names, and there are unique data elements in each entity. Each department is unaware
    of the other’s view of the same data, and by applying logical equivalencies the
    following single entity results as shown in Fig. [4.22](#Fig22).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig22_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig22_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.22
  prefs: []
  type: TYPE_NORMAL
- en: Combined client and customer entity
  prefs: []
  type: TYPE_NORMAL
- en: The above example uses names that made it easier for an analyst to know they
    were the same data elements. In reality, such may not be the case, especially
    when working with legacy systems. In legacy systems, names and definitions of
    elements can vary significantly among departments and applications. Furthermore,
    the data definitions can vary significantly. Suppose Client is defined as VARCHAR2(35)
    and Customer as VARCHAR2(20). The solution is to take the larger definition. In
    still other scenarios, one element could be defined as alphanumeric, and the other
    numeric. In these circumstances the decisions become more involved with user conversations.
    In either situation, it is important that the data elements do get combined and
    that users agree-to-agree. In cases where user agreement is difficult, then analysts
    can take advantage of a data dictionary feature called Alias. An Alias is defined
    as an alternate name for a data element. Multiple Aliases can point to the same
    data dictionary entry. Therefore, screens can display names that are Aliases for
    another element. This alternative can solve many problems when using different
    names is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Another important issue in combining user views is performance. While analysts
    should not be overly concerned about performance issues during LDM, it should
    not be ignored either. Simply put, the fewer entities, the faster the performance;
    therefore, the least number of entities that can be designed in the ERD the better.
  prefs: []
  type: TYPE_NORMAL
- en: 4.12 Integration with Existing Data Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The purpose of this section is to discuss specific analysis and design issues
    relating to how to integrate with existing database applications. The connectivity
    with other database systems is difficult. Indeed, many firms approach the situation
    by phasing each business area over time into a new re-developed operation. In
    these circumstances, each phased area needs a “Legacy Link” which allows the “old”
    applications to work with the new phased-in software.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linking entities with existing databases may force ecommerce analysts to rethink
    how to preserve integrity while still maintaining the physical link to other corporate
    data. This occurrence is a certainty with ecommerce systems given that certain
    portions of the data are used inside and outside the business. The following example
    shows how this problem occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: The analyst is designing a website that utilizes the company’s Orders Master
    database. The website needs this information to allow customers to see information
    about their past orders for items so they can match it to a product database supplied
    by the ecommerce system. This feature is provided to customers to allow them to
    understand how items have been utilized to make their products. Unfortunately,
    the master Order Items database holds only orders for the past year and then stores
    them off-line. There is no desire by the Order department to create a historical
    tracking system. The ERD in Fig. [4.23](#Fig23) shows the relationships with the
    corporate Order Items database file.![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig23_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig23_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.23
  prefs: []
  type: TYPE_NORMAL
- en: ERD showing association between web databases and legacy employee master
  prefs: []
  type: TYPE_NORMAL
- en: Note that the Order Item Products entity has a one or zero relationship with
    the Order Item Master entity. This means that there can be an Order Item in the
    Order Item Product entity that does not exist in the Order item entity. Not only
    does this violate Normalization, it also presents a serious integrity problem.
    For example, if the customer wanted to display information about their products
    and each component Item, all Items that do not exist in the Order Item entity
    will display blanks, since there is no corresponding name information in the Order
    Item file. Obviously, this is a flaw in the database design that needs to be corrected.
    The remedy is to build a subsystem database that will capture all of the Order
    items without purging them. This would entail a system that accesses the Order
    Item database and merges it with the Web version of the file. The merge conversion
    would compare the two files and update or add new Order items without deleting
    the old ones. That is, the master Order Items would be searched daily to pick
    up new Order Items to add to the Web version. Although this is an extra step,
    it maintains integrity, Normalization, and most important, the requirement not
    to modify the original Order Item database. The drawback to this solution is that
    the Web version may not have up-to-date Order items information. This will depend
    on how often records are moved to the Web database. This can be remedied by having
    a replication feature, where the Web Order Item would be created at the same time
    as the master version. The ERD would be reconstructed as shown in Fig. [4.24](#Fig24).![../images/480347_1_En_4_Chapter/480347_1_En_4_Fig24_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fig24_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4.24
  prefs: []
  type: TYPE_NORMAL
- en: ERD reflecting legacy link to the Order Item entity
  prefs: []
  type: TYPE_NORMAL
- en: In the above diagram the Order Item master and its relation to the Web Order
    Item entity are shown for informational purposes only. The master Order Item becomes
    more of an application requirement rather than a permanent part of the ERD. In
    order to “operationalize” this system, the analyst must first have to reconstruct
    the history data from the purged files, or simply offer the historical data as
    of a certain date.
  prefs: []
  type: TYPE_NORMAL
- en: 4.13 Determining Domains and Triggering Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The growth of the relational database model has established processes for storing
    certain application logic at the database level. We have already defined key business
    rules as the vehicle to create constraints at the key attribute level. However,
    there are other constraints and procedures that can occur depending on the behavior
    of non-key attributes. Ultimately, business rules are application logic that is
    coded in the database language, for example PL_SQL for Oracle. These non-key attribute
    rules could enforce such actions as: If CITY is entered, the STATE must also be
    entered. This type of logic rule used to be enforced at the application level.
    Unfortunately, using application logic to enforce business rules is inefficient
    because it requires the code to be replicated in each application program. This
    process also limits control, in that the relational model allows users to “query”
    the database directly. Thus, business rules at the database level need to be written
    only once, and they govern all type of applications, including programs and queries.'
  prefs: []
  type: TYPE_NORMAL
- en: As stated earlier, business rules are implemented at the database level via
    stored procedures. Stored procedures are offered by most database manufacturers,
    and although they are similar, they are not implemented using the same coding
    schemes. Therefore, moving stored procedures from one database to another is not
    trivial. The importance of having portable stored procedures and their relationship
    to partitioning databases across the Internet, Intranets, and distributed networks
    is becoming even more complex in mobile-based architecture. It is important to
    note that distributed network systems are being built under the auspices of client/server
    computing and may require communication among many different database vendor systems.
    If business rules are to be implemented at the database level, the compatibility
    and transportability of such code becomes a challenge. We also see that client/server
    will be addressed more and more as distributed and although normalization remains
    important, the expansion of blockchain will require multiple stored data to exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Business rule implementations fall into three categories: Keys, Domains and
    Triggers. Key business rules have already been discussed as part of the normalization
    process. Domains represent the constraints related to an attribute’s range of
    values. If an attribute (key or non-key) can have a range of values from one to
    nine, we say that range is the domain value of the attribute. This is very important
    information to be included and enforced at the database level through a stored
    procedure for the same reasons as discussed above. The third and most powerful
    business rule is Triggers.'
  prefs: []
  type: TYPE_NORMAL
- en: Triggers are defined as stored procedures that when activated “trigger” one
    or a set of other stored procedures to be executed. Triggers act on other entities,
    although in many database products, triggers are becoming powerful programming
    tools to provide significant capabilities at the database level rather than at
    the application level. Triggers resemble batch type files which when invoked execute
    a “script” or set of logical statements as shown below:![../images/480347_1_En_4_Chapter/480347_1_En_4_Figd_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Figd_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: This trigger was implemented in a contact management ecommerce system. The trigger
    is designed to allow corporate information to be marked as confidential only by
    specific executives. This means that an appointed executive of the corporation
    can enter information that is private. The second component of the trigger is
    programmed to automatically ensure that the executive’s contacts are stored as
    private or confidential. These two stored procedures show how application logic
    executes via Oracle triggers. It is important to remember that these business
    rules are enforced by the database regardless of how the information is accessed.
  prefs: []
  type: TYPE_NORMAL
- en: Triggers, however, can cause problems. Because triggers can initiate activity
    among database files, designers must be careful that they do not impair performance.
    For example, suppose a trigger is written that affects 15 different database files.
    Should the trigger be initiated during the processing of other critical applications,
    it could cause significant degradation in processing, and thus affect critical
    production systems.
  prefs: []
  type: TYPE_NORMAL
- en: The subject of business rules is very broad yet must be specific to the actual
    database product to be used. Since analysts may not know which database will ultimately
    be used, specifications for stored procedures should be developed using the specification
    formats presented in Chap. [3](480347_1_En_3_Chapter.xhtml). This is even more
    salient in ecommerce systems given the possibility that different databases can
    be used across the entire system.
  prefs: []
  type: TYPE_NORMAL
- en: 4.14 De-normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Third NF databases can often have difficulty with performance. Specifically,
    significant numbers of look-up tables, which are actual 3rd NF failures, create
    too many index links. As a result, while we have reached the integrity needed,
    performance becomes an unavoidable dilemma. In fact, the more integrity, the less
    performance. There are a number of ways to deal with the downsides of normalized
    databases. One is to develop data warehouses and other off-line copies of the
    database. There are many bad ways to de-normalize. Indeed, any de-normalization
    hurts integrity. But there are two types of de-normalization that can be implemented
    without significantly hurting the integrity of the data.
  prefs: []
  type: TYPE_NORMAL
- en: The first type of de-normalization is to revisit 3rd NF failures to see if all
    of the validations are necessary. Third NF failures usually create tables that
    ensure that entered values are validated against a master list. For example, in
    Fig. [4.10](#Fig10), the Customers entity, created as a result of a 3rd NF failure
    provides a validation to all customers associated with an Order. This means that
    the user cannot assign any customer, but rather only those residents in the Customer
    entity. The screen to select a Customer would most likely use a “drop-down” menu,
    which would show all of the valid Customers for selection to the Order. However,
    there may be look-up tables that are not as critical. For example, zipcodes may
    or may not be validated. Whether zipcodes need to be validated depends on the
    use of zipcodes by the users. If they are just used to record a Customer’s address,
    then it may not be necessary or worthwhile to have the zipcode validated. If,
    on the other hand, they are used for certain types of geographic analysis or mailing,
    then indeed, validation is necessary. This process—the process of reviewing the
    use and need for a validation table—should occur during the interview process.
    If this step is left out, then there is a high probability that too many non-key
    attributes will contain validation look-up entities that are unnecessary and hurt
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: The second type of de-normalization is to add back “derived” attributes. While
    this is not the preferred method, it can be implemented without sacrificing integrity.
    This can be accomplished by creating triggers that automatically launch a stored
    procedure to recalculate a derived value when a dependent attribute has been altered.
    For example, if Amount is calculated based on Quantity * Unit-Price, then two
    triggers must be developed (one for Quantity and one for Unit-Price) which would
    recalculate Amount if either Quantity or Unit-Price were changed. While this solves
    the integrity issue, analysts must be cognizant over the performance conflict
    should the trigger be initiated during peak processing times. Therefore, there
    must be a balance between the trigger and when it is allowed to occur.
  prefs: []
  type: TYPE_NORMAL
- en: As stated earlier, denormalization will be occurring more often because of IoT
    and blockchain where portions of data will need to be distributed. I advocate
    for always starting the design with normalization in mind, and then depending
    on the network design to allow duplications based on performance of the network
    and the characteristics of the interface devices.
  prefs: []
  type: TYPE_NORMAL
- en: 4.14.1 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter has provided the logical equivalent to the data component of the
    ecommerce system. The process of decomposing data is accomplished using LDM, which
    has eight major steps that need to be applied in order to functionally decompose
    the data. Data Flow Diagrams (DFD) are a powerful tool to use during process analysis
    because they provide direct input into the LDM method. Specifically, data flows
    provide data definitions into the Data Dictionary, which is necessary to complete
    LDM. Furthermore, data stores in the DFD represent the major entities, which is
    the first step in LDM. The output of LDM is an ERD, which represents the schematic
    or blueprint of the database. The ERD shows the relationships among entities and
    the cardinality of those relationships.
  prefs: []
  type: TYPE_NORMAL
- en: The LDM also makes provisions to develop stored procedures, which are programs
    developed at the database level. These procedures allow “referential integrity”
    to be enforced without developing application programs that operate outside the
    data. Stored procedures can be used to enforce key business rules, domain rules,
    and triggers. Triggers are batch-oriented programs that automatically execute
    when a particular condition has occurred at the database level, typically, when
    an attribute has been altered in some way.
  prefs: []
  type: TYPE_NORMAL
- en: The process of LDM also allows for the de-normalization at the logical design
    level. This is allowed so that analysts can avoid significant known performance
    problems before the physical database is completed. De-normalization should occur
    at the user interface time, as many of the issues will depend on the user’s needs
    and the expansion of IoT and blockchain. Another important issue is the reduction
    in natural keys that are being replaced with hash algorithms to protect security.
  prefs: []
  type: TYPE_NORMAL
- en: 4.15 Problems and Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Logical Data Modeling trying to accomplish?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define Normalization. What are the three Normal Forms?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does Normalization not do?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is meant by the term “derived” data element?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the concept of combining user views. What are the political ramifications
    of doing this in many organizations?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are Legacy Links? Describe how they can be used to enforce data integrity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name and define the three types of Business Rules.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are Stored Procedures in some ways a contradiction to the rule that data
    and processes need to be separated?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the disadvantages of database triggers?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is meant by De-Normalization? Is this a responsibility of the analyst?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.16 Mini-project #1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Physician Master File from a DFD contains the following data elements:![../images/480347_1_En_4_Chapter/480347_1_En_4_Fige_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Fige_HTML.png)Assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: a.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Physician can be associated with many hospitals, but must be associated with
    at least one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Physician can have many specialties, or have no specialty.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Assignment: Normalize to 3rd Normal Form.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.17 Mini-project #2'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following enrollment form has been obtained from Southeast University’s
    Computer Science program:![../images/480347_1_En_4_Chapter/480347_1_En_4_Figf_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Figf_HTML.png)The
    students are choosing their courses from the following Course List:![../images/480347_1_En_4_Chapter/480347_1_En_4_Figg_HTML.png](../images/480347_1_En_4_Chapter/480347_1_En_4_Figg_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Assignment: Using the above form, create a normalized ERD. Make sure you are
    in 3rd Normal Form.'
  prefs: []
  type: TYPE_NORMAL
- en: '**HINT**: You should end up with at least four entities, possibly five.'
  prefs: []
  type: TYPE_NORMAL
