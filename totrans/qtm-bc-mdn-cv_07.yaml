- en: '© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022A.
    Kumar et al. (eds.)Quantum and Blockchain for Modern Computing Systems: Vision
    and AdvancementsLecture Notes on Data Engineering and Communications Technologies133[https://doi.org/10.1007/978-3-031-04613-1_7](https://doi.org/10.1007/978-3-031-04613-1_7)'
  prefs: []
  type: TYPE_NORMAL
- en: Quantum Generative Modelling and Its Use Cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kusal M. Abeywickrama^([1](#Aff5)), Srinjoy Ganguly^([2](#Aff6) [ ](#ContactOfAuthor2)),
    Luis Gerardo Ayala Bertel^([3](#Aff7) [ ](#ContactOfAuthor3)) and Saurav Mohanty^([4](#Aff8))(1)University
    of Sri Jayewardenepura, Nugegoda, Sri Lanka(2)Technical University of Madrid,
    Madrid, Spain(3)Cartagena University, Cartagena, Colombia(4)John Moore’s University,
    Liverpool, UKSrinjoy Ganguly (Corresponding author)Email: [srinjoyganguly@gmail.com](mailto:srinjoyganguly@gmail.com)Luis Gerardo
    Ayala BertelEmail: [layalab@unicartagena.edu.co](mailto:layalab@unicartagena.edu.co)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A generative model describes how a dataset is generated according to a probability
    model and how new data can be generated by sampling this model. In the following
    work, we present the generative models’ approach in quantum along with other algorithms
    that have been developed for the optimization of generative models as well for
    a quantum generative model-based. We have described a use case of molecular simulation
    and optimization using variational quantum eigen solver. The quantum generative
    model that we describe has several important use cases in which we discuss the
    generation of probability distributions, drugs, and option pricing for financial
    application. Various applications and their impact evaluations have been reviewed
    to determine if the use of quantum generator algorithms is more appropriate than
    traditional machine learning methods. The use of quantum generator algorithms
    has proven to be more appropriate than traditional machine learning methods according
    to previous works. This leads us to the place of quantum development in the field
    of artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: KeywordsQuantum computingQuantum machine learningQuantum generative adversarial
    networkGenerative adversarial networkMachine learningAbbreviations
  prefs: []
  type: TYPE_NORMAL
- en: DL
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: GAN
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks
  prefs: []
  type: TYPE_NORMAL
- en: KNN
  prefs: []
  type: TYPE_NORMAL
- en: K-Nearest Neighbours
  prefs: []
  type: TYPE_NORMAL
- en: ML
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: NN
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: QGAN
  prefs: []
  type: TYPE_NORMAL
- en: Quantum Generative Adversarial Networks
  prefs: []
  type: TYPE_NORMAL
- en: QAE
  prefs: []
  type: TYPE_NORMAL
- en: Quantum Amplitude Assessment
  prefs: []
  type: TYPE_NORMAL
- en: RL
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning
  prefs: []
  type: TYPE_NORMAL
- en: SVM
  prefs: []
  type: TYPE_NORMAL
- en: Support-Vector Machines
  prefs: []
  type: TYPE_NORMAL
- en: VQE
  prefs: []
  type: TYPE_NORMAL
- en: Variational Quantum Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1 Machine Learning in Classical Computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently, machine learning is one of the most prominent and powerful technologies.
    More significantly, we have yet to fully realize its potential. It will, without
    a question, continue to make headlines for the foreseeable future. Within this
    is a subset called deep learning that focuses on mimicking the behaviour of neural
    networks to solve more tangled and non-linear problems. Thus, having a broader
    spectrum with tools capable of working with tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is machine learning? In 1959, Arthur Samuel [[1](#CR1)] defined machine
    learning as “the field of study that gives computers the ability to learn without
    being explicitly programmed.” A more technical definition given by Tom M. Mitchell
    [[2](#CR2)] in 1997: “A computer program is said to learn from experience E with
    respect to some class of tasks T and performance measure P, if its performance
    at tasks in T, as measured by P, improves with experience E.”'
  prefs: []
  type: TYPE_NORMAL
- en: The way humans interact with and relate to data has radically changed as a result
    of machine learning. Self-driving automobiles to intelligent agents capable of
    beating the greatest humans at Jeopardy and Go are just a few examples. Large
    data sets are present in many applications, pushing current techniques and processing
    resources to their limits. The main types are supervised learning, unsupervised
    learning, and reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning is considered a machine learning algorithm that trains on
    tagged data. Algorithms are trained using supervised learning data when input
    variables are added and their output is known. We have X as independent variables
    and Y as dependent variables. A data is processed and then split or fold. One
    is used to train and the remaining is used to test.
  prefs: []
  type: TYPE_NORMAL
- en: Now in unsupervised Learning instead of the first one, the used data is unlabelled,
    implying that it’s applied to data with no previous knowledge doing a clustering
    approach. The goal is to look into the data and discover any hidden patterns in
    the data. Let’s cover the remarkable central algorithms in ML. Linear Regression
    is considered the basic algorithm. The use of linear regression to establish a
    link among two connected variables is fairly accurate. To do this, it's primarily
    established which is the independent variable, while a dependent variable will
    be used to set up a conditional probability. It searches for analytical relationships
    rather than predetermined ones. For example, using this method it is possible
    to identify the correlation between weight and height. A brother of linear regression,
    Logistic Regression, is utilized for classification preferably than regression
    problems. It uses the same input feature vector as linear regression, but instead
    of a continuous numeric value, it returns a class label. An algorithm that predicts
    whether a patient has an illness or not based on his medical information is an
    example of Logistic Regression.
  prefs: []
  type: TYPE_NORMAL
- en: Another that we can highlight is Decisions Trees, an ML technique that divide
    large input sets toward tinier groups based on a representative characteristic
    continuously to reach small suitable information to be described as accurate output.
    Its demands that we label the data (data need has to be assigned in one or many
    labels, such as the name in a photograph, as an example a plant), and then this
    method attempt to remark new data using that knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Binary Classification, a discrete ML procedure that sorts
    data into classes, the most common example is the spam-email, where assign 0 if
    is a faulty and 1 if is a normal mail, besides this method, exist the Regression
    that to predict values is needed continuously and dependent variables such as
    size with respect the price, like house pricings related to its area. This kind
    of query is ideal for DTs algorithms. When a conditioned variable is quantitative
    for example, to assign a loan customer likelihood, estimation regression trees
    can be utilized. Random forests are an ensemble of decision trees. Ensemble learning
    is a way to improve prediction performance using several learning algorithms.
    This way, the model is generalized more effectively and is less likely to overfit
    [[3](#CR3)].
  prefs: []
  type: TYPE_NORMAL
- en: There is also a special algorithm, support vector machine, the method that strives
    to determine a hyper-plane that is capable of being useful in a space of N variables
    concerning its application or insight analyzing data structures for classification
    or regression. There are many hyper-planes where two types of data points can
    be formed. Its main objective is to identify the plane with highest margins or
    gaps separating data points in two classes. Maximizing every boundary distance
    provides remarkable support, addressing more straightforward to classify data
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: One of the basic algorithms, K-Nearest Neighbours (KNN), can work in both classification
    or regression queries. This determination locates the k-closest neighbours and
    provides an expected output based on the class of the majority vote regarding
    the nearest set-points in the plane.
  prefs: []
  type: TYPE_NORMAL
- en: For regression problems, their aim is to discover the k-closest neighbours and
    predict the ideal output via computing significant mean estimation along with
    the nearest neighbours. This is an unsupervised learning algorithm, which means
    it learns patterns from data that hasn't been labelled. This means that a model
    can be trained to construct clusters on any dataset without needing to label the
    data first.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a special method as Principal Component Analysis which avoids
    concerns like over-fitting in high-dimensional space. It is a method for decreasing
    the data quantity of the variables picking the most relevant of a large input
    data collection. This shortens the data space size and conserves the best potential
    information. This strategy merges highly correlated variables to produce a smaller
    amount of insight variables known as ‘principal components’ that works for the
    preponderance data reduce the variance and don't fall in overfitting or underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Now we cover the basics of Deep Learning. is an ML subset at its most involved
    level. It develops a ‘learn’ to the computer by predicting, analyzing information
    inputs within layers. The data can be unstructured as writing, and waves-sound,
    images, expressed into matrices for investigations to solve or provide a certain
    application. It tries to mimic the human brain following the scheme of neural
    systems. Its goal is to produce some actual magic by simulating how the human
    brain functions. Deep learning algorithms are used to identify the correlations
    between inputs and outputs using what is termed a neural network. Neural network
    structure example is shown in Fig. [1](#Fig1).![](../images/516210_1_En_7_Chapter/516210_1_En_7_Fig1_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 1
  prefs: []
  type: TYPE_NORMAL
- en: Schematic diagram of a three-layer neuron network
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can see that have inputs, after hidden layers for feed-forward, and
    the output, it can be one or many, where each layer consists of nodes manipulating
    the amount of this. The input layers represent the data numerically, for images,
    the information is seen as RGB matrices associated in each pixel, the output layers
    are a reference for the test set and give proof about works or have the accuracy,
    meanwhile, the hidden layers do the greatest computation by processing, analyzing
    and updating the parameters in order to reduce the loss-error of expected values.
  prefs: []
  type: TYPE_NORMAL
- en: So, why is it referred to as “Deep” Learning? Deep learning is defined as the
    creation of many hidden layers called deep networks. Now depending on a large
    number of layers can or not improve the algorithm capability to estimate composite
    functions by adding additional weights and biases. Deep learning is a fascinating
    area that is changing our society dramatically. We should be interested in deep
    learning and the foundations of deep learning because it is straightforward to
    comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: Exist diverse kinds of models in neural networks to name two, like Convolutional
    Neural Network and Recurrent Neural Network, these practices depend on the problem
    standard that we'll be dealing with. NN's overall is designed and described in
    terms of layers that are a response by an activation function and are fed using
    the previous data inputs together with weight plus bias.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, Reinforcement Learning, ML technique which the input isn't
    raw data and the algorithm needs to identify the scenario on its own. Reinforcement
    learning is a type of learning that is commonly employed in instructions for the
    sequence of future independent steps as robotics devices, strategy games, or self-navigation.
    RL cognition learns through trial and error, performing every time to produce
    outstanding results.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look into Quantum Machine Learning which is an
    exciting field as it’s a union of Quantum and ML.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Quantum Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The machine learning algorithm must be extremely effective at manipulating large
    amounts of data, including previously gathered input–output data pairs. As the
    volume of data stored worldwide increases by about 20 percent each year [[4](#CR4)],
    there is pressure to find new ways to execute machine learning algorithms. If
    the data point is projected in higher order, classical computers cannot perform
    such calculations. Even though it can be handled by a classic computer, it takes
    more time. Due to that, the purpose of merging quantum information processing
    and machine learning has been a major body of research in recent years [[5](#CR5)–[7](#CR7)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 The Rise of Quantum Machine Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quantum machine learning is a field of combination of both, machine learning
    and quantum computers, which will change what the future holds [[6](#CR6)]. Large
    quantities of data are computed by using machine learning methods, whereas quantum
    machine learning enhances computing speed and data storage using qubits and quantum
    operations or specific quantum algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on how data are created or handled, there are several ways to combine
    quantum computing and machine learning as shown in Fig. [2](#Fig2). This includes
    a combination of both classical and quantum processing, where computationally
    expensive classical algorithms are executed by a quantum device [[5](#CR5)]. The
    first approach is that the classical datasets generated by classical experiments
    can be processed using quantum algorithms. Another approach is, quantum algorithms
    can be applied instead of classical data to measure quantum states based on quantum
    experiments [[8](#CR8)]. Besides that, classical machine learning methods apply
    to data generated by quantum experiments such as learning phase transitions of
    many-body quantum systems [[9](#CR9)]. The final approach is to process quantum
    data by quantum computers [[7](#CR7)].![](../images/516210_1_En_7_Chapter/516210_1_En_7_Fig2_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 2
  prefs: []
  type: TYPE_NORMAL
- en: Quantum computing approaches
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Review of Quantum Information Processing Concepts^([1](#Fn1))
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Qubit (or a quantum bit) is the quantum counterpart of the classical bit that
    consists of two states. These two-qubit states, known as the Dirac notation, are
    similar to the classical binary state and can be in both states and the superposition
    state of both states simultaneously. A qubit can be described by using the following
    notation as, |*ψ*⟩ = *α* | 0⟩ + *β* | 1⟩, where *α, β* ∈ *C* and | *0*⟩, | *1*⟩
    is in Hilbert Space *H*^(*2*). Quantum dynamics always maintain normalization
    conditions that | *α* |² + | *β* |² = 1\. The Bloch sphere gives the graphical
    representation of the qubit as shown in Fig. [3](#Fig3). The quantum theory is
    based on an observation that indicates the probability of measurement in state
    | 0⟩ or | 1⟩ with squared amplitudes | *α* |², | *β* |². Therefore, the state
    of the qubit cannot be defined as only ‘0’ or ‘1’ state, and there is a probability
    of it being measured in either of two states. This allows calculations to be performed
    in both states simultaneously, it is called quantum parallelism. Qubits can be
    shown in the Bloch Sphere.![](../images/516210_1_En_7_Chapter/516210_1_En_7_Fig3_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3
  prefs: []
  type: TYPE_NORMAL
- en: The representation of qubit using Bloch sphere
  prefs: []
  type: TYPE_NORMAL
- en: The power of quantum machine learning is evident when considering an N number
    of qubit systems with two quantum states. A superposition of all 2n combinations
    can be allowed for the quantum system. For instance, considering a system of 2
    qubits, the superposition includes a total of 4 states, {| *00*⟩ + | *01*⟩ + |
    *10*⟩ + | *11*⟩}. All these combinations can be processed in parallel with an
    algorithm. However, the uncertainty of the results that happen by measurements
    always constrains the efficiency of quantum computers. When measuring a qubit,
    it will be either | 0⟩ or | 1⟩, and then the system will collapse to the quantum
    state which has been observed. After this measurement, the system will only generate
    the observed state as the output.
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of certain formal concepts of quantum information theory is
    crucial. One of the most important concepts, in quantum mechanics, is the Unitary
    operators (U) which represent the conversion of vectors. It shall always be equal
    to one if we analyse the product of Unitary Operators and its Hermitian conjugates
    (*U*^† *U* = 1, where *U*^† is the Hermitian conjugate) [[10](#CR10)]. A quantum
    algorithm is a collection of unitary operators that define the dynamics of quantum
    systems, and these unitary transformations are the quantum counterpart of the
    classical gates of bit manipulation. A certain number of these quantum gates are
    for single-qubit only. For instance, the X-gate can flip the state of qubits while
    the Z-gate can change the amplitude sign of the single qubit. A superposition
    state can be generated by applying the Hadamard gate (H-gate) [[11](#CR11)]. But
    gates like Controlled-NOT (C-NOT) gate need more than one qubit to complete its
    operation and only if the first qubit is in state | 1⟩, it flips the state of
    the second qubit [[11](#CR11)]. The unitary transformation *U* = *e*^(*−iHt*)
    can give a much more general formula for a quantum gate obtained from the Schrödinger
    equation which is a fundamental of quantum theory.
  prefs: []
  type: TYPE_NORMAL
- en: Qubits have a different property of quantum physics called entanglement [[12](#CR12)],
    similar to superposition. Due to the superposition, the qubit can be being in
    many states at the same time whereas entanglement creates a correlation between
    two qubits even though they are in separate locations physically. When the quantum
    entanglement takes place between two particles, the individual quantum states
    are indeterminate until measured, and one determines the measurement result of
    the other, even though the measurement process is far from each other. The above
    figure (Fig. [4](#Fig4)) is one of the graphical representations of the quantum
    entanglement circuit. The quantum state of n qubits means that all possible configurations
    are in a superposition state (2n). The quantum entanglement of the electron spins
    of n qubits expands the dimensions of the Hilbert space exponentially from 2 to
    2^(*n*).![](../images/516210_1_En_7_Chapter/516210_1_En_7_Fig4_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4
  prefs: []
  type: TYPE_NORMAL
- en: Quantum circuit (quantum entanglement circuit)
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, quantum algorithms provide more computational benefits than classical
    algorithms. The quantum paradigm is not to solve a problem fast, but to solve
    it with completely new technology and more effectively. Besides, when using techniques
    in which conventional calculation cannot generate so many states once using concepts
    like superposition or entanglement, Quantum algorithms can have a significant
    advantage.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Variational Quantum Algorithms (VQAs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When running in the quantum realm, the behaviour of the qubits is highly delicate
    and unpredictable. Furthermore, parallel to the increment of the number of qubits,
    the fragility of the qubits also rises. The quality of the material, that is used
    in the fabrication, affects directly the stability of qubits. Because of this
    instability, it is hard to maintain a high coherence time for qubits. However,
    we need a large number of qubits to work on advanced problems, with high coherence
    time. Currently, it is impossible to access these types of quantum computers,
    but noisy intermediate-scale quantum (NISQ) computers [[8](#CR8), [13](#CR13)]
    which are composed of a large number of qubits that are very noisy with less coherence
    time, can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Variational quantum algorithms (VQAs) [[14](#CR14), [15](#CR15)] are an improvement
    in quantum computing that runs on NISQ computers that employ classical optimizers
    to train parameterized quantum circuits. It is possible to use variational algorithms
    as the building blocks for a number of use cases, including the design of a quantum
    classifier. It is similar in operation to a classical support-vector machine in
    terms of complexity and speed of operation. Support-vector machines, often known
    as SVMs, are supervised learning models that are used in machine learning to address
    classification and regression problems. The same as any support-vector machine,
    the quantum variational circuit performs hyperplane slices in the same way that
    a normal SVM does.
  prefs: []
  type: TYPE_NORMAL
- en: As long as the kernel function is performed efficiently on a classical computer,
    quantum logic-based classifiers have no advantage over classical computers in
    terms of performance. However, when the feature space is very wide and kernel
    functions are difficult to discover, quantum computing can be highly beneficial.
    The variational algorithm, like a conventional support-vector machine, uses the
    Hilbert space in which the quantum processor runs to determine the best Hyperplane
    cut. The algorithm is used for both training and classification. The training
    stage makes use of a previously categorized dataset. Quantum algorithms that rely
    on free parameters are known as variational or parameterized quantum circuits.
    They, like normal quantum circuits (as shown in Fig. [5](#Fig5)), are made up
    of three components [[16](#CR16)]; (i) Preparation of a fixed initial state; (ii)
    creation of the *quantum circuit* ***U(θ)***, defined by a collection of free
    parameters ***θ***; and (iii) At the output, an observable ***B̂*** is measured.![](../images/516210_1_En_7_Chapter/516210_1_En_7_Fig5_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 5
  prefs: []
  type: TYPE_NORMAL
- en: Variational quantum circuit
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Variational Quantum Eigensolver (VQE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we analyse the quantum mechanics precisely applied to computation requires
    the introduction of certain postulates that shape the theory and serve as a basis
    for other reasoning. Here we want to establish a formal perspective of what VQE
    is, we must know the relevance of the following claims:'
  prefs: []
  type: TYPE_NORMAL
- en: The observables of a system are represented by hermitic linear operators. The
    set of eigenvalues of the observable is called a spectrum and its eigenvectors,
    exact or approximate, define a base in the Hilbert space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a system is in the normalized state, the measurement of an observable A
    will result in the eigenvalue a, with a probability ![$$PA|\Psi \rangle ={|\langle
    a|\Psi \rangle |}^{2}$$](../images/516210_1_En_7_Chapter/516210_1_En_7_Chapter_TeX_IEq1.png).
    The eigenvector is associated with the eigenvalue a, which in Hilbert space notation
    is expressed as A|a⟩ = a|a⟩ [[17](#CR17)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then is possible to affirm VQE is an application of the variational method of
    quantum mechanics by doing algorithm in the quantum–classical hybrid way mostly
    used to find eigenvalues of Hamiltonian matrices which is done by preparing an
    arbitrarily selecting wave function |Ψ⟩ and making use of Ansatz, a parameterized
    quantum circuit with rotation gate which creates the state vector providing measurement
    to estimate the expectation value [[17](#CR17)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 Quantum Variational Algorithm for Eigenvector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The VQE is correlated to a variational quantum algorithm whose most common purpose
    is trying to approximate the ground state of quantum systems. It is a method for
    approximating certain ground states |Ψ⟩ and the lowest energy E[min] consisting
    of two primary steps.
  prefs: []
  type: TYPE_NORMAL
- en: First select ansatz or criterion trial state by some parameters theta |Ψ(θ)⟩
    on condition that only belong into a subspace instead of looking at the full Hilbert
    Space describing all the possible quantum states, then the second step is to vary
    the theta parameters to minimize the energy value, finding the proper parameters
    theta for which this energy of the trial state becomes lowest, automatically we
    can approximate the ground state |Ψ⟩, therefore the choices of ansatz are very
    important for the variational methods work. Figure [6](#Fig6) is a simple representation
    of such a quantum circuit.![](../images/516210_1_En_7_Chapter/516210_1_En_7_Fig6_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 6
  prefs: []
  type: TYPE_NORMAL
- en: Quantum circuit that depends on parameters θ; Initial state, parameterized circuit,
    and observable measurement
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the quantum state with variational quantum circuit U(θ) is the basis
    for obtaining the ansatz state providing in such way to minimize the loss of theta
    parameters by running the circuit on a quantum computer giving us the measurements
    of the energy value which will be using it as input in a classical optimizer for
    feedforward and after that get a hybrid quantum/classical feedback iteration to
    update the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 VQE Use Case—Chemistry
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Variational Quantum Eigensolver is one of the attractive algorithms when it
    comes to large-scale matrices, in chemistry we can highlight its use due to big
    data as the number of atoms increases, this algorithm has worked in finding the
    ground state energy by simulating molecules and chemical reactions. Since one
    of the VQE main reasons is to have this approximation with the eigenvalues of
    a matrix, its principle for chemistry in quantum is solving the molecular Hamiltonian
    eigenvalue problem which shows an approach about these properties of large molecules
    in a way that a classical computer is difficult and not efficiently to do it.
  prefs: []
  type: TYPE_NORMAL
- en: In a practical case, there are already demonstrations of VQE in chemical compounds
    such H[2]O, LiH, BeH[2], the last one in 2017 was successfully implemented VQE
    for the estimation of the ground state energies of molecules on an IBM quantum
    computer [[18](#CR18)]. By studying the molecules’ ground-state is possible to
    obtain the energy potential between two atoms in the variating interatomic distance
    and the lowest energy in potential-energy surfaces corresponds to the most stable
    structure of the molecules [[19](#CR19)].
  prefs: []
  type: TYPE_NORMAL
- en: To have high efficiency in the superconducting quantum processor for solving
    molecular structure problems if we take the BeH[2] compound reference is necessary
    based on mappings between fermionic and qubit operators [[20](#CR20)]. The BeH[2]
    Hamiltonian is defined upon the 1 s, 2 s, 2px orbitals associated to Be, and 1 s
    orbital associated with each H atom. To obtain the potential energy surfaces for
    BeH[2], the authors search for the ground state energy of their molecular Hamiltonians,
    using 2, 4, and 6 qubits respectively, for depth d = 1, for a range of different
    interatomic distances [[19](#CR19)].
  prefs: []
  type: TYPE_NORMAL
- en: Showing a perspective for using a single qubit in the variational form to solve
    a problem similar to ground state energy, should be given a random probability
    vector and figure out how to determine a possible parameterization for this qubit
    where it has to generate a close probability distribution to the vector.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Quantum Generative Adversarial Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most promising advances which have been made in the field of classical
    deep learning is GAN’s which is the acronym for Generative Adversarial Networks
    [[21](#CR21)] and it is part of generative machine learning models. The idea of
    GANs is to generate as realistic data samples as possible based on other training
    data samples provided. GANs are much larger models and more difficult to train
    than traditional in-deep learning models, but despite their training limitations,
    they have found some quite useful applications in the industry. Some of the primary
    applications include image super-resolution, image generation, 3D objects generation,
    and text generation.
  prefs: []
  type: TYPE_NORMAL
- en: The construction of classical GAN’s consists of two parts as shown in Fig. [7](#Fig7)—*generator*
    which generates new data samples, it can be images or text or audio, and *discriminator*
    whose job is to distinguish the samples generated by the generator and real-world
    data samples. Architecturally, generator and discriminator, both are composed
    of neural networks layers which can approximate complex functions and can capture
    the essence of data features provided to them. Now we are going to see the architecture
    of classical GAN’s in detail from which a generalization of quantum GAN’s will
    make much more sense and you will see that it is fairly intuitive.![](../images/516210_1_En_7_Chapter/516210_1_En_7_Fig7_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 7
  prefs: []
  type: TYPE_NORMAL
- en: Common GAN components are shown in this figure
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Architecture and Training of GAN’s
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall from the explanation of GAN’s previously that the generator tries to
    generate samples as close as possible to the real-world samples. For convenience,
    the generator will be called G and the discriminator will be called D. Therefore,
    we can suppose that we have some real data samples source R, which can generate
    a certain probability distribution of these real data samples called *p*[*R*]*(x)*.
    Also, the generator distribution of the data samples is given by *p*[*G*]*(x)*
    where *x* is the input data samples. The G network takes a random variable *z*
    as an input which is drawn from some normal or uniform distribution and then the
    task for G is to transform this input to data samples thereby generating the *p*[*G*]*(x)*.
    The D takes real or the G-generated data samples as inputs and compares whether
    they are closer to each other or not. Now according to the adversarial learning
    principle, G would want to maximize the probability that D misclassifies its generated
    data points compared to the real data samples. On the other hand, D would like
    to maximize the probability of successfully classifying the real data samples
    which are opposite to what G wants, and this creates an adversarial training system.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Architecture and Training of Quantum GAN’s
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Throughout the entire discussion of this work on quantum machine learning, we
    considered only the supervised machine learning approach as that is the most popular
    and used by various industries as well. Here also for the quantum GAN’s discussion
    [[22](#CR22), [23](#CR23)], we are going to take the case of supervised learning
    as well and this means labels associated with the data samples. The primary quantum
    generative model structure and training which we are going to describe has been
    explained in [[24](#CR24)] and we are going to explain briefly the main structure
    of GAN’s and its properties. We will be focusing on conditional GAN’s aspect of
    quantum GAN’s as they provide conditioning into labels for samples generation.
  prefs: []
  type: TYPE_NORMAL
- en: Now, architecturally, the classical one and the quantum GAN’s shown in Fig. [8](#Fig8)
    above are very similar, where we have a quantum D which discriminates the quantum
    states it gets as input from the quantum G (quantum G takes a quantum noise vector
    state | *z* > , similar to that of classical G case of you recall) and the real
    data source R. You can recall that G and D in the classical case are deep neural
    network layers and therefore in the case of quantum GAN’s these are replaced by
    variational quantum circuits where quantum G is parameterized by *θ*[*G*] and
    it takes the input label state | *λ* > and quantum noise | *z* > to produce a
    density matrix that has the corresponding similarity to that of real data. When
    the | *z* > fluctuates randomly, more output samples can be created for a particular
    label, and by tuning | *z* > various latent properties of generated samples can
    be captured which are not captured by the labels *λ*.![](../images/516210_1_En_7_Chapter/516210_1_En_7_Fig8_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 8
  prefs: []
  type: TYPE_NORMAL
- en: The basic QGAN components are shown in this figure
  prefs: []
  type: TYPE_NORMAL
- en: For the training of the quantum GAN model, a quantum objective function is utilized
    so that the training happens efficiently. As for every training process, the G
    and D parameters are initialized to some random values and then a fair coin is
    made to select the process of commencement for either G or D. For the training
    of the model, quantum gradients can be utilized which relies on the numerical
    finite difference method. By the usage of this technique, the gradients can be
    calculated on the quantum computer itself.
  prefs: []
  type: TYPE_NORMAL
- en: 4 QGAN’s Use Case—Drug Discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the working of QGAN’s covered in the previous section, we will now consider
    a use case or a practical application of the QGAN technique. This application
    is in the field of chemistry, more specifically on drug discovery where the chemical
    space to search is very large in dimensions and classical GAN’s are not suitable
    to carry out these discoveries because of the curse of dimensionality. GAN’s try
    to discover those molecular compounds which have more affinity toward the disease
    cell and will have higher success in defeating the disease mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: The technique called QGAN with Hybrid Generator (QGAN-HG) used in the paper
    [[25](#CR25)] will be reviewed as it is one of the significant case studies for
    the QGAN’s for chemistry/drug discovery applications apart from usual financial
    applications. Usually, we require a larger number of qubits to even generate a
    small molecule, but QGAN-HG is very qubit efficient and molecules can be generated
    with a fewer number of qubits.
  prefs: []
  type: TYPE_NORMAL
- en: QGAN-HG, just like QGAN’s consists of a variational quantum circuit which is
    composed of initialization layers, parametric layers, and measurement layers.
    By the application of the measurement layers, a feature vector is extracted which
    is then fed into a classical neural network layer (HG part of QGAN-HG) followed
    by a layer of atom and bond layer to form that atom vectors and bond matrices
    successfully. A QM9 dataset [[25](#CR25)] has been used for training the QGAN-HG
    model and Frechet Distance has been used as a metric to measure the performance
    of the QGAN-HG generated molecules by comparing them with real molecules from
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: With the architecture and setup as described before, the QGAN-HG performs significantly
    well than, its classical GAN counterparts even with only 4 qubits because of the
    expressive power of the variational quantum circuits being used for the experiment.
    The Frechet distance values obtained are consistent with the original value results.
    The generated molecules are evaluated for their various chemical properties using
    the RDKit and then finally the prediction scores generated are backpropagated
    to the classical neural network (HG) and the variational quantum circuit for the
    update of their parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 QGANs for Loading Random Distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we are discussing a QGAN version with a quantum generator
    and a classical discriminator that records the distribution of the probability
    of classical training samples which has been published in IBM Documentations [[22](#CR22)].
    We will examine how QGAN can be implemented, in polynomial time in quantum states,
    for loading distribution data.
  prefs: []
  type: TYPE_NORMAL
- en: A quantum Generative Adversarial Network (QGAN) can be used, for a given k-dimensional
    data sample, to learn the random distribution of the database, and to load it
    directly into a quantum state:![$$\left| {{\text{g}}_{\theta } } \right. = \mathop
    \sum \limits_{i = 0}^{{2^{n} - 1}} \sqrt {p_{i} } \left| i \right._{n}$$](../images/516210_1_En_7_Chapter/516210_1_En_7_Chapter_TeX_Equ1.png)(1)
  prefs: []
  type: TYPE_NORMAL
- en: where describe the occurrence probabilities of the basis states | *j*⟩
  prefs: []
  type: TYPE_NORMAL
- en: The QGAN training aims to generate a state | *g*⟩ where, for, describes a probability
    distribution that is close to the distribution underlying the training data. Loading
    a uniform distribution into a quantum state is straightforward since it only takes
    one Hadamard gate per qubit [[23](#CR23), [26](#CR26)]. Although loading a normal
    distribution requires more advanced approaches, the required *O(poly(n))* gates
    [[24](#CR24)] will seldom dominate the quantum algorithm's total gate complexity.
    Here is a summary of the steps of training and evaluating the QGAN model.
  prefs: []
  type: TYPE_NORMAL
- en: The QGAN consists of a quantum generator *G*[*θ*], a variational quantum circuit,
    and a classical discriminator *D*[*ϕ*], a neural network. To implement the quantum
    generator, a depth-1 variational form was selected, that implements *R*[*Y*] rotations
    and *C*[*Z*] gates which take a uniform distribution as an input state. In particular,
    the parameters of the generator must be carefully selected for k > 1\. The circuit
    depth should, for example, be > 1, as higher circuit depths allow more complex
    structures to be shown. The classical discriminator has been used based on an
    implementation of the neural network, implemented by NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the parameters of the discriminator and generator are changed
    concerning the following loss functions:![$$L_{G} \left( {\phi ,\theta } \right)
    = - \frac{1}{m}\mathop \sum \limits_{I = 1}^{m} \left[ {\log \left( {D_{\phi }
    \left( {g^{l} } \right)} \right)} \right]$$](../images/516210_1_En_7_Chapter/516210_1_En_7_Chapter_TeX_Equ2.png)(2)and![$$L_{D}
    \left( {\phi ,\theta } \right) = \frac{1}{m}\mathop \sum \limits_{I = 1}^{m} \left[
    {\log \left( {D_{\phi } \left( {x^{l} } \right)} \right) + log\left( {1 - \left(
    {D_{\phi } \left( {g^{l} } \right)} \right)} \right)} \right]$$](../images/516210_1_En_7_Chapter/516210_1_En_7_Chapter_TeX_Equ3.png)(3)
  prefs: []
  type: TYPE_NORMAL
- en: where m is the batch size description and *g*^l is the description of the quantum
    generator data samples.
  prefs: []
  type: TYPE_NORMAL
- en: With the usage of QGANs, we can effectively load distributions of random probability
    into the quantum data state within polynomial time, which can be utilized for
    use in banking and financial sectors on other quantum algorithms such as QAE.
    To ensure the QGAN optimal performance, the probability distribution for the generator
    initialization must be properly set. Further studies on other distributions were
    released to demonstrate the effectiveness of QGAN in the research with application
    in quantum finance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Option Pricing with QGAN^([2](#Fn2))
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Options are financial derivatives giving the purchaser the right to purchase
    or sell an asset that is under the buyer's agreed price and date. The price of
    an option, called the premium, consists of a number of variables and it is really
    important to price options properly. The Black–Scholes model, the Monte-Carlo
    Simulating, and the Binomial Price Option are some of the models widely used to
    estimate options [[27](#CR27)]. These theories have wide margins for errors due
    to their value being derived from other assets. The pricing theory of options
    is primarily aimed at determining the probability of exercising options at maturity.
    Stock price, exercise price, volatility rate, and expiry time are typical variables
    used to enter into mathematical models to calculate the option price accurately.
    A stock's underlying option is a Call Option, which gives the holder the right
    to buy the stock at a certain price within a specific time period, and a Put Option
    allows the holder to sell the stock at a defined price within a given time period.
    This section will be further discussed the work performed by a team of IBM researchers,
    based on European call options that can be modelled at maturity only and that
    are modelled and analytically computed by Black–Scholes. They presented a method
    for pricing options and option portfolios, and they have suggested a way to implement
    it using quantum circuits.
  prefs: []
  type: TYPE_NORMAL
- en: The dependent variables of the option are e underlying asset price distribution
    (S[T]), the option's maturity (*T*), and the payoff function of options (*f(S*[*T*]*)*).
    The Black–Scholes model is built on the assumption that the spot price of a maturing
    ST for a European calling option is normally distributed [[28](#CR28)]. QGAN can
    therefore be trained on log-normal distribution, and the output can also be utilized
    as an uncertainty model for option. As follows, a quantum circuit can be constructed
    that loads the uncertainty model. The circuit output is read using Eq. ([1](#Equ1)).
  prefs: []
  type: TYPE_NORMAL
- en: Quantum Amplitude Estimation (QAE) can be used in association with the trained
    uncertainty model to predict the expected value of the payoff function of an option.
    Quantum Amplitude Assessment (QAE) is a simple but powerful quantum method that,
    when used appropriately, is capable of providing high-speed solutions for problems
    that traditionally use Monte Carlo simulation [[22](#CR22), [28](#CR28)–[30](#CR30)].
    However, it's expected that, in general, a universal fault-tolerant quantum computer
    would be required to get a quadratic speed-up compared to the conventional Monte
    Carlo simulations. However, the cost of standard portfolios in the financial sector
    requires more powerful quantum hardware than the current generation of quantum
    computers, capable of executing more complex quantum circuits with more qubits.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we began our journey from the basic concepts behind the field
    of classical machine learning and its various types which are relevant in the
    machine learning industry nowadays. Then we focused our attention on the fundamentals
    of quantum machine learning models where variational circuits composed of parameterized
    circuits are used and have better expressive power than the classical neural networks
    for learning features. Apart from this, we saw a use case of the variational algorithm
    for molecular simulation using the VQE algorithm. We then build upon the quantum
    machine learning concepts to learn more about quantum generative adversarial networks
    which are the most basic form of quantum generative modelling and utilize the
    variational circuits for the construction of its generator and discriminator components
    as well as uses quantum gradients for the optimization of its adversarial objective
    function. Finally, we explored 3 important use cases of QGAN’s including small
    molecule drug discovery, loading of random distributions, and option pricing for
    financial applications. Moreover, another application of quantum machine learning
    is for processing natural languages, and in this work, we hope to extend our work
    on quantum natural language processing.
  prefs: []
  type: TYPE_NORMAL
