- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2021Y.
    Maleh et al. (eds.)Artificial Intelligence and Blockchain for Future Cybersecurity
    ApplicationsStudies in Big Data90[https://doi.org/10.1007/978-3-030-74575-2_14](https://doi.org/10.1007/978-3-030-74575-2_14)
  prefs: []
  type: TYPE_NORMAL
- en: Automated Methods for Detection and Classification Pneumonia Based on X-Ray
    Images Using Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Khalid El Asnaoui^([1](#Aff7)  ), Youness Chawki^([2](#Aff8)) and Ali Idri^([3](#Aff9))(1)National
    School of Applied Sciences (ENSAO), Department of Electronics, Computer Sciences,
    and Telecommunications, Laboratory Smart Information, Communication and Technologies
    “SmarICT Lab”, Mohammed First University, BP: 669, 60000 Oujda, Morocco(2)Faculty
    of Sciences and Techniques, Moulay Ismail University, Errachidia, Morocco(3)Software
    Project Management Research Team, ENSIAS, Mohammed V University, Rabat, Morocco'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recently, researchers, specialists, and companies around the world are rolling
    out deep learning and image processing-based systems that can fastly process hundreds
    of X-Ray and Computed Tomography (CT) images to accelerate the diagnosis of pneumonia
    such as SARS, covid-19, etc., and aid in its containment. Medical image analysis
    is one of the most promising research areas; it provides facilities for diagnosis
    and making decisions of several diseases such as MERS, covid-19, etc. In this
    paper, we present a comparison of recent deep convolutional neural network (CNN)
    architectures for automatic binary classification of pneumonia images based on
    fined tuned versions of (VGG16, VGG19, DenseNet201, Inception_ResNet_V2, Inception_V3,
    Resnet50, MobileNet_V2 and Xception) and a retraining of a baseline CNN. The proposed
    work has been tested using chest X-Ray & CT dataset, which contains 6087 images
    (4504 pneumonia and 1583 normal). As a result, we can conclude that the fine-tuned
    version of Resnet50 shows highly satisfactory performance with rate of increase
    in training and testing accuracy (more than 96% of accuracy).
  prefs: []
  type: TYPE_NORMAL
- en: KeywordsComputer-aided diagnosisPneumonia automatic detectionCT and X-Ray imagesPneumoniaCoronavirusCovid-19Deep
    learning
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Epidemics and chronic diseases have killed numerous individuals throughout history
    and caused significant emergencies that have set aside a long effort to survive.
    Two words are utilised epidemic and outbreak to portray a malady inside populaces
    that emerge over a timeframe [[1](#CR1), [2](#CR2)]. Indeed, we can define epidemic
    as the occurrence of more cases of illnesses, injury or other health condition
    than expected in a given area or among a specific group of persons during a specific
    period. For the most part, the cases are pretending to have a common cause [[2](#CR2)].
    The outbreak is distinguished from an epidemic as more localized or less likely
    to evoke public panic.
  prefs: []
  type: TYPE_NORMAL
- en: Past epidemics include pneumonia. The pneumonia is an infection of the lungs,
    most often caused by a virus or bacteria. The infection affects the pulmonary
    alveoli, the tiny balloon-shaped sacs at the end of the bronchioles (Fig. [1](#Fig1)).
    It usually affects only one of the lung’s 5 lobes (3 lobes in the right lung and
    2 in the left), hence the term lobar pneumonia. When pneumonia also reaches the
    bronchial tubes, it is called “Bronchopneumonia”. It is the most important cause
    of death in the world for children younger than 5 years (about 12.8% of annual
    deaths) [[3](#CR3), [4](#CR4)]. It is also a leading cause of morbidity and mortality
    in adults worldwide and in particular in China [[5](#CR5)–[7](#CR7)]. Pneumonia
    is the third leading cause of death in Japan with a higher mortality rate for
    the elderly, particularly among individuals ≥80 years old [[8](#CR8)]. Excluding
    lung cancer, in Portugal, Pneumonia is the huge cause of respiratory death [[9](#CR9)].![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig1_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig1_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 1
  prefs: []
  type: TYPE_NORMAL
- en: Pneumonia diagram
  prefs: []
  type: TYPE_NORMAL
- en: 'Several Coronavirus have passed over the species barrier to cause deadly pneumonia
    in humans since the beginning of the 21st century. To know the pathogenesis of
    these deadly epidemics, the specialists need to inspect the structure of the infections
    and its component. This permits them to explain and provide information for the
    development of effective treatment and possibly vaccines [[10](#CR10)]. Based
    on Table [1](#Tab1), that shows the major pandemics that have occurred over time.
    We will summarise the epidemiology and history of the type of Coronavirus in particular:
    SARS, MERS and Covid-19\. SARS-Cov (Severe Acute Respiratory Syndrome Coronavirus)
    [[11](#CR11), [12](#CR12)] is an acute respiratory illness caused by a coronavirus,
    characterized by fever, coughing, breathing difficulty, and usually pneumonia.
    SARS appeared first time in China exactly in the province of Guangdong in 2002
    and spread to the world through air travel routes. Approximately 8098 people were
    affected, causing 774 deaths [[13](#CR13), [14](#CR14)] with a lethal rate of
    about 10% [[15](#CR15)]. It is suggested to originate from bats [[13](#CR13),
    [16](#CR16)]. SARS symptoms are usually the same as flu symptoms: fever, chills,
    muscle aches, headache and occasionally diarrhea. After about one-week, other
    symptoms appear like fever of 38 °C or higher, dry cough, breath shortness [[15](#CR15)].Table
    1'
  prefs: []
  type: TYPE_NORMAL
- en: Major pandemics that have occurred over time
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Time period | Type/Pre-human host | Death toll |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Spanish Flu | 1918–1919 | H1N1 virus/Pigs | 40–50M |'
  prefs: []
  type: TYPE_TB
- en: '| Asian Flu | 1957–1958 | H2N2 virus | 1.1M |'
  prefs: []
  type: TYPE_TB
- en: '| Hong Kong Flu | 1968–1970 | H3N2 virus | 1M |'
  prefs: []
  type: TYPE_TB
- en: '| HIV/AIDS | 1981-Present | Virus/Chimpanzees | 25–35M |'
  prefs: []
  type: TYPE_TB
- en: '| Swine Flu | 2009–2010 | H1N1 virus/Pigs | 200 000 |'
  prefs: []
  type: TYPE_TB
- en: '| SARS | 2002–2003 | Coronavirus/Bats, Civets | 774 |'
  prefs: []
  type: TYPE_TB
- en: '| Ebola | 2014–2016 | Ebolavirus/Wild animals | 11 000 |'
  prefs: []
  type: TYPE_TB
- en: '| MERS | 2015-Present | Coronavirus/Bats, Camels | 850 |'
  prefs: []
  type: TYPE_TB
- en: '| Covid-19 | 2019-Present | Coronavirus-Unknown (possibly Batsor pangolins)
    | Coronavirus Cases:89.711.341Deaths: 1,936,554Recovered: 64,572,624January 10,
    2021, 11:36 GMT |'
  prefs: []
  type: TYPE_TB
- en: 'MERS-Cov (Middle East Respiratory Syndrome Coronavirus) is a viral respiratory
    illness caused by a virus [[17](#CR17)]. It appeared first in the Middle East
    and exactly in Saudi Arabia in 2012 [[18](#CR18), [19](#CR19)]. Other cases were
    identified in Jordan [[20](#CR20)], Qatar [[21](#CR21)] then spread to the world.
    MERS is a zoonotic virus that can be transmitted between animals and humans. Indeed,
    the World Health Organization has confirmed that humans are affected by contact
    with affected dromedary camels [[22](#CR22)–[24](#CR24)]. Studies have shown that
    the way the virus is transmitted from animals to humans is not yet understood,
    and the human-to-human transmission is very limited unless there is close contact
    [[17](#CR17), [25](#CR25), [26](#CR26)]. The different MERS symptoms are as follows:
    Fever, Cough (Dry, Productive), Shortness of breath, Diarrhea, Myalgia, Headache,
    Nausea, Vomiting, Abdominal pain, Chest pain, Sore throat, Hemoptysis [[17](#CR17),
    [21](#CR21), [27](#CR27)–[30](#CR30)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The world is currently experiencing a dangerous viral epidemic caused by a
    virus that has killed tens of thousands of people. This new virus called Covid-19
    was identified in Wuhan [[5](#CR5), [31](#CR31)–[41](#CR41)], China, in December
    2019\. It belongs to the Corona family of viruses, but it is more deadly and dangerous
    than the rest of the coronaviruses [[42](#CR42), [43](#CR43)]. First cases of
    the disease have been related to a live animal seafood market in Wuhan, denoting
    to a zoonotic origin of the epidemic [[36](#CR36), [41](#CR41), [44](#CR44)–[47](#CR47)].
    The routes of transmission, treatments, and results of Covid-19 continually receiving
    much research attention in the world [[31](#CR31)]. Indeed, researchers have identified
    three main modes of virus transmission: close person-to-person contact, aerosol
    transmission and transmission by touch [[10](#CR10), [42](#CR42), [48](#CR48),
    [49](#CR49)]. The Coronavirus is very dangerous because it can have up to two
    weeks of incubation without symptoms. We can cite the symptoms of Covid-19: high
    fever, dry cough, tiredness, shortness of breath, aches and pains, sore throat
    and very few people will report diarrhea, nausea or a runny nose [[10](#CR10),
    [43](#CR43), [50](#CR50)]. As the number of patients infected by this disease
    increases, it becomes increasingly complex for radiologists to finish the diagnostic
    process in the constrained accessible time [[51](#CR51)]. Medical images analysis
    is one of the most promising research areas; it provides facilities for diagnosis
    and making decisions of a number of diseases such as MERS, COVID-19\. Recently,
    many efforts and more attention are paid to imaging modalities and Deep Learning
    (DL) in pneumonia. Therefore, interpretation of these images requires expertise
    and necessitates several algorithms in order to enhance, accelerate and make an
    accurate diagnosis. Following this context, DL algorithms [[52](#CR52)] have obtained
    better performance in detecting pneumonia and demonstrated high accuracy compared
    with the previous state of the art methods. Motivated by the fastest and accurate
    detection rate of pneumonia using DL, our work will present a comparison of recent
    deep convolutional neural network architectures for automatic binary classification
    of X-Ray and CT images between normal and pneumonia in order to answer the following
    research questions: **(1)**. Are there any DL techniques which distinctly outperforms
    other DL techniques? **(2)**. Can DL used to early screen pneumonia from CT and
    X-Ray images? **(3)**. What is the diagnostic accuracy that DL can be attained
    based on CT and X-Ray images?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our paper’s contributions are as follows: **(1)** We design fined tuned versions
    of (VGG16, VGG19, DenseNet201, Inception_ResNet_V2, Inception_V3, Xception, Resnet50,
    and MobileNet_V2) and retraining of a baseline CNN. **(2)** To avoid over-fitting
    in different models, we used weight decay and L2-regularizers. **(3)** The various
    models have been tested on chest X-Ray & CT datasets [[53](#CR53), [54](#CR54)]
    for binary classification and outperform state-of-the-art algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of this paper is organized as follows. Section [2](#Sec2) deals
    with some related work. In Sect. [3](#Sec3), we describe our proposed method.
    Section [4](#Sec6) presents some results obtained and interpreting the results.
    The conclusions are given in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Up to this point, there is no compelling method to prevent the occurrence of
    lung abnormalities such as cancer and pneumonia. Therefore, early detection and
    accurate screening methods the most punctual indications of lung abnormalities
    are the initial steps to limit the risk of suffering. In this section, a brief
    review of some important contributions from the existing literature is presented.
    Pneumonia remains one of the diseases that is increasingly becoming research hotspots
    in recent years. Indeed, Toğaçar et al. [[55](#CR55)] employed Convolutional Neural
    Network (CNN) as feature extractor based on lung X-Ray images and used some existing
    CNN models like AlexNet, VGG16 and VGG19 for classification between normal and
    pneumonia. Using the algorithm of minimum redundancy maximum relevance, the authors
    were able to reduce the number of deep features. A step of classification was
    then done using a decision tree, k-NN, linear discriminant analysis, linear regression,
    and SVM. Liang and Zeng [[56](#CR56)] proposed a new deep learning framework to
    classify child pneumonia image by combining residual thought and dilated convolution.
    Thereby, to overcome the over-fitting and the model’s degradation problems, the
    proposed method used a residual structure. The authors used also dilated convolution
    to resolve the issue of loss of feature space information breed by the increment
    in depth of the model. A deep learning method to identify and localize the pneumonia
    in Chest X-Rays images has been suggested by [[57](#CR57)]. The identification
    model is based on Mask-RCNN that can incorporate global and local features for
    pixel-wise segmentation. The investigation of post-stroke pneumonia prediction
    models using advanced machine learning algorithms, specifically deep learning
    approaches has been presented in [[58](#CR58)]. Indeed, the authors have used
    the classical classification methods (logistic regression, support vector machines,
    and extreme gradient boosting). They also implemented methods based on multiple
    layer perceptron neural networks and recurrent neural networks to use the temporal
    sequence information in electronic health record systems. The obtained results
    showed that the deep learning-based predictive model achieved the optimal performance
    compared to many classical machine learning methods. In [[59](#CR59)], the authors
    proposed an automated detection and localization method of pneumonia on chest
    X-Ray images using machine learning solutions. They presented two CNN (RetinaNet
    and Mask R-CNN). The proposed method was validated on a dataset of 26,684 images
    from Kaggle Pneumonia Detection Challenge. Bhandary et al. [[52](#CR52)] have
    reported a deep learning framework for examining lung pneumonia and cancer. Thus,
    they proposed two different deep learning techniques: the first one was a Modified
    AlexNet (MAN). It was intended to classify chest X-Ray images into normal, and
    pneumonia class using Support Vector Machine and its performance was validated
    with pre-trained deep learning (AlexNet, VGG16, VGG19 and ResNet50). Simultaneously,
    the second method implemented a fusion of handcrafted and learned features in
    the MAN to improve classification accuracy during lung cancer assessment. To assist
    radiologists for better diagnosis, [[60](#CR60)] suggested a method for detection
    consolidations in chest X-Ray images using deep learning. Authors have used a
    deep convolutional neural network pre-trained with ImageNet data to improve the
    models’ accuracy. Then, to enhance the models’ generalisation, they proposed a
    three-step pre-processing approach: removing the confounding variables, histogram
    matching and improving the contrast of colorful image.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Proposed Contribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning methods have recently demonstrated huge potential with state-of-the-art
    performance on image processing and computer vision [[61](#CR61)]. These techniques
    have been applied in various medical imaging modalities with high performance
    [[62](#CR62)] in segmentation, detection, and classification. Some DL methods
    incorporate skin cancer detection, breast cancer detection, and classification,
    lung cancer detection [[62](#CR62)], etc. Even though these methods have shown
    huge achievement in medical imaging success, they require a large amount of data,
    which is yet not available in this field of applications. Following the context
    of no availability of medical imaging dataset and motivated by the success of
    deep learning and medical image processing, our work is going to deeply compare
    different fine-tuned [[52](#CR52)] architectures: (VGG16, VGG19, DenseNet201,
    Inception_ResNet_V2, Inception_V3, Xception, Resnet50, and MobileNet_V2). The
    following sections detail the proposed models.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Proposed Baseline CNN Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generally, a CNN model consists of five layers: input layer, convolutional
    layers, pooling layers, full-connection layers, and output layer (Fig. [2](#Fig2)).
    Moreover, it is known that a CNN model can be trained end-to-end to allow the
    feature extraction and selection, and finally classification or prediction. Understanding
    how the network interprets an image and processes it is difficult. However, it
    has been shown that features extracted by the layers of a network work better
    than human-built features [[63](#CR63)].![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig2_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig2_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 2
  prefs: []
  type: TYPE_NORMAL
- en: The main architecture of our baseline CNN
  prefs: []
  type: TYPE_NORMAL
- en: 'The proposed baseline CNN for our experiment has the following architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input layer: In our experiment, the inputs are X-Ray and CT images. The parameters
    are defining the image dimension (244 × 244).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Convolutional layers: a convolution is a linear operation consisting of a set
    of weights with the input. It is designed for two-dimensional input; the multiplication
    is performed between a two-dimensional array of weights (filters) and an array
    of input data. In the proposed architecture we have 3 layers with a filter of
    size 3 × 3 and zero padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pooling Layers: represent a technique to down-sample feature maps by summarizing
    the presence of features in patches of the feature map. There are two types of
    pooling methods that are average pooling and max pooling. In the proposed architecture,
    we used max-pooling in order to calculate the maximum value in each patch for
    every feature map. The max-pooling is set to 2 × 2 with a stride of 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rectified Linear Unit (ReLU) layers: we have used 4 ReLU layers for each convolutional
    layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fully connected layers: They treat the input data as a simple vector and produce
    an output as a single vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.2 Deep Learning Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning architectures are highly used to diagnose pneumonia since 2016
    [[52](#CR52), [53](#CR53)], the most investigated DL techniques are VGG16, VGG19,
    Inception_V3, DenseNet201, Xception, Resnet50, Inception_ResnetV_2, and MobileNet_V2\.
    We have chosen these 8 techniques due to the accuracies they offer.
  prefs: []
  type: TYPE_NORMAL
- en: '**VGG16 and VGG19**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proposed in 2014 by Simonyan and Zisserman, Visual Geometry Group (VGG) is a
    convolutional neural network architecture that won the ILSVR competition in 2014
    [[64](#CR64)]. The major characteristic of this architecture is that instead of
    having a large number of hyper-parameters, they concentrated on simple 3 × 3 size
    kernels in the convolutional layers and 2 × 2 size in the max-pooling layers.
    In the end, it has 2 Fully Connected (FC) layers trailed by a softmax for output.
    The most familiar VGG models are VGG16 and VGG19, which include 16 and 19 layers,
    respectively. The difference between VGG16 and VGG19 is that VGG19 has one more
    layer in each of the three convolutional blocks [[65](#CR65)].
  prefs: []
  type: TYPE_NORMAL
- en: '**Inception_V3**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception models are a type of Convolutional Neural Networks developed by Szegedy
    in 2014 [[66](#CR66)]. The inception models differ from the ordinary CNN in the
    structure where the inception models are inception blocks that mean lapping the
    same input tensor with multiple filters and concatenating their results. Inception_V3
    is a new version of the inception model presented for the first time in 2015 [[67](#CR67)].
    It is an improved version of inception_V1 and inception_V2 with more parameters.
    Indeed, it has a block of parallel convolutional layers with 3 different sizes
    of filters (1 × 1, 3 × 3, 5 × 5).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, 3 × 3 max pooling is also performed. The outputs are concatenated
    and sent to the next inception module. This model accepts an input image size
    of 299 × 299 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '**Resnet50**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resnet50 is a deep residual network developed by [[68](#CR68)] and is a subclass
    of convolutional neural networks used for image classification. It is the winner
    of ILSVRC 2015\. The principal innovation is the introducing of the new architecture
    network-in-network using residual layers. The Resnet50 consists of five steps,
    each with a convolution and identity block, each convolution block and each identity
    block have 3 convolution layers. Resnet50 has 50 residual networks and accepts
    images size of 224 × 224 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inception_ResNet_V2**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception_ResNet_V2 is a convolutional neural network trained on more than a
    million images from the ImageNet database [[69](#CR69)]. It is a hybrid technique
    combining the inception structure and the residual connection. The model accepts
    images of 299 × 299 image, and its output is a list of estimated class probabilities.
    The advantages of Inception_Resnet_V2 are converting inception modules to Residual
    Inception blocks, adding more Inception modules and adding a new type of Inception
    module (Inception-A) after the Stem module.
  prefs: []
  type: TYPE_NORMAL
- en: '**DenseNet201**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense Convolutional Network (DenseNet201) is a convolutional neural network
    with 201 layers deep and accepts an input image size of 224 × 224 [[70](#CR70)].
    DenseNet201 is an improvement of ResNet that includes dense connections among
    layers. It connects each layer to every other layer in a feed-forward fashion.
    Unlike traditional convolutional networks with L layers that have L connections,
    DensNet201 has L(L + 1)/2 direct connections. Indeed, compared to traditional
    networks, DenseNet can improve the performance by increasing the computation requirement,
    reducing the number of parameters, encouraging feature reuse and reinforcing feature
    propagation.
  prefs: []
  type: TYPE_NORMAL
- en: '**MobileNet**_**V2**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MobileNet_V2 [[71](#CR71)] is a convolutional neural network being an improved
    version of MobileNet_V1\. It is made of only 54 layers and has an input image
    size of 224 × 224\. Its main characteristic is instead of performing a 2D convolution
    with a single kernel, instead of performing a 2D convolution with a single kernel.
    It uses depthwise separable convolutions that consist of applying two 1D convolutions
    with two kernels. That means, less memory and parameters are required for training
    leading to a small and efficient model. We can distinguish two types of blocks:
    the first one is a residual block with a stride of 1, the second one is block
    with a stride of 2 for downsizing. For each block, there are three layers: the
    first layer is 1 × 1 convolution with ReLU6, the second layer is the depthwise
    convolution, and the third layer is another 1 × 1 convolution but without any
    non-linearity.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Xception**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xception, presented by Chollet [[72](#CR72)], is a convolutional neural network
    that is 71 layers deep. It is an improved version of Inception architecture and
    involves depthwise separable convolutions. More precisely, Xception replaces the
    standard Inception modules with depthwise separable convolutions. It showed good
    results compared to VGG16, Resnet and Inception in classical classification problems.
    Xception has an input image size of 299 × 299.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Results and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We compared all mentioned above models once they have been fine-tuned for the
    automatic binary classification on two new publicly available image datasets (chest
    X-Ray & CT dataset [[53](#CR53), [54](#CR54)]). As it can be observed in Fig. [3](#Fig3),
    which shows the diagram of the main steps necessary to compare the different models:
    data acquisition, data pre-processing, training and classification. The following
    sections give out in detail the steps of this comparison.![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig3_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig3_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3
  prefs: []
  type: TYPE_NORMAL
- en: Block diagram of the process of X-Ray and CT classification
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This present work introduces two publicly available image datasets which contain
    X-Ray and computed tomography images. The first dataset [[53](#CR53)] is a chest
    X-Ray & CT dataset composed of 5856 images with two categories (4273 pneumonia
    and 1583 normal) while the second one is named Covid Chest X-ray Dataset [[54](#CR54)]
    containing 231 Covid-19 Chest X-Ray images. We joined the second dataset to the
    first one to form a joint dataset which finally composed of 6087 images (jpeg
    format) and has two classes (4504 pneumonia and 1583 normal). The pneumonia class
    contains images of bacterial pneumonia, viral pneumonia and Covid19\. As can be
    seen from Fig. [4](#Fig4) that illustrates an example of chest X-Rays in patients
    with pneumonia, the normal chest X-Ray (Fig. [4](#Fig4)(a)) shows clear lungs
    with no zones of abnormal opacification. Moreover, Fig. [4](#Fig4)(b) shows a
    focal lobar consolidation (white arrows). Also, Fig. [4](#Fig4)(c) shows a more
    diffuse “interstitial” pattern in both lungs [[53](#CR53)] while Fig. [4](#Fig4)(d)
    presents an image of a patient infected by covid19 [[54](#CR54)].![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig4_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig4_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 4
  prefs: []
  type: TYPE_NORMAL
- en: Examples of Chest X-Rays in patients with pneumonia
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Data Pre-processing and Splitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next stage is to pre-process input images using different pre-processing
    techniques. The motivation behind image pre-processing is to improve each input
    image’s quality of visual information (to eliminate or decrease noise present
    in the original input image, enhance the quality of image through increased contrast,
    and delete the low or high freqes etc.). In this study, we used intensity normalization
    [[73](#CR73)] and Contrast Limited Adaptive Histogram Equalization (CLAHE) [[74](#CR74),
    [75](#CR75)]. Intensity normalization is a straightforward pre-processing step
    in image processing applications [[73](#CR73)]. In our contribution, we normalize
    the input image (Fig. [5](#Fig5)(b)) to the standard normal distribution using
    min-max normalization (Eq. [1](#Equ1)).![$$X_{norm \, } = \frac{{x - x_{\min }
    }}{{x_{\max } - x_{\min } }}$$](../images/507793_1_En_14_Chapter/507793_1_En_14_Chapter_TeX_Equ1.png)(1)![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig5_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig5_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 5
  prefs: []
  type: TYPE_NORMAL
- en: X-ray image pre-processing
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, before feeding input image into the proposed models, CLAHE is necessary
    to improve the contrast in images [[74](#CR74), [75](#CR75)]. Figure [5](#Fig5)
    illustrates an example of using these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: For data splitting, we used in this experiment 60% of the images for training
    and 40% of the images for testing. We ensure that the images chosen for testing
    are not used during training to perform the binary classification task successfully.
    Moreover, we observed that the dataset is imbalanced. Thereby 75% of the images
    represent the pneumonia class. To overcome this issue, we resampled the dataset
    by using data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Data Augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data augmentation is used for the training process after dataset pre-processing
    and splitting and aims to avoid the risk of over-fitting. Moreover, the strategies
    we used include geometric transforms such as rescaling, rotations, shifts, shears,
    zooms and flips (Table [2](#Tab2)). We generated from each single input image
    2 new images with different augmentation techniques. Therefore, the total number
    of images in the normal class was increased by 2 times.Table 2
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation used
  prefs: []
  type: TYPE_NORMAL
- en: '| Argument | Parameter value | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rescale | 1/255.0 | Scale images from integers 0–255 to floats 0–1 |'
  prefs: []
  type: TYPE_TB
- en: '| Rotation range | 90 | Degree range of the random rotations |'
  prefs: []
  type: TYPE_TB
- en: '| Horizontal and Vertical shift range | 0.2 | The parameter value of horizontal
    and vertical shifts (20%) is a fraction of the given dimension |'
  prefs: []
  type: TYPE_TB
- en: '| Shear range | 0.2 | Controls the angle in counterclockwise direction as radians
    in which our image will allow to be sheared |'
  prefs: []
  type: TYPE_TB
- en: '| Zoom range | 0.2 | Allows the image to be “zoomed out” or “zoomed in” |'
  prefs: []
  type: TYPE_TB
- en: '| Horizontal flip | True | Controls when a given input is allowed to be flipped
    horizontally during the training process |'
  prefs: []
  type: TYPE_TB
- en: '| Fill mode | Nearest | This is the default option where the closest pixel
    value is chosen and repeated for all the empty values |'
  prefs: []
  type: TYPE_TB
- en: 4.4 Training and Classification Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After data pre-processing, splitting and data augmentation techniques, our training
    dataset size is increased and ready to be passed to the feature extraction step
    with the proposed models to extract the appropriate and pertinent features. The
    extracted features from each proposed model are flattened together to create the
    vectorized feature maps. The generated feature vector is passed to a multilayer
    perceptron to classify each image into corresponding classes. Finally, the performance
    of the proposed method is evaluated on test images using the trained model. We
    repeated each experiment three times and reported their average results.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Towards an automatic binary classification based on a publicly available image
    dataset (Chest X-Ray dataset [[53](#CR53), [54](#CR54)]), our experimentations
    were carried out based on following experimental parameters: All the images of
    the dataset were resized to 224 × 224 pixels except those of Inception_V3, Inception_Resnet_V2
    and Xception models that were resized to 299 × 299\. To train the models, we set
    the batch size to 32 with the number of epochs set to 300\. The training and testing
    samples are initiated to 159 and 109, respectively. Adam with β1 = 0.9, β2 = 0.999
    is used for optimization, and learning rate initiated to 0.00001 and decreased
    it to 0.000001\. Moreover, we used weight decay and L2-regularizers to reduce
    over-fitting for the different models. A fully connected layer was trained with
    the ReLU, followed by a dropout layer with a probability of 0.5\. We updated the
    last dense layer in all models to output two classes corresponding to normal and
    pneumonia instead of 1000 classes as was utilized for ImageNet. The implementation
    of the proposed models is done using a computer with Processor: Intel (R) core
    (TM) i7-7700 CPU @ 3.60 GHz and 8 GB in RAM running on a Microsoft Windows 10
    Professional (64-bit). For implementation, Keras/Tensorflow is used as deep learning
    backend. Our training and testing steps run using NVIDIA Tesla P40 with 24 GB
    RAM.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Evaluation Criteria
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After extracting the appropriate feature, the last step is to classify the
    attained data and assign it to a specific class [[76](#CR76)]. Among the different
    classification performance properties, and since the dataset is now balanced,
    our study uses the following benchmark metrics: accuracy (ACC), sensitivity (SEN),
    specificity (SPE), precision (PRE) and F1 score (F1) [[52](#CR52), [76](#CR76)].
    These metrics are defined as follows:![$$\begin{aligned} ACC &amp; = \frac{TP
    + TN}{{TP + TN + FP + FN}} \times 100\quad \quad PRE = \frac{TP}{{TP + FP}} \times
    100 \\ &amp; SPE = \frac{TN}{{TN + FP}} \times 100\quad \quad SEN = \frac{TP}{{TP
    + FN}} \times 100 \\ &amp; \quad \quad \quad \;F1 = 2 \times \frac{ \, Re call
    \, \times Pr e cision \, }{{ \, Re call \, + \, Pr e cision \, }} \times 100 \\
    \end{aligned}$$](../images/507793_1_En_14_Chapter/507793_1_En_14_Chapter_TeX_Equ2.png)(2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'where: TP stands for: True Positive. FP: False Positive.TN: True Negative,
    and FN: False Negative.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Results and Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section presents the results for the binary classification for the chest
    X-Ray & CT images [[53](#CR53), [54](#CR54)] with the following architectures
    (Baseline CNN, Fine-tuning the top layers of VGG16, VGG19, Inception_V3, Xception,
    Resnet50, Inception_Resnet_V2, DenseNet201, and MobileNet_V2). Also, to check
    each proposed model’s performance and robustness, several experiments are conducted
    on chest X-Ray dataset [[53](#CR53), [54](#CR54)]. The results are presented separately
    using training and testing curves of accuracy and loss and confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.1 Classification Results of the Different Architectures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This subsection presents and discusses the classification results of chest
    X-Ray & CT images [[53](#CR53), [54](#CR54)]. Before discussing these results,
    let us define some parameters related to the deep learning process: the training
    curve is calculated from the training dataset that provides an idea of how well
    the model is learning. In contrast, the testing curve is calculated from a hold-out
    testing dataset that explains how well the model is generalizing. Simultaneously,
    the training and testing loss are defined as a summation of the errors made for
    each example in testing or training sets. Note that in contrast to accuracy, loss
    is not a percentage. Furthermore, the confusion matrix shows a detailed representation
    of images after classification [[52](#CR52)]. To summarize, a model that generalizes
    well is a model that is neither over-fit nor under-fit.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Baseline CNN**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to the Fig. [6](#Fig6), it is observed that the accuracy curve of
    training data is rapidly increasing from epoch 0 to epoch 6 where the accuracy
    is equal to 83.17%, after that, it begins to increase slightly until epoch 300
    where the accuracy is equal to 86.28%. The same applies to the accuracy curve
    of testing data with an accuracy of 84.84 for epoch 300\. Concerning the loss
    curve of training data, it is rapidly decreasing from epoch 0 to epoch 6 where
    the loss is 43.77\. At that point, it begins to decrease slightly until the end
    of training (epoch 300) where the loss is equivalent to 36.56\. Same for loss
    curve of testing data with a loss of 37.85 for epoch 300\. From the confusion
    matrix, it is noted that the first images (Normal class), the model recognizes
    1634 images correctly, but 95 were marked as pneumonia. Likewise, for the second
    image’s class (Pneumonia), the model was capable to identify 1277 images correctly,
    unlike 252 images were marked as Normal.![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig6_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig6_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 6
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss curve and confusion matrix of Baseline CNN
  prefs: []
  type: TYPE_NORMAL
- en: '**VGG16**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure [7](#Fig7) presents the accuracy, loss curve and confusion matrix of
    VGG16\. Indeed, from the epoch 0 to epoch 11, the accuracy curve of training data
    is quickly increasing where it is equal to 81.05%, and then it converges to a
    value of 87.51%. The same applies to the accuracy curve of testing data with an
    accuracy of 86.32% for epoch 300\. A rapid decreasing of loss curve can be noted
    for training data from epoch 0 to epoch 25, where the loss is equivalent to 1.43\.
    At this epoch, a kind of stability can be observed up to the value of 1.15\. The
    same goes for the loss curve of testing data where the loss is equal to 2.21 for
    epoch 300\. The model can predict 1517 images correctly in the normal class from
    the confusion matrix, yet 212 were named pneumonia. For the Pneumonia class, the
    model was capable to identify 1466 images correctly, and 263 images were marked
    as Normal.![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig7_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig7_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 7
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss curve and confusion matrix of VGG16
  prefs: []
  type: TYPE_NORMAL
- en: '**VGG19**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As it is shown in Fig. [8](#Fig8), the curve of training data (testing data)
    can be divided into two intervals: the first one starts from epoch 0 to epoch
    13 (from epoch 0 to epoch 10). We can observe a quick increase in accuracy where
    the accuracy is equal to 81.01% (83.05%). In the second interval, the accuracy
    becomes stable and converges toward 87.42% (86.89%). For the loss curve of training
    and testing data, we see a good fit. Indeed, from epoch 0 to 18, the loss is rapidly
    decreasing, where it is equal to 1.27\. Afterwards, it begins to increase slightly
    until the end of the training, equivalent to 1.31\. As observed (see confusion
    matrix) in the normal class, the VGG19 model had the option to predict 1390 images
    correctly and 339 images as pneumonia. The model also was capable to classify
    1582 images as pneumonia and 147 images as Normal for the Pneumonia class.![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig8_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig8_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 8
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss curve and confusion matrix of VGG19
  prefs: []
  type: TYPE_NORMAL
- en: '**Inception_V3**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure [9](#Fig9) shows the accuracy, loss curve and confusion matrix of Inception_V3\.
    Thereby, for the training and testing accuracy and from epoch 0 to epoch 7, we
    can see that the accuracy is increasing until the value of 91.03%. After epoch
    7, the accuracy gets started to be stable where it is equal to 97.01% and 95.94%
    for training and testing data respectively. A good fit can be noticed for the
    loss curve of training data in either the quick increasing interval from epoch
    0 to epoch 32 where the loss is 3.98 or in the other interval where the decreasing
    is slow and converges to 1.76\. As shown in the confusion matrix, for the Pneumonia
    class, the model was able to identify 1650 images as pneumonia and 79 images as
    Normal. Concerning class Normal, Inception_V3 model can predict 1621 images as
    Normal and 108 as pneumonia.![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig9_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig9_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 9
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss curve and confusion matrix of Fine-tuned Inception_V3
  prefs: []
  type: TYPE_NORMAL
- en: '**ResNet50**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure [10](#Fig10) illustrates the results obtained by Resnet50\. In fact,
    from epoch 0 to 24, the accuracy values are increasing expeditiously either for
    training or testing data where the maximum value is 97.36%. After that, the values
    start to be stables (99.23% and 96.23% for training and testing data respectively).
    We observe a good fit for the loss curve of training and testing data, indeed,
    from epoch 0 to 21, the loss is briskly decreasing where it is equal to 6.89,
    afterwards, it gets started to be stable until the epoch 300 where it is equal
    to 0.85\. The confusion matrix indicates that, for images of Normal class, 1703
    images were predicted correctly as Normal and 26 were marked as pneumonia. Same
    for the second images of Pneumonia class, the model had the option to identify
    1638 images correctly, unlike 91 images were labeled as Normal.![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig10_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig10_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 10
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss curve and confusion matrix of Fine-tuned ResNet50
  prefs: []
  type: TYPE_NORMAL
- en: '**Inception_ResNet_V2**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure [11](#Fig11) shows that from epoch 0 to 18, the training and testing
    accuracy curve are increasing until the value of 95.51%. After epoch 18, the accuracy
    begins to be stable, equivalent to 99.11% and 96.41% for training and testing
    data. We can see an excellent fit for the loss curve for training and testing
    data where the values are 3.99 (epoch 24) and 1.17 (epoch 300). For the Pneumonia
    class, as depicted by the confusion matrix, the Inception_ResNet_V2 model was
    able to identify 1618 images correctly as pneumonia and 111 images as Normal.
    On other hand, for Normal class, 1705 were correctly classified as Normal and
    24 images as pneumonia.![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig11_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig11_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 11
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss curve and confusion matrix of Fine-tuned Inception_Res-Net_V2
  prefs: []
  type: TYPE_NORMAL
- en: '**DensNet201**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The obtained accuracy curve of training data is speedily increasing until the
    value of 93.49% (Fig. [12](#Fig12)). After epoch 16, the accuracy enters the stability
    stage where it is equivalent to 97.16% and 94.91% for training and testing data
    respectively. A good fit can be seen for the loss curve of training and testing
    data. In fact, from epoch 0 to epoch 17, the loss is quickly decreasing where
    it is equal to 3.99, and then it becomes stable until the epoch 300 where it is
    equal to 1.91\. The confusion matrix depicts that for the first images (Normal
    class) the model can recognize 1712 images correctly in the normal class, yet
    17 were named as pneumonia. The model also was able to identify 1527 images correctly,
    and 202 images were marked as Normal for the second images (Pneumonia class).![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig12_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig12_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 12
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss curve and confusion matrix of Fine-tuned DensNet201
  prefs: []
  type: TYPE_NORMAL
- en: '**MobileNet_V2**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As illustrated by Fig. [13](#Fig13), we can observe that from epoch 0 to 16,
    the training and testing accuracy are increasing until the value where the accuracy
    is equal to 96.38%. After epoch 16, the accuracy becomes stable and it is equal
    to 98.27% and 96.64% for training and testing data respectively. For the loss
    curve of training data, an excellent fit is noticed. Until the epoch 44, the value
    of loss is expeditiously decreasing where the value is 1.49\. Then it converges
    towards 0.24\. Regarding the confusion matrix, for the first images (Normal class),
    the model was able to identify 1696 images correctly in the normal class, but
    33 were classified as pneumonia. Likewise, in the Pneumonia class, 1634 images
    were labeled correctly as pneumonia and 95 were identified as Normal.![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig13_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig13_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 13
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss curve and confusion matrix of Fine-tuned MobileNet_V2
  prefs: []
  type: TYPE_NORMAL
- en: '**Xception**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is noted that the accuracy of training data is fastly increasing from epoch
    0 to 10 where the accuracy is equal to 93.10% (see Fig. [14](#Fig14)). Then it
    gets stable until the end of training where the accuracy is equal to 95.45%. For
    the testing data, a quick increasing can be seen from epoch 0 to 12 where the
    value is 86.87%, after that, it begins to decrease till 69.03% for epoch 300\.
    For the loss curve of training and testing data, the values are rapidly decreasing
    from epoch 0 to epoch 26, where the value is 1.10\. After epoch 26, the value
    converges to 0.44 and 0.69 for training and testing data respectively. When we
    see this confusion matrix, we can say that for the first images (Normal class),
    the model has the option to recognize 1656 images correctly. Moreover, 73 were
    selected as pneumonia. The model also can recognize 1219 images correctly (Pneumonia
    class). Thus 510 images were marked as Normal for the second images (Pneumonia
    class).![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig14_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig14_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 14
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss curve and confusion matrix of Fine-tuned Xception
  prefs: []
  type: TYPE_NORMAL
- en: 4.7.2 Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this study, we investigated the binary classification (Normal and pneumonia)
    based on X-Ray images using transfer learning of recent deep learning architectures
    to identify the best performing architecture based on the several parameters defined
    in Eq. ([2](#Equ2)). First, we individually compare the deep learning architectures
    by measuring their accuracies. After that, we compare each deep learning architecture’s
    accuracy and loss results to discern the outperforming architecture (Figs. [15](#Fig15),
    [16](#Fig16)). Moreover, Table [3](#Tab3) illustrates a comparison between the
    various deep learning models used in our experiments in terms of parameters defined
    in Eq. ([2](#Equ2)).![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig15_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig15_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 15
  prefs: []
  type: TYPE_NORMAL
- en: Summarization of the previous figures in term of accuracy curve for different
    architectures
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/507793_1_En_14_Chapter/507793_1_En_14_Fig16_HTML.png](../images/507793_1_En_14_Chapter/507793_1_En_14_Fig16_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 16
  prefs: []
  type: TYPE_NORMAL
- en: Summarization of the previous figures in term of Loss curve for different architectures
  prefs: []
  type: TYPE_NORMAL
- en: Table 3
  prefs: []
  type: TYPE_NORMAL
- en: Evaluations metrics in (%)
  prefs: []
  type: TYPE_NORMAL
- en: '|   | TP | TN | FN | FP | ACC | SEN | SPE | PRE | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline CNN | 1634 | 1277 | 452 | 95 | 84.18 | 78.33 | 93.07 | 94.05 | 85.66
    |'
  prefs: []
  type: TYPE_TB
- en: '| VGG16 | 1517 | 1466 | 263 | 212 | 86.26 | 85.22 | 87.36 | 87.73 | 86.46 |'
  prefs: []
  type: TYPE_TB
- en: '| VGG19 | 1390 | 1582 | 147 | 339 | 85.94 | 90.43 | 82.35 | 80.39 | 85.11 |'
  prefs: []
  type: TYPE_TB
- en: '| Xception | 1656 | 1219 | 510 | 73 | 83.14 | 76.45 | 94.34 | 95.77 | 85.03
    |'
  prefs: []
  type: TYPE_TB
- en: '| DensNet201 | 1712 | 1527 | 202 | 17 | 93.66 | 89.44 | 98.89 | 99.01 | 93.98
    |'
  prefs: []
  type: TYPE_TB
- en: '| Inception_V3 | 1621 | 1650 | 79 | 108 | 94.59 | 95.35 | 93.85 | 93.75 | 94.54
    |'
  prefs: []
  type: TYPE_TB
- en: '| Inception_ Resnet_V2 | 1705 | 1618 | 111 | 24 | 96.09 | 93.88 | 98.53 | 98.61
    | 96.19 |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNet_V2 | 1696 | 1634 | 95 | 33 | 96.27 | 94.61 | 98.02 | 98.06 | 96.30
    |'
  prefs: []
  type: TYPE_TB
- en: '| Resnet50 | 1703 | 1638 | 91 | 26 | 96.61 | 94.92 | 98.43 | 98.49 | 96.67
    |'
  prefs: []
  type: TYPE_TB
- en: For each model in Fig. [15](#Fig15)(a) and (b), which summarize the previous
    training and testing accuracy figures, the plots of training and testing accuracy
    increase to the point of stability. It is observed that fine-tuned version of
    Inception_Resnet_V2, Inception_V3, Resnet50, Densnet201 and Mobilenet_V2 show
    highly satisfactory performance with a rate of increase in training and testing
    accuracy with each epoch. They outperform the baseline CNN, Xception, VGG16 and
    VGG19 that demonstrate low performance. From epoch 20, they start to be stable
    until the end of training where the training and testing accuracy of baseline
    CNN, VGG16 and VGG16 are equal to 85%. However, Xception reaches 83% in testing
    accuracy and 95% in training accuracy. In this case, the predictive model produced
    by Xception algorithm does not adapt well to the training set (Over-fitting).
    Besides, the plots of training and testing loss (Fig. [16](#Fig16)(a) and (b))
    decrease to the point of stability for each proposed model. As can be seen, the
    fine-tuned version of the models shows highly satisfactory performance with the
    rate of decrease in training and testing loss with each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Results for our multi-experiment classification are tabulated in Table [3](#Tab3)
    based on different fine-tuned versions of recent deep learning architectures.
    The table depicts in detail classification performances across each experiment.
    From the results, it is noted that the accuracy when we use Xception, baseline
    CNN, VGG19 and VGG16 are low compared with other DL architectures, since these
    last models help to obtain respectively 83.14%, 84.18%, 85.94% and 86.26% of accuracy.
    Unlike, the highest accuracies are reported by DensNet201 (93.66%), Inception_V3
    (94.59%), Inception_Resnet_V2 (96.09%), MobileNet_V2 (96.27%) and Resnet50 (96.61%).
    In addition, MobileNet_V2 has been proven to obtain remarkable results in related
    tasks [[77](#CR77)] whereas ResNet50 [[68](#CR68), [78](#CR78)] provides a good
    combination of performance and number of parameters and has proved faster training.
    Therefore, we recommend the MobileNet_V2 (96.27% of accuracy) and Resnet50 (96.61%
    of accuracy) models to be used for the Computer-Aided Diagnosis systems to identify
    the health status of patients against pneumonia in X-ray and CT images, since
    the best scores of training and testing accuracy were obtained. Clinical examinations
    are the following step of this research work.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusions and Future Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this work, we presented automated methods used to classify the chest X-Ray
    & CT images into pneumonia and the normal class using eight deep learning architectures
    (VGG16, VGG19, DenseNet201, Inception_ResNet_V2, Inception_V3, Xception, Resnet50,
    and MobileNet_V2) and a baseline CNN. The main goal is to answer the following
    research questions: (1). Are there any DL techniques which distinctly outperforms
    other DL techniques? (2). Can DL use to early screen pneumonia from CT and X-Ray
    images? (3). What is the diagnostic accuracy that DL can be attained based on
    CT and X-Ray images?. The experiments were conducted using chest X-Ray & CT dataset,
    which contains 6087 images (4504 pneumonia and 1583 normal). The pneumonia class
    contains images of bacterial pneumonia, viral pneumonia and Covid19\. Moreover,
    the performances of these experiments were evaluated using various performance
    metrics. Furthermore, the obtained results show that the Resnet50 gave high performance
    (accuracy is more than 96%) against other architectures cited in this work (accuracy
    is lower than 96%). Due to these models’ high performance, we believe that these
    results help doctors make decisions in clinical practice.'
  prefs: []
  type: TYPE_NORMAL
- en: Ongoing work intends to develop a full system for pneumonia via deep learning
    detection, segmentation, and classification. In addition, the performance may
    be improved using more datasets, more sophisticated feature extraction techniques
    such as color [[79](#CR79)], texture [[80](#CR80)], shape [[81](#CR81), [82](#CR82)].
    In addition, the performance may be improved using more datasets, more sophisticated
    feature extraction techniques. Also other fusion approaches would be interesting
    [[83](#CR83)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Authors’ Contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: The experiments and the programming stage were carried out by Khalid El Asnaoui.
    All authors wrote the paper, and all approve this submission.
  prefs: []
  type: TYPE_NORMAL
