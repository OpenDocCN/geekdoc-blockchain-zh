- en: © The Author(s), under exclusive license to APress Media, LLC, part of Springer
    Nature 2022J. T. GeorgeIntroducing Blockchain Applications[https://doi.org/10.1007/978-1-4842-7480-4_17](https://doi.org/10.1007/978-1-4842-7480-4_17)
  prefs: []
  type: TYPE_NORMAL
- en: 17. AI and Blockchain Monitoring Autonomous Vehicles Management Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Joseph Thachil George^([1](#Aff2)  )(1)Rome, Italy
  prefs: []
  type: TYPE_NORMAL
- en: The autonomous vehicle management system is the subject of this chapter. This
    entails operating a vehicle or system in a distributed environment. *Artificial
    intelligence* and message exchanges (blockchain) are both present in this scenario.
    Simultaneously, we must prevent catastrophic collapse when it comes to autonomous
    vehicle management.
  prefs: []
  type: TYPE_NORMAL
- en: 17.1 The Connection Between Blockchain and Artificial intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Blockchain and AI are two of the most popular technological developments right
    now. Scientists have been debating and researching the integration of the two
    technologies, despite the fact that their development partners and implementations
    are vastly different. A blockchain, by definition, is a distributed, decentralized,
    immutable ledger for storing encrypted data. AI, on the other hand, is the motor
    or “mind” that will enable analysis and judgment based on the acquired information^([1](#Fn1)).
  prefs: []
  type: TYPE_NORMAL
- en: AI and blockchain are in a position where they can support and profit from each
    other. Because both of these technologies may affect data in various ways, combining
    them makes logical sense and potentially pushes data exploitation to new heights.
    Simultaneously, incorporating machine learning and AI into blockchain, and vice
    versa, can improve blockchain’s fundamental architecture while also enhancing
    AI’s capabilities. Blockchain could also make AI more logical and intelligible,
    allowing developers to track and comprehend why deep learning choices are made.
    The blockchain and its ledger can keep track of all the data and factors that
    go into a deep learning conclusion (see Figure [17-1](#Fig1))¹.![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig1_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig1_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-1
  prefs: []
  type: TYPE_NORMAL
- en: Blockchain and AI
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, AI can improve blockchain efficiency considerably more effectively
    than people or even traditional technology. A glance at how blockchains are now
    operated on conventional computers demonstrates this, with a significant amount
    of computing power required to complete even basic activities^([2](#Fn2)).
  prefs: []
  type: TYPE_NORMAL
- en: 17.1.1 AI and Blockchain in Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The number of automobiles on the road is rising these days. As a result, preventing
    traffic accidents is a problem for society. Machine Learning (ML) techniques,
    for example, are particularly useful in improving the overall performance of the
    road safety management system. Blockchain uses consensus methods and smart contracts
    to govern communication between nodes without the need for a third-party intermediary.
    Simultaneously, AI has the potential to provide intelligent, decision-making robots
    that are comparable to human minds².
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, we need to consider the following two aspects when we apply blockchain
    and AI for application development:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monetization of data.** With consensus algorithms and smart contracts, blockchain
    manages communication among nodes without the involvement of a third-party or
    intermediary body. Additionally, blockchain technology facilitates sharing of
    information on the network, which is decentralized, secure, persistent, anonymous,
    and trustworthy.²'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision making using AI.** Artificial Intelligence (AI) such as Machine
    Learning (ML) algorithms are very helpful for improving the performance of the
    overall vehicle safety management system.²'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 17.1.2 AI’s Role in Making Real-Time Intelligent and Decision-Making Machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AI technology enables machines to think for themselves without the need for
    human involvement. The AI approach is extremely careful in analyzing the collected
    data via IoT devices that have been placed within a vehicle’s driver’s cabin.
    For real-time decision-making operations in vehicles, several machine-learning
    techniques are significant.
  prefs: []
  type: TYPE_NORMAL
- en: The system evaluates the driver’s unfit or uncomfortable state based on the
    system’s training. Then, first and foremost, it communicates with the driver.
    After that, if it detects the vehicle’s uncomfortable body expressions, it sends
    a report to the pre-programmed network. This chapter explains how to implement
    AI in vehicle system management^([3](#Fn3)).
  prefs: []
  type: TYPE_NORMAL
- en: 17.2 Blockchain for Information Sharing and Exchange
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Blockchain is critical for sharing  and transferring data across IoT device
    endpoints. It manages the wireless communication network between the various network
    control locations, systems, and servers using BC. BC is an IoT backbone technology
    that collects data and distributes it to endpoints or final nodes. As is generally
    known, BC offers IoT nodes and stakeholders with traceability, trust, privacy,
    security, and transparency when sharing information.
  prefs: []
  type: TYPE_NORMAL
- en: Blockchain allows providers and Internet firms to share data and approve interoperability
    based on customer privacy data. It also shares data sharing records in order to
    authorize data access. Data traceability is also important to guarantee that data
    exchange is legitimate, controlled, auditable, and regulated³.
  prefs: []
  type: TYPE_NORMAL
- en: The information should be communicated among the endpoints with trust, security,
    and transparency via this blockchain system. To achieve data access permission,
    blockchain shares data sharing records. Vehicle drivers have their own private
    access control mechanisms and the ability to share or sell their data, as shown
    in Figure [17-2](#Fig2). Here, the carrier or communication service provider (CSP)
    provides benefits such as (1) no private data is sent out, (2) confirmation information
    may be verified and modified, and (3) traceable and tamper-proof information.³![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig2_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig2_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-2
  prefs: []
  type: TYPE_NORMAL
- en: Data transfer via blockchain technology  in the vehicle management system
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we created a project (autonomous vehicle management) that combines
    artificial intelligence and blockchain technology. The goal is to focus on artificial
    intelligence because the previous chapter talked a lot about blockchain technology.
    This is a combination of most trending technologies such as artificial intelligence,
    blockchain, and IoT. There are lots of examples in Western countries where both
    technologies are implemented in application development.
  prefs: []
  type: TYPE_NORMAL
- en: In all Western countries, self-driving vehicles are now one of the primary axes
    of mobility development. The European Commission is dedicated to supporting any
    connected and automated mobility solutions that can help meet the previously established
    sustainability and safety goals in its smart and sustainable mobility plan.
  prefs: []
  type: TYPE_NORMAL
- en: Urban tram and train options with self-driving capabilities currently exist
    for public transportation. We may see autonomous taxis with digital car-sharing
    systems develop in the next several years, operating in limited regions. In terms
    of private transportation, the objective of producing a fully driverless car is
    still a long way off, especially in congested areas. However, in the next few
    years, automobiles with increasingly advanced autonomous driving functions will
    be available for purchase. Several cities might potentially provide the infrastructure
    needed to develop environmental digital systems that allow for the deployment
    of some autonomous driving features within a decade. Even if the change will be
    gradual, let’s expect a totally new path.
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous driving is, at the moment, one of the biggest challenges facing the
    automotive world. Creating cars with artificial intelligence developed to the
    point of being able to drive without human intervention and capable of making
    fundamental decisions in a few thousandths of a second is by no means simple.
    In the test phase on the roads open to traffic, there was no shortage of accidents
    involving self-driving cars, but step by step we are arriving at increasingly
    advanced models capable of not requiring human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages of autonomous vehicle management include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multitasking:** The driver can devote themself to a completely different
    activity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety:** Sensors and predictive algorithms will allow self-driving cars
    to assess and in some cases predict risks. Thanks to safe driving, the number
    of road accidents would decrease.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency:** Abrupt braking and sudden acceleration can be avoided, thus
    optimizing fuel consumption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Less traffic:** Once on the road, vehicles would continuously communicate
    with each other, exchanging data on position, driving speed, and other useful
    and traffic-compliant information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No one excluded:** Self-driving cars can also be used by disabled people.
    In fact, they do not require particular physical skills. Just indicate the destination
    to your driverless driver and that’s it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computers now play a crucial role in our culture. These systems are today utilized
    for a variety of applications in a variety of fields, ranging from medicine to
    avionics, this also includes the comparatively recent emerging technology called
    artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the new fields in which these technologies are utilized, the idea
    of “computer systems” has to be redefined. Most of these systems must meet strict
    timelines in order to complete their jobs, otherwise catastrophic repercussions
    may occur, including widespread destruction, injuries, and even fatalities.
  prefs: []
  type: TYPE_NORMAL
- en: We refer to systems as *crucial systems* when they must adhere to strict time
    frames. When a system’s failure might be catastrophic, inflicting serious harm
    to the environment, infrastructure, or persons, we refer to it as a *safety critical
    system* .
  prefs: []
  type: TYPE_NORMAL
- en: 'In certain situations, these systems are used in a physical context. An automated
    automobile is an example of such a system: it consists of a computer system that
    performs the computations required to complete the given job and a physical component
    that interacts with the environment by changing both the environment’s and the
    platform’s state.'
  prefs: []
  type: TYPE_NORMAL
- en: Since these systems are so widespread and yet so hazardous if they fail, meeting
    and ensuring their (typically) ultra-high reliability criteria during the design
    and development process is critical. A system’s dependability is a gauge of how
    “trustworthy” it is, or its capacity to offer accurate service.
  prefs: []
  type: TYPE_NORMAL
- en: A breakdown is an occurrence that causes the given service to be disrupted.
    A critical system’s *dependability* is described as a collection of theoretical
    and practical indicators. Let’s look more closely at the most significant indicators.
  prefs: []
  type: TYPE_NORMAL
- en: 17.3 Dependability and Safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A critical system’s *dependability* is described as a collection of *quantitative*
    indicators. Here are a few of the more important ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Availability* is a metric that compares the frequency of right versus faulty
    service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Figa_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Figa_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Reliability* refers to a system’s capacity to deliver consistent service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Safety* is defined as the absence of catastrophic effects in the event of
    a failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because safety standards necessitate a quantifiable measure, the safety of a
    system is frequently described by integrating additional metrics like the Mean
    Time to the Next Catastrophic Failure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Figb_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Figb_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Maintainability* refers to the ability to maintain and restore a system after
    it has failed. This factor has an impact on availability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Coverage* is a metric for how effective the system’s fault-tolerance measures
    are at preventing, avoiding, or correcting problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A *risk* is something that threatens the system’s reliability: it’s an “event”
    that causes the system to offer erroneous service. Threats can take various forms
    and originate from a variety of places, such as incorrect specification or incorrect
    execution of a requirement, or disasters.'
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to system reliability, we want to make sure that the service is
    always right. A failure occurs when a service transitions from being accurate
    to being wrong. The requirement of minimizing probable transitions from a stage
    of suitable service to a stage of false service leads project deployment while
    building a critical system. (See Figure [17-3](#Fig3).)![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig3_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig3_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-3
  prefs: []
  type: TYPE_NORMAL
- en: Correct vs. incorrect services
  prefs: []
  type: TYPE_NORMAL
- en: We should differentiate between benign and catastrophic failures when assessing
    the reliability of safety-critical systems. With benign failures, the system may
    not provide the best service possible, but it will remain safe. In instances when
    these systems and people operate in close proximity, unsafe service can have disastrous
    effects such as environmental damage, disturbance of the system’s infrastructure,
    or even fatal accidents. Consider a self-driving automobile as an example. Consider
    a situation in which the automobile is traveling under “regular” conditions when
    an obstruction appears in front of it.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the fact that a screeching halt and subsequent ride interruption is
    a failed state, it is considered a benign failure because nobody was injured.
    Apart from this, a scenario in which the vehicle accelerates toward a barrier
    (and eventually collides with it) is deemed a dangerous failure since the occupants
    may be seriously injured. (See Figure [17-4](#Fig4).)![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig4_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig4_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-4
  prefs: []
  type: TYPE_NORMAL
- en: States
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to discover all probable failures to assess the reliability of safety-critical
    systems. The researchers used the faulty mistake failure chain, which is well-known
    among academics and policymakers alike, to accomplish their goal:'
  prefs: []
  type: TYPE_NORMAL
- en: A fault is an error with an adjudicated or speculated cause.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Error: A portion of the system’s condition might result in the failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Failure: The circumstance in which a mistake enters the service interface,
    causing the entire system’s service to be disrupted. (See Figure [17-5](#Fig5).)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig5_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig5_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-5
  prefs: []
  type: TYPE_NORMAL
- en: Fault, error, and failure
  prefs: []
  type: TYPE_NORMAL
- en: 'When a mistake has progressed to the point that it can no longer be corrected,
    a system’s dependability is determined by a collection of four approaches aimed
    at preventing or mitigating the impact of potential failures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fault Prevent the Occurrence:** Methods of preventing failures from occurring
    or being introduced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault Tolerance** **:** Methods for allowing defects to be tolerated. Even
    if a failure occurs, the system is still capable of providing proper service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault Rid** **:** Lowering the quantity or severity of system defects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault Predicting** **:** Estimating the current number, future occurrence,
    and potential effects of defects using statistical approaches. The validation
    procedure is used to assess the efficacy of the steps taken to meet a system’s
    dependability criteria.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'System requirements confirmation is a method that must be followed throughout
    the development phase, including at the start of the design process. For each
    stage of the system development process, there are a number of validation techniques
    to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numerical Modeling** **:** Methods for modeling system capabilities using
    numerical models with a simple analytical solution. In other words, a quantitative
    analytic function may be used to express changes in the system. These models include
    the Sequential model and the State-Based model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simulation:** In a simulated environment, there is an empirical estimate
    of system reliability. This approach allows you to test if a certain fault-tolerance
    mechanism operates without causing damage to the real system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurement:** Once a prototype of the system is ready, it may be monitored
    in action and the relevant metrics produced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s important to note that these techniques aren’t mutually exclusive, and
    that all of them should be considered during the validation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'As previously mentioned, validation must take place throughout the lifecycle
    of a project, beginning with the modeling phase and continuing after implementation.
    In certain periods, some of the approaches described in this book are more appropriate:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specification:** Validity is accomplished by the description of reliability
    criteria, which may be verified using numerical techniques. To identify the failure
    criteria for the system’s components, use approaches such as combinatorial designs.
    Failures are thought to be separate from each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Design:** It’s appropriate to use State-Based models to represent the system’s
    state space during the design phase. Markov Chains and Petri Nets are examples
    of these models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation:** When the project is far enough along, it may be possible
    to build a prototype model that can be closely monitored to see how effective
    fault-tolerance approaches are in improving system dependability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functioning:** It is possible to test the system in a real-world setting
    after it has been installed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 17.4 Tracking the System
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring the system is a technique that involves seeing a system operate in
    its surroundings and collecting data and evidence regarding its features. Currently,
    it’s seen to be a useful way to assess a system’s dependability, and many techniques
    to do so have been presented in the research. We use the approaches presented
    in these books in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method seeks to continuously monitor a system in its end environment,
    ensuring that the observed behavior and performance fulfill the specific needs.
    Validation of the data acquired during the monitoring activity is required:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Offline:** While the system is functioning, data is gathered and saved someplace,
    then evaluated afterward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Online** (or in real time): Data is evaluated as it is obtained. All of these
    factors must be considered while developing a monitoring strategy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the system’s important events, measures, and qualities that must
    be evaluated in order to determine the system’s dependability.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data labeling in order to add more information to raw measurements.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collection and transfer to the analysis node for processing.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data filtering and classification based on the metrics of interest The entire
    system is referred to as target system in the description of a monitoring process.
    When the tracking activity is linked to a specific H/w or S/w or piece of the
    system, an aimed component or final application is employed. The professionals
    and academicians are full in agreement that these diametrically opposed techniques
    are effective:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis in a black-box setting.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis in a white-box setting.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The strategy you choose is determined by how much command you have over the
    target system, particularly its inner implementations.
  prefs: []
  type: TYPE_NORMAL
- en: A black box approach can be used when the implementations are unknown, such
    as when the ability to monitor action is performed by a third-party system. After
    defining a workload, the task is offered to the system, and its outputs are noticed.
    (See Figure [17-6](#Fig6).)![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig6_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig6_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-6
  prefs: []
  type: TYPE_NORMAL
- en: Target system output processing
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to study the target machine “on the inside” by attaching a probe
    directly to the computer and viewing the system’s intermediary outputs while it
    runs, if the internal characteristics of the target system are known and easily
    available. Because these probes are directly connected to the system’s internal
    components, they can offer considerably more information than simply observing
    the system’s outputs can.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the one side, this technique gives a lot more information on the system’s
    behavior, but it also necessitates more caution in terms of monitoring and system
    probing. These two principles, in particularly, must be followed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Representativeness of choices:** In order to execute a successful monitoring
    activity, the probes should be able to get a sufficient number of relevant facts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No intrusiveness:** Probing must not alter the system’s behavior; otherwise,
    the acquired data will be useless.![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig7_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig7_HTML.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 17-7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Probes
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Additional rationale is required in the design and development of the monitoring
    system, particularly in regard to how the probing is carried out. We can differentiate
    between the following in specific.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware monitoring, software monitoring, and hybrid monitoring are all options.
    Because of its obvious minimal intrusiveness, a devoted hardware channel for tracking
    is the best approach to observe a system. However, as systems get more sophisticated,
    installing hardware probes becomes increasingly difficult, if not extremely difficult.
    (See Figure [17-7](#Fig7).)
  prefs: []
  type: TYPE_NORMAL
- en: Software probes are more powerful than hardware probes because they have access
    to more relevant data and can establish the context in which a specific output
    was created. Data extraction and measurement instructions can be included in the
    procedure, the operative system, or a new probe process can be created. In a hybrid
    technique, equipment/software probes might be utilized simultaneously, with specific
    emphasis devoted to reducing the disadvantages of each technology.
  prefs: []
  type: TYPE_NORMAL
- en: 17.5 Motivation and Goal of This Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *neural network* , which is in charge of driving the automobile, is pitted
    against a system supervisor, which offers fault-tolerance mechanisms to prevent
    catastrophic failures. In relation to the effects autonomous automobiles may have
    on future system advancements, they are among the most current and promising safety
    essential technologies.
  prefs: []
  type: TYPE_NORMAL
- en: This type of technology typically has extremely high-capacity needs that are
    difficult to verify. Furthermore, the nature of the software architecture, which
    involves artificial intelligence and non-artificial intelligence software interacting
    with one another, makes validation much more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of this research is to provide an experimental technique for evaluating
    the dependability of such complex systems, with an emphasis on what metrics are
    acceptable for these processes and what factors impact the analysis process. To
    exemplify the principles provided in this book, an experimental activity was undertaken
    in a realistic simulation setting.
  prefs: []
  type: TYPE_NORMAL
- en: You can’t get access to a system supervisor or a trained neural network. The
    network was developed from the ground up, and the project included the creation
    of a basic system supervisor.
  prefs: []
  type: TYPE_NORMAL
- en: It’s crucial to note that the goal of this work isn’t to provide a complete
    treatment of the argument; rather, it’s to begin a period of exploration of these
    notions and difficulties, which will need additional validation and research in
    future designs.
  prefs: []
  type: TYPE_NORMAL
- en: 17.6 Automotive Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the popular trends this decade is self-driving cars. AI that have been
    actually trained to drive using machine learning techniques have shown that a
    computer can drive a car. However, if these systems fail, individuals might be
    hurt or killed. Simultaneously, certifying the ultra-high dependability specifications
    demanded is proving difficult. Today’s modern problems with self-driving vehicle
    safety are reviewed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 17.6.1 Autonomous Cars as Cyber-Physical Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A car’s ability to drive itself necessitates the use of appropriate technology
    and software. As a result, autonomous vehicles are classified as *cyber-physical
    systems* (CPSs), with the potentially devastating effects of a failure in/of these
    systems placing them in the important systems category.
  prefs: []
  type: TYPE_NORMAL
- en: 'The system gathers data from various sensors to perceive and map the local
    environment. Here are some of the most significant sensors and their functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Highly accurate GPS sensors:** These sensors are useful for monitoring change
    in the status of the automobile and objects in the surroundings over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cameras:** Face recognition software and cameras are generally used to process
    recorded images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lidars and Radars:** Lidars are the next step in the evolution of traditional
    radars. The information gathered from these sensors is used to map the surroundings
    and detect obstacles and objects in the vicinity of the vehicle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These sensors’ results are integrated and sent into the car’s control system.
    Figure [17-8](#Fig8) depicts a simplified version of the software architecture.![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig8_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig8_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-8
  prefs: []
  type: TYPE_NORMAL
- en: The software architecture of the system is abstracted at a high level
  prefs: []
  type: TYPE_NORMAL
- en: Sensor data are inputs to the management system, which are divided into two
    parts. One input collects data straight from the sensors, processes it to create
    an occupancy grid1 to map the nearby region, and generates a physical model of
    the system in terms of following the appropriate path to a final stage without
    collapsing. The other input collects data from the sensors, processes it to create
    an occupancy grid1 to map the nearby area, and generates a physical model of the
    system to get the correct path to a final stage without collision.
  prefs: []
  type: TYPE_NORMAL
- en: A car’s activities include accelerating, braking, and steering. Because of the
    importance of their job, a system supervisor is required. This system is in charge
    of detecting potential hardware faults or erroneous process control outputs for
    and, if required, executing a right action.
  prefs: []
  type: TYPE_NORMAL
- en: The system administrator is the key part of such systems and prevents breakdown.
    Without a doubt, certain checks may be performed when data is analyzed, but the
    final decision rests with the system’s monitor, and underestimating its importance
    might have disastrous results, such as the 2018 Arizona tragedy in which a lady
    was killed by a self-driving car during a practice run.¹
  prefs: []
  type: TYPE_NORMAL
- en: Additional examination found that the vehicle’s radar and Lidar sensors detected
    the victim around six seconds before impact, and that it took four seconds to
    infer that there was an impediment on the road requiring an emergency stop. During
    testing for “smoother rides,” however, this safety-checker was deactivated, resulting
    in tragedy.^([4](#Fn4))
  prefs: []
  type: TYPE_NORMAL
- en: The high complexity of these systems raises issues among specialists, including
    the need to develop a new perspective for studying and testing their safety, as
    well as the need to raise awareness about safety.
  prefs: []
  type: TYPE_NORMAL
- en: 17.7 Safety and Self-Controlled Vehicles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The amount of automation may be split into six categories, ranging from 0 to
    5, by an SAE International proposal to categorize self-driving cars’ autonomy.
    Level 0 implies no autonomy: the car is operated only by a person; level 5 means
    that no human involvement is necessary, and the vehicle must be capable of not
    just driving safely on the road, but also avoiding catastrophic failures that
    may seriously injure people. The greater the reliability criteria for an automobile
    to be used on open streets, the more autonomous it is.⁴'
  prefs: []
  type: TYPE_NORMAL
- en: 'Demonstrating a device’s dependability is a difficult undertaking in and of
    itself, but it becomes considerably more difficult with ultra-high dependability
    systems like these. In addition to the problem at hand, showing the dependability
    of autonomous vehicles has two additional challenges: how do you test the system
    efficiently and safely, and the presence of neural networks, and as such it is
    difficult to explain why it produced the output y given the input x.¹'
  prefs: []
  type: TYPE_NORMAL
- en: Several studies have shown that road testing vehicles is impossible. One of
    these, dubbed the RAND study, considers how many miles of driving it would take
    to demonstrate autonomous vehicles’ accuracy using traditional statistical inference,
    estimating that if autonomous vehicles’ fatality rate was 20 percentage points
    lower than humans’, it would take more than five centuries with “a fleet of 100
    driverless cars being test-driven 24/7 a day, 24/7/365 a year.”¹
  prefs: []
  type: TYPE_NORMAL
- en: The confirmation of ultra-high dependability criteria for safety-critical systems
    is a well-known topic in the safety literature, and autonomous cars haven’t been
    added to it. In actuality, the RAND research is just one example of the problem
    described in Littlewood & Strigini’s paper from 1993, in which the same ideas
    are examined and extended for any ultra-high dependability system.¹
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental flaw in the RAND study approach is that the frequency of future
    failures cannot be anticipated only based on the observed one. Not only because
    of the quantitative findings of its impossibility, but also because this method
    is ineffective: a failure rate that was observed. This technique cannot function,
    not only because of the quantitative approaches of its impossibility, but also
    because an observed frequency failure of zero would lead to optimistic (and potentially
    dangerous) forecasts. Fortunately, as Zhao et al. demonstrate, this dilemma is
    solvable.¹'
  prefs: []
  type: TYPE_NORMAL
- en: Validating the reliability criteria of an autonomous vehicle appears to be a
    difficult challenge in and of itself. The fact that these automobiles are controlled
    by neural networks makes things much more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: The area of machine learning has seen a rise in attention in recent years, resulting
    in important scientific advancements. As a result of these advancements, autonomous
    automobiles appear to be a reality, as AIs have achieved astounding results with
    their abilities, and large businesses such as Amazon and Alibaba are investing
    even more in artificial intelligence related research. The way people interact
    with computers is changing dramatically thanks to this new wave of AI research,
    and neural networks have produced surprising results.
  prefs: []
  type: TYPE_NORMAL
- en: While neural networks have shown promise and appear to be the only method to
    reach objectives like automated vehicles, it has been proven numerous times how
    weird a network’s forecast may become when the inputs are skewed and how wide
    the confidence level can be when the inputs are skewed. The absence of established
    standards and certifications for this sort of software, as well as the need to
    comprehend neural networks fully, has prompted worries about how reliant these
    systems may be. With advanced AI businesses lobbying for more limitations, there
    is now a growing awareness of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: '17.8 Controller: The Problem with the Checker'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The connections between the control sys and the sys admin are at the heart of
    the vehicle’s motions. The control sys, often known as the *primary factor* ,
    is the software that conducts the system’s major computations, which are required
    for the vehicle to run. To avoid catastrophic failures in this situation, fault-tolerance
    measures such as the system administrator are required. Due to high-reliability
    demands of these systems, this type of architecture is required in order to attempt
    to cover all conceivable failures. (See Figure [17-9](#Fig9).)![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig9_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig9_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-9
  prefs: []
  type: TYPE_NORMAL
- en: The system’s secure states are depicted in this diagram
  prefs: []
  type: TYPE_NORMAL
- en: A safe condition is one in which the control sys generates an output that doesn’t
    cause the vehicle to crash. The system’s degree of safety may be represented as
    the union of the controller’s and supervisor’s failure regions, with an overlap
    region where the supervisor is truly damaging to the system’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a self-driving automobile on the road when an unexpected stumbling
    barrier arises. If the primary identifies the barriers properly, it should take
    a self-protective action to prevent going into alert mode.
  prefs: []
  type: TYPE_NORMAL
- en: If the controller fails to detect the obstacle or detects it but continues to
    throttle, the controller is judged to have failed, and the fault features kick
    in, the controller switches from a safe to an alert state. It is now the sys administrator’s
    obligation to take remedial action to put the system into fail-safe mode. As a
    result of the failure of both elements, an administrator mistake will definitely
    force the system to fail, resulting in a failure state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *clustering algorithm* *,* a unique table arrangement that allows visualization
    of a classification system’s performance, may be used to show the system supervisor’s
    probable actions. This is achieved by dividing the world into two categories:
    positive and negative.'
  prefs: []
  type: TYPE_NORMAL
- en: Assume we’re trying to tell the difference between white and black sheep in
    a flock. The Positive Class will be the black sheep’s class. A black sheep that
    has been identified as such is known as a True +Ve. White sheep make up the -Ve
    class, and each white sheep is a True -Ve. When a white or black sheep is misclassified,
    it is called a False Positive or False Negative¹. Actual and anticipated values
    are the two dimensions of this form of contingency table, as shown in Figure [17-10](#Fig10).![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig10_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig10_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-10
  prefs: []
  type: TYPE_NORMAL
- en: Matrix of perplexity table
  prefs: []
  type: TYPE_NORMAL
- en: We need to specify what constitutes a good and bad classification in the realm
    of self-driving automobiles. Remember that a controller “failure” was defined
    as a transition from a safe position, in which the supervisor can manage the ecosystem
    without collapsing, to an alert state, in which the system would eventually collapse
    without the administrator.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the +ve class represents the set of events that will ultimately
    cause the controller to fail, whereas the negative class represents the set of
    events that the controller successfully controls. We’ll put the administrator’s
    choice to distinguish between safe and alert statuses to the test. To accomplish
    this, we used the following definitions for positive and negative forecasting:![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig11_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig11_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-11
  prefs: []
  type: TYPE_NORMAL
- en: True and erroneous predictions are represented graphically in the sys’s state
    space. The system’s present status is represented by dots. A blue dot indicates
    that no alert has been raised, whereas a red dot indicates that an alarm has been
    triggered by the monitor
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive (TP):** Due to a controller failure, the system went into alert
    mode, which was correctly recognized and stopped by the controller.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative (TN):** The sys is in good working order, and the controller
    does not sound an alert.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive (FB):** Although the sys is safe, the monitoring signals an
    alarm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative (FN):** The sys is on high alert, and the controller is unable
    to identify the threat.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These raw numbers are just a starting point for more sophisticated and valuable
    measurements, which are generally given as rates and linked together using statistical
    laws, making them statistically comparable and straightforward to compute provided
    the amounts of right and erroneous predictions are known. These are some instances
    of these metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitivity (True Positive Rate):** Calculates the percentage of true +Ves
    that are accurately recognized as such.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Figc_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Figc_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Specificity (True Negative Rate):** A metric that evaluates the percentage
    of true negatives that are accurately classified as being such.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Figd_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Figd_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***Miss Rate*** **(also known as the False Negative Rate):** The percentage
    of real positives that are projected as negative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Fige_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fige_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***Fall-Out:*** The fraction of real negatives projected as positives is measured
    by the fall-out (false positive rate).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Figf_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Figf_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These four rates can also be coupled to provide a variety of other metrics that
    indicate the model’s forecast performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few of the much more commonly utilized metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Accuracy* is a metric for quantifying systematic mistakes in forecasts. Disparities
    between such a prediction and its “actual” value are caused by inadequate precision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Figg_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Figg_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Precision* is a measure of statistical variation in the model used for forecasts
    that is used to describe random mistakes in forecasts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Figh_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Figh_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Fβ-score* is a combined accuracy and sensitivities measure of a test’s reliability.
    This runs from 0 (worst value) to 1 (best value), and it is directly related to
    the true positive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Figi_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Figi_HTML.jpg)Matthew’s
    Correlation Coefficient is an indicator of overall forecast quality that considers
    every cell in the confusion matrix. This measure runs from 1 (worst value) to
    1 (highest value), allowing for a more comprehensive assessment of model correctness.![../images/520777_1_En_17_Chapter/520777_1_En_17_Figj_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Figj_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *ROC5 Curve Plot* is a graphical representation of a binary classifier’s
    effectiveness, with the x-axis indicating false positive rate and the y-axis indicating
    true positive rate, with the plot divided by the y = x line.
  prefs: []
  type: TYPE_NORMAL
- en: Values above this line indicate “excellent” forecasts, while values below this
    line indicate “worse-than-random” forecasts, and values on the line indicate arbitrary
    guesses.
  prefs: []
  type: TYPE_NORMAL
- en: All of these indicators may be computed and connected to one another if the
    real number of positive and negative expectations is known, making switching points
    of view of the analysis simple. We want to show that by utilizing this technique,
    we can estimate all of the quantities needed to compute complicated extra metrics
    like Threat Score and False Discovery Rate, which are reliant on the assessment’s
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: This is just a safety-related extension of the asymmetric fault-tolerant architecture
    for computer systems, with a primary component doing the major calculations and
    a primary checker detecting (and correcting) any primary defects. The challenge
    of determining the reliability of these simpler systems is well-studied in the
    publications.
  prefs: []
  type: TYPE_NORMAL
- en: The chance of a system failing on a given input (or collection of inputs) is
    demonstrated to be tightly dependent on both the coverage of the primary checker
    and the coverage of the secondary checker in a paper released by Popov and Strigini
    in 2010.
  prefs: []
  type: TYPE_NORMAL
- en: We want the primary to cover as much ground as possible in the case of self-driving
    cars. This is achieved by putting the neural networks that will drive the car
    through extensive training. The control sys must be able to manage the bulk of
    potentially hazardous occurrences if the network is “fully trained.” It’s feasible
    that, at some time, the controller learns to manage the system supervisor’s “alert
    states,” diminishing the supervisor’s total contribution to the software’s safety.
    (See Figure [17-12](#Fig12).)![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig12_HTML.png](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig12_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-12
  prefs: []
  type: TYPE_NORMAL
- en: The controller now captures all of the covered states in the past, as well as
    several that were previously only covered by the monitor
  prefs: []
  type: TYPE_NORMAL
- en: Another scenario is that a part of the failure region covered by the controller
    becomes exposed during the training. This might lead to a scenario where certain
    formerly safe conditions are no longer safe.
  prefs: []
  type: TYPE_NORMAL
- en: Because the system supervisor’s coverage area cannot vary without modifying
    its implementation, a transition to one of these states will invariably end in
    a failure. (See Figure [17-13](#Fig13).)![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig13_HTML.png](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig13_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-13
  prefs: []
  type: TYPE_NORMAL
- en: Although some of the states covered previously in the training are no longer
    covered, the controller now captures all the states covered in the past by the
    monitor
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to self-driving cars, we want the main to cover as much ground
    as possible. This is accomplished by rigorously training the neural networks that
    will control the vehicle. The management system should be able to handle the bulk
    of potentially harmful occurrences as long as the network is “fully educated”.
  prefs: []
  type: TYPE_NORMAL
- en: The creation and execution of an experimental approach to examine these elements
    is presented and discussed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 17.9 System Analysis Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 17.9.1 Preliminary Rounds and Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The objective  of this research is to provide a preliminary testing approach
    for monitoring emergence-related traits that arise in a controlled environment
    when a control system interacts with a system administrator. An antiviral program’s
    software design was reduced to just two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A controller:** A neural network that has been taught to control the automobile
    using reinforcement learning methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A** **safety supervisor** **:** A system supervisor submodule that uses data
    from a Lidar sensor to determine if the vehicle is approaching an object too rapidly
    and applies an emergency brake if necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of this study is to take a fresh look at the problem and assess its
    feasibility in a controlled, simulated setting. Because the system is made up
    of two constituent systems—controlling and monitoring—we believe that a point
    of view based on emerging behavior emerging from the interplay of the systems
    might increase the standard of the evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: This section explains and explores a strategy for investigating the safety level
    of an autonomous vehicle over time, which includes observing the emergent behavior
    of a neural network operator and evaluating it safely in a virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: The suggested framework is focused on investigating the emergence that occurs
    as a result of the interplay of these two constituent networks.
  prefs: []
  type: TYPE_NORMAL
- en: We are focused on how a monitor’s efficacy changes as the neural network learns
    and the effects of training techniques on a safety monitor’s efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to enhance neural networks by training them on datasets is one of
    their most appealing elements. One step of training involves gathering data across
    n stages and revising the weights of the prediction function.
  prefs: []
  type: TYPE_NORMAL
- en: The weights of the function indicate the network’s status at epoch after the
    training phases. A neural net should produce satisfactory results after “enough”
    epochs. As the number of epochs required grows, the task gets more difficult.
    Driving a car is a difficult task, and saving the weights of every period is impossible.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we define a breakpoint for a neural network N as a generic epoch
    of N. Assume N has completed thousand epochs of training. If we save the weights
    of the usually suppose every 100 epochs, we’ll have ten checkpoints:¹
  prefs: []
  type: TYPE_NORMAL
- en: 'Checkpoint1 < checkpoint2 < : : : < checkpoint10'
  prefs: []
  type: TYPE_NORMAL
- en: where checkpoint1 represents the channel’s weights at epoch 100, checkpoint2
    represents the network’s weights at epoch 200, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at a self-driving car that is undergoing testing on the road. Its
    objective is to stay in the car as long as possible without crashing. As the automobile
    progresses on its journey, the world around it will alter. It is possible that
    the chance of a subsequent accident increases dramatically in specific system
    states, such as when a person unexpectedly crosses the road. If and only if the
    controller’s action results in the pedestrian being struck, will we consider it
    a failure. If the pedestrian is genuinely identified and the automobile strikes
    something else while attempting to avoid it, the same logic applies.
  prefs: []
  type: TYPE_NORMAL
- en: Any action taken by the controller that might cause a crash is deemed a failure.
    When a potentially damaging event occurs, such as when the chances of seeing a
    crash are higher than normal, the controllers is destined to fail if and only
    if its attempts to prevent the coming failure are unsuccessful. In this sense,
    we don’t distinguish between climatic changes that enhance the chance of an accident
    (e.g., a passenger strolling down the street) and the controller’s harmful actions
    at this stage of the project.
  prefs: []
  type: TYPE_NORMAL
- en: It is the sensor’s job to run security to prevent a system failure if the controllers
    fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the controller fails, the monitoring must not only identify whether it succeeded
    or not, but also run a security routine to keep the entire system from failing.
    We believe the monitor’s actions to always be safe in this initial step of analysis.
    That is to say:'
  prefs: []
  type: TYPE_NORMAL
- en: The network will be in a safe condition if the controller completes all of the
    stages in the safety routine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the following might cause the safety monitoring to fail:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The obstruction has not been found.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The barrier is identified, but the routine’s execution is not completed. If
    both the administrator and the safety monitor crash, resulting in a failure, we
    evaluate the system to have failed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We may separate the system states into three groups based on this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Safe States:** States in which the controller does not require the monitor’s
    assistance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alert States:** Situations that necessitate the monitor’s intervention. The
    safe state space would be reentered if the impending mishap was correctly detected
    (and prevented).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failure States:** Areas where an accident has occurred. It’s vital to keep
    in mind that the monitor can’t identify every circumstance. There are some incidents
    that cannot be avoided and for which no monitoring can save the system, resulting
    in a straight shift from a safe to a failure state. (See Figure [17-14](#Fig14).)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig14_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig14_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-14
  prefs: []
  type: TYPE_NORMAL
- en: The phase space of the system is shown
  prefs: []
  type: TYPE_NORMAL
- en: What’s important is discovering the likelihood of a system failure and figuring
    out how to reduce it. Simultaneously, we want to see how the efficacy of the safety
    monitor evolves over time as the controller learns.
  prefs: []
  type: TYPE_NORMAL
- en: This is beneficial not just for ensuring that the network improves during the
    learning, but also for the controller to get a better knowledge of the monitor’s
    use as they gain experience. Because several checkpoints of the very same system
    are evaluated under the same exact circumstances to discover how the operator’s
    behavior changes over time, this is necessary for the experimental activity.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple checkpoints are required not just to ensure that the connection is
    developing, but also to track how the efficacy of the monitor changes over time.
    Furthermore, as you’ll see in the following section, if multiple checkpoints from
    the same system are tested in the same situations, it can obtain relevant metrics
    and compare the behavior of two checkpoints in the same circumstance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several scenarios must be established before the analysis can begin. A scenario
    is a collection of beginning conditions under which the automobile is to be tested
    (for example, the car’s spawn position, seeds used in random number generators:
    : :). The difficulty level for the given example is represented by the pedix h.
    The rationale for this is that we want to test the car in the same beginning settings
    as previously, with the exception of one aspect, so that we may learn more about
    what causes the system to fail more frequently. The h variations should become
    more complicated as time goes on while remaining realistic. Scenario S might be
    modified by increasing the number of cars in the scenario or simulating climate
    change.'
  prefs: []
  type: TYPE_NORMAL
- en: A tougher version of scenario1 should arise from combining these two versions.
    It’s crucial to remember that these scenarios should not be modified after they’ve
    been established because they’ll be utilized to test all of the checkpoints. The
    information gathered here will aid in the process of determining what makes a
    scenario “harder” for some systems than others.
  prefs: []
  type: TYPE_NORMAL
- en: 17.10 The Experimental Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The method for this exploratory study is broken into three stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 1:** Given c checkpoints of a neural network and nh circumstances,
    the operator is assessed in all situations and its runs are logged.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stage 2:** The security monitor is verified once it has been connected to
    the system by repeating the tests that were performed in Stage 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stage 3:** The network is retrained using various techniques to enhance its
    efficiency from the previous checkpoint. After that, the updated controllers and
    the safety monitor are evaluated in all of the specified situations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’re interested in determining the neural network and monitor’s quality in
    the first phase. This is accomplished in two phases. The controller’s m checkpoints
    are tested in all circumstances in the first stage. We’re primarily interested
    in seeing how the controller’s reliability varies in relation to these checkpoints
    in this phase.
  prefs: []
  type: TYPE_NORMAL
- en: The issue of repeatability is one of the most difficult aspects of evaluating
    a neural network. It’s quite improbable that the same neural network would act
    in the same way in numerous runs if the beginning circumstances were the same.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this network characteristic, it’s feasible that the mode of failure
    indicated in one of the scenario runs may never happen again, or that the time
    necessary to do so would be unreasonably long. Because it is hard to foresee all
    conceivable failure scenarios, we believe that this type of situation approach
    may help in resolving this issue by creating more challenging operating conditions
    in which the variables that cause a crash may be examined.
  prefs: []
  type: TYPE_NORMAL
- en: To address the issue of repeatability, a black box was built for each test scenario
    that was run. This method keeps track of the operator’s actions so that the particular
    run may be looked at further to discover whatever went wrong with the controller.
    These data may then be utilized to find out which dangerous circumstances the
    controller safeguards at checkpoint j, and whether or not these scenarios are
    still protected when the network is reviewed at checkpoints --:j + x.
  prefs: []
  type: TYPE_NORMAL
- en: 17.10.1 Controller Test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The controllers are assessed in solitude in each situation, at each difficulty
    level, until a crash occurs. It’s still conceivable that there won’t be any failures
    recorded. Acceptable acting criteria are outside the scope of this study, although
    they remain a source of debate in academia; nevertheless, as mentioned in the
    previous section, this issue may be resolved.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the difficulties that occurred during the method development phase prompted
    the decision to isolate the controller in order to test it. The primary challenges
    are consistency and non-intrusiveness. As previously mentioned, the neural network’s
    repeatability problem is handled by creating a black box that saves information
    about the car’s condition in every frame. Because a safety-brake enforced by the
    monitor would almost certainly modify the ambient circumstances for the duration
    of the simulator, we wouldn’t be able to calculate controller performance metrics,
    we can’t test the entire system (controller and monitor) at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the controller in isolation aids in the resolution of these difficulties
    and serves as a warmup for the second step. In all cases when the controller has
    been tested for each complexity, the following must be computed at a minimum:![../images/520777_1_En_17_Chapter/520777_1_En_17_Figk_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Figk_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important metrics for assessing a system’s dependability function
    R(t) is the Median Time to Failure, which is simple to compute under simulated
    conditions and is used to determine the rate of the exponential function. Statistics
    in the automobile sector, on the other hand, are frequently expressed in terms
    of traveled distance, such as average distance to failure, collision incidence
    per kilometer, and so on. If the parameters and simulation hardware are powerful
    enough to conduct the computations at a defined time-step, it’s very simple to
    flip the point of view on the data using this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'We expect the following inequation to continue as long as the neural network
    is fully trained: Ri(t) 6 Rj(t), where I and j are two checkpoints, and I j. This
    may be easily confirmed using the data collection method described previously.![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig15_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig15_HTML.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-15
  prefs: []
  type: TYPE_NORMAL
- en: The reliable function determines the likelihood that the system will continue
    to work at time t. We anticipate that more experienced drivers will be able to
    complete longer runs than less trained networks
  prefs: []
  type: TYPE_NORMAL
- en: 'Other information should be recorded as well, if the simulator used for testing
    permits it, in order to improve knowledge of the controller’s behavior, such as
    :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The car’s instantaneous velocity and acceleration vector.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What the automobile collided with (another car, a walker).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Set one or more destination goals and track whether the vehicle meets them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a crash happened at time t, determine environmental conditions at time t-x.
    These figures are crucial for distinguishing between “safe” and “catastrophic”
    failures. If a fence is struck at less than 10 km/h, for example, it may be deemed
    a less harmful accident than striking a person at the same speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because a trained driver causes fewer accidents than a newbie, we anticipate
    the reliable function to increase as the controller learns, boosting the MTBF.
  prefs: []
  type: TYPE_NORMAL
- en: If the black box utilizes better data, it’s also possible to track changes in
    the car’s behavior in connection to the situation and difficulty level. If we
    double the number of automobiles in the situation, for example, the number of
    crashes with other barriers will almost certainly increase. If this is the case,
    the simulation should be looked into more in order to focus the training strategy
    in a specific direction, since this might result in the controllers colliding
    with walls while attempting to avoid other automobiles.
  prefs: []
  type: TYPE_NORMAL
- en: 17.10.2 Monitor or Observation Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the operator’s runs have been documented, the monitor testing  may commence.
    Not only are we evaluating how good the security monitor is at preventing crashes
    during this phase, but we’re also searching for evidence of which situations are
    “difficult” for the monitoring and which are “easy” for the controller.
  prefs: []
  type: TYPE_NORMAL
- en: Because the controller now encompasses all of the failures previously disclosed
    by the screen, and the risks posed by its novel behavior are too great for the
    watch to detect, its behavior may evolve to the point where the monitor is no
    longer capable of detecting impending failures as the internet backbone matures.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the opposite is a viable option. In the early epochs, the
    controller, for example, would drive “crazy,” making a lot of abrupt, high angle
    steering and ride at high speeds. Because the controller’s behavior is unexpected,
    the monitor will have a harder difficulty anticipating the future state. The network
    will move more smoothly as it learns, making it simpler for the monitor to see
    potential accidents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main issue is that the monitor’s effectiveness may diminish as the network
    learns, resulting in a completely pointless component that may even be harmful
    to the system’s performance: the operator may grow sufficient to cover all of
    the damage covered by the screen at a previous checkpoint, resulting in a useless
    component. The system’s overall safety may not be jeopardized if we believe the
    monitor’s activities to be completely safe, but the safety monitor’s security
    will result in fewer smooth trips.'
  prefs: []
  type: TYPE_NORMAL
- en: The monitor is put to the test by repeating the runs that were produced during
    the controllers testing step. The beginning conditions must also be the same,
    and they must be kept in the black box from the previous stage. The controller’s
    prior runs are now repeated, with the monitors linked to the systems and the warnings
    raised throughout the run recorded, as well as whether it was able to avoid the
    previous crash.
  prefs: []
  type: TYPE_NORMAL
- en: The main issue at this point in the investigation was non-intrusiveness. We
    cannot consider rerunning the simulations just by adding the safety monitor and
    watching how things proceed for the reasons stated above. If an alarm is triggered,
    the safety monitor overwrites the controller’s action, altering the next section
    of the run. This isn’t an issue in theory if you can tell the difference between
    false and real positives. However, without developing software sensors monitor
    and mapping the environment, we won’t be able to predict what a technical error
    will be.
  prefs: []
  type: TYPE_NORMAL
- en: Because this is a safety monitor, there will be false +ves, even if they are
    extremely low, thus this method will not solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to maintain track of the warnings that occur during the operation
    while also avoiding the safety procedure from being initiated. Because we’ll be
    repeating the runs from Phase 1, we’ll be able to pinpoint the exact time t when
    the switch to the alert state happened before the crash. Because we know t, all
    prior alerts to t are false +ves or false reports, based on this information.
    Coverage can be approximated by allowing the Monitors to start the safety procedure
    after this point in time.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned  in the previous chapter, we use the confusion matrix to show the
    predicted values of a model in comparison to real values to see how good it is
    at classifying tasks. However, measuring all of them for real-time critical systems
    is difficult, if not impossible. It’s especially difficult to comprehend when
    the operator averted a collision, but the monitor did not sound an alarm.
  prefs: []
  type: TYPE_NORMAL
- en: This makes calculating the number of real negatives exceedingly difficult since,
    in most cases, such instances can only be recognized when an operator reviews
    each individual run, presenting a degree of arbitrary nature into metrics like
    true negatives.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a safety monitor input is made up of a series of timed input that
    may be thought of as the development of the environment while the system is running.
    As a result, it’s impossible to predict whether another state will be alerted
    or secure since we don’t know when the collection of inputs that would ultimately
    lead to a crash will begin.
  prefs: []
  type: TYPE_NORMAL
- en: Because the monitor requires a certain number of true negatives, the metrics
    and rates that may be calculated are limited. Simultaneously, the security monitor
    classifies the condition of the system in each frame, collecting Lidar data, assessing
    it, and deciding whether or not a safety brake is required. Due to the system’s
    real-time nature, we may regard any frame in which the monitors does not raise
    an alarm as a true negative. We can compute all of the widely used metrics in
    statistical classification models using this method.
  prefs: []
  type: TYPE_NORMAL
- en: We must combine genuine and false pluses and minuses, such as precision and
    accuracy, to achieve large benefits since the vast quantity of real negatives
    is likely to be far greater than the other metrics for the reasons previously
    stated. Because the false positive rate is stated as FP/FP+TN, numbers with numerous
    zeroes before the first significant digit result. Furthermore, in order to give
    an intelligible rate measure for false positives, the quantity of false positives
    was computed over the distance traveled in a session  .
  prefs: []
  type: TYPE_NORMAL
- en: 'The rates of correct and wrong operator predictions were determined as previously
    mentioned and may be used to compare checkpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: Miss rate and awareness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specificity and consequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correctness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matthew’s Factor of Connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percentage of false positives per meter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision recall curves reduced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ISO standard was used to choose accuracy and reliability. The notion of accuracy
    has been superseded by the concept of prediction trueness, with accuracy being
    explained as a representation of a mix of random mistakes needing large precision
    and accuracy. The most helpful metric for assessing a confusion matrix is Matthew’s
    Correlation Coefficient, which is calculated as the total of all forecasts and
    has been found to offer more accurate overall data than the F-score.
  prefs: []
  type: TYPE_NORMAL
- en: We chose not to use Fi since it does not function well with imbalanced datasets
    in general, resulting in conclusions that are either overly optimistic, depending
    on the size. We opted to utilize MCC as a measurement of the monitor’s performance
    because of the large number of True Negatives and our interest in the monitor’s
    overall utility.
  prefs: []
  type: TYPE_NORMAL
- en: The False +Ves per Meter is used to predict how the system would behave if the
    controllers and the security were both turned on and operating together, as well
    as to quantify the impact of the latter’s faulty safety-brakes. The ROC2 curve,
    a graphical representation of a classifier tool’s diagnostic abilities, might
    have offered a fast visual representation of the sensor’s efficacy, however this
    method was not possible. The x-axis is determined by PR, whereas the y-axis is
    determined by TPR.
  prefs: []
  type: TYPE_NORMAL
- en: The figure would be flattened on the left side due to the significant imbalance
    of our dataset, where the number of false is likely to be lower than 10-2, based
    on how genuine negatives were classified. As a result, based on data obtained
    for each checkpoint in each difficulty, we used highly precise cropped to produce
    a graphical mean, a curve generated by defining the x-axis by recall and the y-axis
    by accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: We couldn’t construct a test-set to evaluate the monitor while altering the
    cutoff for choosing whether to brake and presenting the whole curve since the
    monitor only defines states as safe or dangerous in real time depending on data
    provided by the Lidar sensor. As a consequence, past data for each degree of complexity
    was computed, and these values were displayed to explore the link between difficulty
    and monitor efficacy, cropped to the region containing the anticipated value.
  prefs: []
  type: TYPE_NORMAL
- en: When testing the monitor, it’s a good idea to put it through its paces in order
    to gather additional data and develop links between the data. This element is
    unconnected to the level of difficulty mentioned above, since it is anything linked
    to the environment that may provide proof as to when the monitor is operating
    well and when it is not, and it is more involved with fault injection at a higher
    level.
  prefs: []
  type: TYPE_NORMAL
- en: The ways by which a security monitors’ settings may be modified are mostly defined
    by how it’s implemented, and the development team is in charge of selecting what
    problems to include and how many to include. Reduce the amount of data read by
    the sensors or add noise into the observations are two easy options. This stage
    will teach you how to change the software’s internal settings to get the “ideal”
    version, which will be utilized throughout the controllers retraining process  .
  prefs: []
  type: TYPE_NORMAL
- en: 17.10.3 Controller Retraining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point  , we’ve gathered information on the monitor’s and controller’s
    actions. Whenever the neural network is saved from the latest checkpoint, we will
    investigate how much these values differ depending on the learning technique used.
  prefs: []
  type: TYPE_NORMAL
- en: We’re looking at a controller constructed up of a neural network that has been
    trained using reinforcement learning techniques, as described at the start of
    this chapter. In these methods, a reward function is utilized to inform the network
    whether it is functioning GOOD or BAD, and it is calculated for each prediction
    step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The essential parameters are required by the training function and are reevaluated
    at each algorithm’s iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: Response of the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current state of the system where the action was performed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A prize given for completing a task in a specific manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this phase, we’ll look at how different neural network training techniques
    impact the overall behavior of the system, with a focus on the security monitor’s
    efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: To begin our investigation into this issue, we identified four techniques as
    well as the expected consequences when the training is done. These results are
    “predicted” in the sense that we don’t know how the network will react to a modification
    in the training method or if the observed behavior will be the same as the one
    predicted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategies created using this technique are mostly dependent on the reward
    function and the channel’s actions:'
  prefs: []
  type: TYPE_NORMAL
- en: S1) When the automobile strikes anything, the reward function is more punitive,
    but braking is somewhat more rewarded, as long as the car does not stop going.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S2) The security monitor is connected to the system, and if an alarm is raised,
    the monitor’s response takes precedence over the network’s (the safety brake).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3) If the monitor raises an alarm, the network’s activity is substituted by
    the monitor’s. For acting like the monitor, the network receives a good reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S4) If an alarm is triggered, the training phase is terminated and the network
    is given a poor reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We estimate the controller’s median time/distance among failures to be reduced
    if we use S1\. Giving a higher negative incentive for crashes and a lower positive
    reward for braking (if the car does not stop driving) should encourage the car
    to choose braking over swerving (which might result in a new dangerous scenario),
    thereby increasing the time between failures and the distance traveled.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of S2 is to “train” the network to brake if the monitor raises an alert.
    As a result, the warring states formerly covered by the Monitor might become safe
    states.
  prefs: []
  type: TYPE_NORMAL
- en: S3 resembles S2 in many ways. Providing a positive incentive for behaving like
    the monitors is expected to speed up learning.
  prefs: []
  type: TYPE_NORMAL
- en: S4 is by far the most promising method, and it has the potential to provide
    the most intriguing outcomes. When the security monitor raises an alert, regardless
    of whether it’s a true or false positive, a negative reward and a change in the
    training step should compel the network to totally avoid instances where the monitor
    intervenes. Essentially, we expect the safety component’s efficacy to be significantly
    decreased.
  prefs: []
  type: TYPE_NORMAL
- en: The new devices are tested as in Phase 1 when the four controllers have been
    sufficiently educated. For the controllers and monitor, the same measurements
    are estimated, and the results are compared to the actual checkpoints  .
  prefs: []
  type: TYPE_NORMAL
- en: 17.11 Method Implementation and Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tools utilized, the software architecture and technique implementation,
    as well as the data gathered throughout the analysis, are reviewed in this chapter.
    A DDPG Agent1 was taught how to drive in a city setting. During the training,
    checkpoints of the network’s status were recorded for comparison. To provide a
    unique view on AV behavior, these network checkpoints were tested with and without
    a simple safety monitor.
  prefs: []
  type: TYPE_NORMAL
- en: 17.12 The Tools and Software
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 17.12.1 CARLA Simulator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CARLA^([5](#Fn5)), an open-source simulator created by University of Barcelona
    researchers, was used to construct a realistic environment with accurate physics
    simulation and data sensors. The objective of this simulator was to provide an
    environment in which AI agents could be trained to drive, with a high level of
    control over simulation settings and the simulation of real sensors that could
    be modified to improve or diminish data quality or insert mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'CARLA is designed to work in a client-server environment. The server is essentially
    a game created in C++ using Unreal Engine 4\. C++ speed is unquestionably critical
    to the server’s functionality: not only must the surroundings be simulated (including
    pedestrian/vehicle movements, climate modeling, etc.), but also all of the data
    required from the sensors linked to the system'
  prefs: []
  type: TYPE_NORMAL
- en: CARLA is now at version 0.9.7, and each release brings significant improvements,
    earning further attention from experts for its realism. Unfortunately, CARLA 0.9
    had just been released when our study began, and the tools we needed were not
    yet available online⁵.
  prefs: []
  type: TYPE_NORMAL
- en: Version 0.8.4 was utilized at first due to the number of jobs completed for
    the previous stable version of CARLA.
  prefs: []
  type: TYPE_NORMAL
- en: Version prior to 0.9 have limitations on the amount of control you have over
    the simulation’s parameters and the data it collects. This does not obstruct our
    study, but it does limit the useful information about the surroundings and system
    in some way. Some of these flaws still exist in previous simulator editions, but
    the bulk of them were fixed after the upgrade from version 0.8 to version 0.9².
  prefs: []
  type: TYPE_NORMAL
- en: One of the most severe difficulties was revealed to be coordinate systems. Prior
    to version 0.9, developers utilized UE4’s default coordinate system, which is
    left-handed despite the standard being right-handed. This looks to be a minor
    issue, since the problem may be easily solved with the use of a transformation
    matrix. However, due to time constraints, it was decided to stick with the developers’
    method and modify the data mostly at the analysis stage.²
  prefs: []
  type: TYPE_NORMAL
- en: 'This version of CARLA includes four sensors, which were all used in the experiments.
    Because of the Python APIs, they’re straightforward to learn:'
  prefs: []
  type: TYPE_NORMAL
- en: Cameras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A picture of the scene is provided by the final camera.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To understand depth in the surroundings, the depth map camera assigns RGB values
    to items.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CARLA (see [http://carla.org/](http://carla.org/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A semantic segmentation classifies distinct items in the view by presenting
    them in various colors based on their class.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lidar based on raycast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By lighting the object with laser beams and measuring the time it takes for
    the reflected light to return to the sensor, Light Detection & Ranging (LDR) is
    a technique for detecting the environment and estimating the distance between
    objects.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The three cameras were utilized throughout the network’s training phase. Three
    scene final cameras are mounted on the automobile to allow the driver to observe
    the surroundings.
  prefs: []
  type: TYPE_NORMAL
- en: The vehicle may acquire a color-map of the distances between objects in the
    scene thanks to the depth map camera. Semantic segmentation provides picture categorization
    features by contacting the server for ground-truth information. This is undeniably
    a simplification of a real system, in which the most powerful photo software is
    essentially other neural networks that have been trained independently. A misclassification
    can also be viewed as a control system error.
  prefs: []
  type: TYPE_NORMAL
- en: If a possible threat is detected, the safety monitor will not “fix” the misinterpretation;
    instead, it will react promptly and safely to avoid the consequences; hence, this
    simplification will have no influence on the entire approach.
  prefs: []
  type: TYPE_NORMAL
- en: The other accessible sensor for this version of CARLA is a raycast based Lidar.
    This sensor’s parameters may be readily tweaked to mimic actual Lidars like the
    Velodyne Lidar or flaws like low data quality, noisy data, or data loss. The Point
    Cloud format was used to produce the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because high hardware resources are required to mimic a genuine Lidar in the
    simulations, a significantly modified version of the Velodyne64 Lidar is used
    with the given criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of channels = 64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system’s total number of laser beams. These lasers are spaced evenly along
    the y axis. The more lasers there are, the more precise the scanning will be.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The range is 75 meters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The range of lasers in meters
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequency of rotation = 15 Hz
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These settings define the scanning beams’ rotation rate (in Hz)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.000.000 scores per each second
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The quantity of points produced by the sensor for each picture
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: FOV vertical boundaries (high = 24m, lower = -2m). Distances are measured in
    relation to the sensor’s location.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scanning’s min and max heights
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The simulator has Python APIs for modifying sensors as well as a lot of control
    over what’s being emulated, such as seeding places for spawning, pedestrian and
    vehicle behavior, and the status of “actors” in the scene, such as their position
    and velocity.
  prefs: []
  type: TYPE_NORMAL
- en: All of this information, as well as ground-truth values, is provided by the
    simulator. Simulation-related measures, such as simulation time-steps or frames
    per second, might be used. Metrics connected to actors include vehicle speed,
    collision severity, and the 3D acceleration vector.
  prefs: []
  type: TYPE_NORMAL
- en: During the testing of this tool, an issue with the Lidar sensor data was discovered,
    which has yet to be rectified. The issue caused the bounding boxes of the cars
    to be warped when they moved, resulting in very poor data. Following considerable
    investigation, an updated version of the simulator was discovered, with the bounding
    boundaries for each vehicle being modified to produce correct data. Developers
    are currently working on this problem, but it cannot be fixed without human involvement
    in the source code.
  prefs: []
  type: TYPE_NORMAL
- en: 17.12.2 Controller Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The neural network that will select what action to take to move the automobile
    is the most essential element for the controller implementation. We required a
    framework with the following features.
  prefs: []
  type: TYPE_NORMAL
- en: A code for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no significant flaws in the codebase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an atmosphere in which the network can communicate with CARLA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a default training approach available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We chose Intel AI Lab’s reinforcement-learning platform Coach after analyzing
    all of the device applications for CARLA.
  prefs: []
  type: TYPE_NORMAL
- en: 'This framework fulfills all of the following criteria: it is distributed as
    a Python package that may be edited. The project’s development team assures us
    that the product will be of high quality. There are also numerous settings to
    choose from as a starting point. Two of these choices include a CARLA simulator
    interface and a preset training strategy, which is precisely what we needed.'
  prefs: []
  type: TYPE_NORMAL
- en: The Deep Deterministic Policy Gradient method, introduced in 2015, is implemented
    in these settings. As demonstrated in the original work, this method performed
    well in tasks such as vehicle driving.
  prefs: []
  type: TYPE_NORMAL
- en: 'P1: The first setup perceives the environment using a single front camera as
    well as the additional data enhancement cameras given by CARLA. This preset’s
    agent will very definitely lack any sense of depth provided by the regular camera,
    instead depending only on the depth camera.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P2: Because the automobile is equipped with three conventional cameras as well
    as all of the data augmentation cameras described in the preceding section, this
    setup employs all of the cameras accessible in CARLA and has a much more fascinating
    design.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The usage of data augmentation cameras allows us to ignore object misclassifications
    as well as other benefits such as estimating distances between objects. This is
    a simplified form of a true architecture, in which object recognition units are
    neural networks that must be educated and assessed differently, as stated in the
    prior section.
  prefs: []
  type: TYPE_NORMAL
- en: A misclassification, on the other hand, would very definitely cause the controllers
    to act in unexpected ways, which the security monitor would have to be able to
    detect and perhaps correct. Sadly, even if the vehicle is being rewarded positively,
    the initial setting has a flaw that causes it to stop throttling. It would have
    been interesting to test this setup as well, to see how it impacts the safety
    monitor’s efficacy in terms of network data access.
  prefs: []
  type: TYPE_NORMAL
- en: It’s crucial to note that our objective isn’t to create the “ideal” agent or
    autonomous vehicle. The codebase was examined, but owing to time constraints,
    we were unable to examine all of the specifics of the given implementation. This
    framework served as an illustration of the ideas discussed in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 17.12.3 Safety Monitor Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To begin the research, we required software that could read Lidar data and
    map the environment as well as detect impediments. This software must also be
    real-time and capable of communicating with CARLA. The latter is self-evident:
    the CARLA server must receive the “brake” order whenever an alert is triggered.
    The first required some thought: one might envision capturing the Lidar data ahead
    of time and then conducting the simulations using this data. Unfortunately, this
    method will not work since we are concerned not only with having a 100% accurate
    forecast, but also with the quality of monitor’s security measures.'
  prefs: []
  type: TYPE_NORMAL
- en: If the same measurement is made in two separate simulations and the results
    are somewhat different, the monitor may respond in a completely different way
    than it would have if the real-time data provided during the model had been used.
    If the precision of measurements is solely relevant to the research simulator,
    we can’t disregard the impact that a brake has on a particular experiment.
  prefs: []
  type: TYPE_NORMAL
- en: A survey of the best “non-neural network” approaches, as well as an evaluation
    of the open-source instruments available, were done in order to build an appropriate
    security monitor. To process point cloud data, the Point Cloud library was chosen
    as the open-source library. It was written in C++ to achieve those objectives
    while processing large amounts of data. This library was initially released in
    2011, and each successive version has enhanced it, due in part to the large community
    that has helped with testing and debugging new features.
  prefs: []
  type: TYPE_NORMAL
- en: 'This package contains a set of methods that implement the most common Point
    Cloud processing techniques. The steps for creating an object-detection module
    are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Down sampling:** With a considerable degree of redundancy and noise, a single
    check can produce 100,000 records. As a first step, a down sample is generally
    required to eliminate all of the “useless” data. The Voxel Grid Filter was selected
    for this stage after considerable consideration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ground segmentation:** The first required step after downsampling (if necessary)
    is to filter out data that is worthless for object recognition, such as points
    relative to the ground. To distinguish the ground from the things we want to identify,
    these points must be filtered. In our implementation, we use the RANSAC2 algorithm,
    which is a mechanism for distinguishing between “inliers” and “outliers.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering:** This last stage is necessary to properly define what constitutes
    a scene item and which data points are linked to that item. Clustering techniques
    based on point proximity can be used to accomplish this. Because it is a range-finding
    approach calculating the Euclidean distance between points and the premise that
    dense points indicate the same item, the Euclidean Clustering Technique was chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracking and Avoidance of Objects:** Items identified in the first three
    components must be watched throughout time to determine if two objects observed
    in two sequential stages are the same entity. This is usually done by combining
    a failure safety procedure with physical models, such as the Kalman Filter, that
    anticipate their behavior over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, developing a Kalman Filter for such a complex model would have taken
    much too long, forcing us to put our study on hold. As a result, we streamlined
    the object identification and failure avoidance procedure as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Only data from in front of the automobile is saved. This is a significant improvement
    to the concept, since the monitor can now only identify obstructions in front
    of the vehicle. However, while this will undoubtedly affect the safety monitor’s
    efficacy, the ideas discussed in the preceding section remain valid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The failure prevention procedure used is based on Mobileye’s Responsibility-Sensitive
    Safety paradigm, which was proposed by Intel. When an item is detected, the speed
    of the object in relation to the system is calculated. If the system’s distance
    traveled in one second plus the distance is greater than the object’s distance
    traveled plus the space between the systems and the objects, a safety brake is
    engaged.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the safety monitor is written in C++, an architecture to communicate
    data with CARLA was created, both to use the Point Cloud library and for performance
    considerations.
  prefs: []
  type: TYPE_NORMAL
- en: The CARLA client’s point clouds. This data is analyzed in the same way as the
    previous phases, and if necessary, an alarm is given back to the client. If the
    monitor receives an alarm message, the controller’s operations are overridden,
    and a brake is applied. The object identification module was influenced by Engin
    Bozkurt’s open-source project. To adjust the detection algorithm settings to our
    purposes and to interface with CARLA, the software was heavily changed.
  prefs: []
  type: TYPE_NORMAL
- en: 17.13 Experimental Activity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This part explains how the approach created in the preceding chapter was put
    into practice, as well as the technical issues that arose during the process.
  prefs: []
  type: TYPE_NORMAL
- en: Coach’s default technique was used to train the Controllers in an urban context
    in order to produce four checkpoints that were then assessed.
  prefs: []
  type: TYPE_NORMAL
- en: 'CARLA offered 152 spawn points, which were used to create scenarios. A basic
    configuration and three variants of it were created for each spawn point:'
  prefs: []
  type: TYPE_NORMAL
- en: 'h0-Default Setting: With 30 people and 15 automobiles, the map is produced
    using the same circumstances as when the controllers were trained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'h1- Setting for Pedestrians: The map is created with the number of pedestrians
    increased from 30 to 60.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'h2- Setting Vehicles: The map is produced with the number of cars in the environment
    increased from 15 to 30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: h3- The map is made by merging h1 and h2, resulting in a scenario with 60 pedestrians
    and 30 automobiles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 152 pairs of unique seeds were produced for consistency and reproducibility
    throughout the variations, each with its own beginning point. You may use the
    same seed for many iterations of the same starting spot this way.
  prefs: []
  type: TYPE_NORMAL
- en: The automobile is also assigned a destination goal to attain, which is likewise
    documented for repeatability, to supplement the information regarding the system’s
    overall performance. If the goal is met, the achievement is noted, and the sys
    is assigned a new target to pursue.
  prefs: []
  type: TYPE_NORMAL
- en: In normal cases, a crash might take an extremely long period to occur. As a
    result, a maximum operating duration of 15 min was set to show the principles
    presented in this work, after which the system’s mission was regarded as complete.
  prefs: []
  type: TYPE_NORMAL
- en: Four checkpoints were developed and evaluated in the first part of the investigation
    using the technique outlined in Section 3\. The coding was publicly available
    because the Coach project is open-source.
  prefs: []
  type: TYPE_NORMAL
- en: 'We were able to utilize software probes to monitor the controller’s activities
    as a result of this. The source code was changed to include instructions for writing
    all required data into a separate file for every run:'
  prefs: []
  type: TYPE_NORMAL
- en: The situations’ starting position and seeds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions that the controller has taken
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What will happen if there is a collision?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a target is met, make a note of it as well as the new target coordinates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general, altering the source code may result in a reduction in overall performance
    and outcome accuracy. The CARLA simulator, thankfully, allows you to run simulations
    at a specific time step. To guarantee that all data received from the server is
    correct and timely, this was set to the minimum: ten frames per second. Because
    we have no control over the variable time-step, it may introduce unpredictability
    into the data. This also serves as a baseline for testing repeatability. It took
    roughly a month and a half to finish the initial training portion, as well as
    the Controller test.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second step, the security monitors were assessed at each checkpoint.
    The source code was changed once again to establish two autonomous, parallel processes:
    one takes Lidar data from the CARLA server and sends it to the security monitor
    server, while the other waits for the security monitors to decide whether or not
    a brake is required.'
  prefs: []
  type: TYPE_NORMAL
- en: Each frame, the CARLA system creates Lidar data. Because the burden for data
    creation is completely dependent on the CPU, execution times are significantly
    slower. Unfortunately, if FPSs are less than ten, CARLA has a problem that causes
    data to be incorrect. To guarantee the lowest constraint of ten frames per second,
    a powerful machine was necessary. Software probes were added to the codebase to
    gather information regarding the monitor’s alarms, and the source code was modified
    once more. While the safety-behavior monitor is being watched, the first step’s
    runs are now repeated. During the test, all of the alerts that were generated
    were false positives.
  prefs: []
  type: TYPE_NORMAL
- en: To assess the quantity of true positives, the emergency brake is activated two
    seconds before the accident. This method allows us to assess the safety-performance
    monitor in its operating context.
  prefs: []
  type: TYPE_NORMAL
- en: Following the completion of the first phase, the data obtained is processed
    to computerize the measurements stated in the preceding chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Checkpoints for the controller:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average time among failures
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Interval between failures (MTBF)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Rate of failure
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stability
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Security Inspector:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix of predictions confusing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Positives/negatives rates (true/false)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Perfection details
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Matthew’s Correlation of Coefficient
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Percentage of false positives
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Training is continued from the previous checkpoints using the previously mentioned
    techniques, and the measuring procedure is performed in the same manner.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the approaches mentioned need security monitor training, leading in
    exceptionally fast execution times. Because of the high processing load required
    to create Lidar data, this stage took roughly three months, after which the training
    was stopped. The data collected may be compared to evaluate if the network is
    learning correctly as well as how the security-efficacy monitor evolves over time.
  prefs: []
  type: TYPE_NORMAL
- en: Data was gathered and analyzed using Python scripts to aggregate data and provide
    metrics for the controller and the safety-overall monitor’s functionality. For
    this type of work, data aggregation is essential. However, due to the complexity
    of these systems and the infrequency of many detrimental occurrences, individual
    runs must be evaluated and compared before data can be aggregated.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the large quantity of data obtained, we offer the collected results primarily
    in aggregated form in the following part. Other issues about “raw” data, which
    were not included in this research but were provided separately for evaluation,
    exist.
  prefs: []
  type: TYPE_NORMAL
- en: 17.14 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 17.14.1 Testing the Controller (Phase 1)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The collected data is kept in different files for each run, and the measures
    specified in this chapter are computed using Python code. The obtained data is
    compared to determine the controller’s goodness and modifications in the safety-efficacy
    monitor’s across the neural network’s stages.
  prefs: []
  type: TYPE_NORMAL
- en: The distances traveled for each circumstance were plotted to determine whether
    the number of meters walked was increasing and if there were any scenarios that
    were particularly “positive” or “challenging” for the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the first step, which aids testers in distinguishing between significant
    and minor scenarios. If the distance covered by the controller in scenario x is
    less than the usual distance traveled in all checkpoints, there are two options:'
  prefs: []
  type: TYPE_NORMAL
- en: There may be a danger that the controller has not learned to handle if the controller
    always goes the same distances in all checkpoints before exploding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the distance traveled varies in a brief interval, the likelihood of a collision
    may be determined by the initial conditions. In any event, using this method,
    it’s easy to figure out which situations lead the controllers to crash rapidly,
    and each one may be investigated further.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s important to remember that if a pattern arises when testing various checkpoints,
    such as when the controller regularly works exceptionally well or exceptionally
    poorly, in both cases, a study of the particular instance is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, the rationale for “poor” runs is self-evident. It may sound
    counterintuitive but doing many “good” runs in the same circumstance necessitates
    more care. An examination of the relationship between performance and beginning
    conditions is required, and it may be shown or refuted using this method.
  prefs: []
  type: TYPE_NORMAL
- en: When the sys recognizes a “good pattern,” like a run in which the car just repeats
    the very same sequence without encountering any other cars on the road, a more
    complex problem arises.
  prefs: []
  type: TYPE_NORMAL
- en: 'The order of magnitude differs significantly between Checkpoints 1 and 4 at
    normal difficulty, and in a few situations, the maximum length set for these tests
    has been achieved. Another fascinating feature of this method is that the bottom
    constraint on distance traveled for C2 : : 4 remains constant throughout the challenges,
    remaining at 22 meters.'
  prefs: []
  type: TYPE_NORMAL
- en: A closer look at the travel distances and running times indicates that there
    are runs in which the controller makes no progress, implying that the event that
    triggered the crash was of a sort that the controller couldn’t handle. These types
    of results are undoubtedly intriguing in terms of determining whether the shortest
    runs share any environmental variables that might cause the controller to perform
    poorly. (See Figure [17-16](#Fig16).)![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig16_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig16_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-16
  prefs: []
  type: TYPE_NORMAL
- en: At normal difficulty, controller C1 moves in meters at every situation
  prefs: []
  type: TYPE_NORMAL
- en: 'The time gap between two consecutive activities is known because the simulation
    time that has elapsed between two stages of the simulation is specified and understood:
    the true total length of a single run may be estimated by counting how many actions
    the controller has performed. (See Figure [17-17](#Fig17).)![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig17_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig17_HTML.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-17
  prefs: []
  type: TYPE_NORMAL
- en: Controller C4’s distance traveled at every scenario at normal troubles
  prefs: []
  type: TYPE_NORMAL
- en: We can simply approximate the dependability function of every checkpoint in
    this way, using the mean MTTF in each degree of difficulty.![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig18_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig18_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-18
  prefs: []
  type: TYPE_NORMAL
- en: After x min of reaction, the chance y of the systems being operational is shown
    graphically
  prefs: []
  type: TYPE_NORMAL
- en: The MTTF of the initial checkpoint is relatively low (20 sec), as predicted,
    and grows as the number of checkpoints rises. Interestingly, the second checkpoint
    appears to perform better than the third and fourth checkpoints. Despite this,
    analysis of a portion of the runs in which the second checkpoint accomplished
    extremely good results in terms of time to collapse and distance traveled revealed
    that its driving is far more dangerous than the other two and is likely to result
    in a poor driving style in which the car prevented a crash almost every time by
    steering at the last possible moment. Another indication that C2 isn’t always
    superior to C3 and C4\. C3/C4 indicates that the apparent increased safety is
    most likely due to “fortunate patterns” in the sense indicated above.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, as the findings from Phase 3 will demonstrate, the controller was
    able to overcome this issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Checkpoint distances follow a similar pattern: longer runs are really trips
    in which the automobile travels more meters. This guarantees that the system isn’t
    “cheating”: if the car isn’t moving at all, the execution time will be greater.
    We may estimate the durability function in terms of km traveled prior to crashing
    by computing the average distance traveled by each checkpoint in each level of
    danger.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the observed advances in the distance to failure tend to coincide
    with the progress for a while to failure, implying that the two metrics are linked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because CARLA permits the kind of item with which a collision happened to be
    recorded, rates of the type of object collided in relation to total collisions
    are calculated for every checkpoint, for each risk category. This method is beneficial
    for a number of reasons: (See Figure [17-19](#Fig19).)'
  prefs: []
  type: TYPE_NORMAL
- en: To see whether the automobile slams into walls, lamp poles, or fences, indicating
    that it is going off-road.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To see how the automobile reacts to different levels of difficulty; for example,
    if the number of people is raised and there are fewer pedestrian collisions, the
    system may be capable of preventing human accidents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To recognize the system’s capacity to prevent failures when interacting with
    certain things.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig19_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig19_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-19
  prefs: []
  type: TYPE_NORMAL
- en: After x km, the chance y of the system becoming operational is shown graphically
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Figl_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Figl_HTML.jpg)Those
    diagrams may be used to see how the system performs as the scenario’s complexity
    is increased while it is being trained, as well as to see whether there are any
    correlations between the types of collision and the security checks. The first
    checkpoint, as expected, collides with off-road objects, showing the controller’s
    weak driving abilities. (See Figure [17-20](#Fig20).)![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig20_HTML.png](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig20_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-20
  prefs: []
  type: TYPE_NORMAL
- en: Charts
  prefs: []
  type: TYPE_NORMAL
- en: It also appears that the ratio of collisions with generic barriers has a tendency
    to follow the total checkpoint achievement. When comparing this graph to the contributing,
    it can be seen that C1 has the largest ratio of collisions against barriers, whereas
    C2 has the lowest ratio. C3 increased the frequency of these impacts, which was
    accompanied by a decrease in the MTTF/ MDTF, which was later decreased by C4.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the four degrees of difficulty described were fairly simple, they
    proved to be an effective means of evaluating the system’s performance in situations
    other than the training environment. In problems three and four MTTFs and MDTFs
    are significantly lower, indicating that situations with high traffic are more
    difficult to manage. At the similar time, the elements of the difficulty chosen
    influence the increase/decrease in hit percentages with a certain type of object.
  prefs: []
  type: TYPE_NORMAL
- en: This means that the reported collisions are dependent on the surroundings. Increasing
    the population, as well as the number of automobiles, leads to more collisions
    with these “objects.” As the number of people and automobiles grows, it becomes
    clear that collisions with other vehicles remain the major cause of crashes, showing
    that crowded traffic puts the controller in more hazardous positions.
  prefs: []
  type: TYPE_NORMAL
- en: These characteristics are important in defining more difficult problems. These
    metrics also allow for the detection of connections between the rise and fall
    of different types of collisions.
  prefs: []
  type: TYPE_NORMAL
- en: By using the technique provided for the first step, which includes the development
    of situations, issue levels, and metrics selected, many features of the controller
    may be seen without actively watching its executions. Furthermore, by combining
    the few measures specified, potential connections between ambient circumstances,
    starting conditions, and checkpoints might be overlooked.
  prefs: []
  type: TYPE_NORMAL
- en: 17.14.2 Monitor Assessment (Phase 2)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The security may be assessed after the controller has been verified and its
    runs have been logged in order to measure its prediction accuracy and how it affects
    the overall system’s safety.
  prefs: []
  type: TYPE_NORMAL
- en: 'As previously indicated, Lidar data is collected at every frame and transmitted
    to the security monitor. These data are then evaluated using the algorithm outlined
    in the Sensor Development sections, and a message with specifics on whether the
    vehicle is braking is sent back to the control system. There is no coordination
    between these two components, as there is in real-world systems of this type:
    Because this would result in choppy runs, the controller would be unable to wait
    for the security monitor at every frame.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following technique is used to calculate the quantities of real positives,
    real negatives, fake positives, and false negatives:'
  prefs: []
  type: TYPE_NORMAL
- en: The control system runs recorded in the initiation section are replicated after
    attaching the Safe operation to the automobile.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Safe operation is permitted to issue alerts, but the safety brake is not
    applied until the period t has passed since the system entered an alert state,
    as described in Chapter [3](520777_1_En_3_Chapter.xhtml).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because a choice of a “early” t may induce a brake for an activity that the
    processor managed, and a choice of a “late” t may cause the safety monitor to
    fail merely because the safe stop was activated too late, t is an important parameter
    for the observation’s dependability. When a vehicle transitions from a secure
    to an alert condition, the safety brake should be activated as quickly as possible.
    Unfortunately, understanding when this transition occurred is not always feasible
    (if not impossible in complicated runs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All warnings produced before t are considered false positives, based on the
    average speed of the automobile in relation to the velocity limit and the time
    required to analyze Lidar data and obtain an answer from the security monitor,
    which was set at two seconds in this study.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: True negatives are any frames for which the monitor reaches a prediction but
    does not issue an alarm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the system lives for the maximum duration set for a single mission, but the
    safety issues an alarm after time t, a false positive warning is raised.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An effective avoidance of an accident (if one occurs) as a result of the Safety-brake
    Monitor’s is regarded as a real positive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of whether the Safe operation triggered an alert or not, if a collision
    occurs, it is classed as a false negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After determining the amounts of genuine, fake, affirmative, and negatives,
    they can be combined in more exact measures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The technique proposed in this study looks to be a good strategy to understand
    and verify the controller’s behavioral assumption as well as monitor the monitor’s
    behavior. The monitor’s efficacy appears to be impacted by the controller’s dependability.
    While the True Positive Rate remains steady around 0.75, we can observe how the
    monitor’s precision decreases after C2, raising some questions about the safety
    monitor’s long-term efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: The decrease in precision indicates an increase in false positives, which leads
    us to believe that the monitor is unable to accurately identify the system’s states
    owing to the controller’s new behavior. The MCC of every checkpoint supports this
    notion, exhibiting the same accuracy behavior that the controller is taught. We’re
    interested in seeing if the MCC lowers in phase 3 once the controller is restrained
    because it’s a measure of the monitor’s performance overall.
  prefs: []
  type: TYPE_NORMAL
- en: As expected, this monitor creates a large number of false positives. This has
    no effect on the efficacy of the surveillance approach outlined, but it does assist
    to understand how the system would behave if both parts functioned together, i.e.
    with a lot of “false” security produced by the safety-false monitor’s alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the accuracy-recall curves offers a fast visual representation of the
    safety-performance monitor’s decline, which works well with a “dumb” controller
    but becomes nearly disruptive with better trained controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [17-21](#Fig21) demonstrates that, despite its low efficiency, the monitors
    perform best at the highest difficulty setting, i.e. when the number of people
    and automobiles in the scenarios is doubled. The controller’s second-best performances
    are shown with the second-most difficult difficulty, i.e. the level when the number
    of vehicles is increased, after C1\. Although the link between something “hard”
    for the controllers and something “hard” for the observer cannot be completely
    examined in this study, charting these pictures revealed unexpected behavior,
    leading us to look into this more in future studies.![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig21_HTML.png](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig21_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-21
  prefs: []
  type: TYPE_NORMAL
- en: The safety monitor forecast rates for control points 1 to 4
  prefs: []
  type: TYPE_NORMAL
- en: These findings are in line with the data collected and analyzed, as the security
    performs well with C1, while C2 to C4 produce almost identical outcomes, with
    C2 producing somewhat superior results.![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig22_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig22_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-22
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy recall curve for checkpoints C1–4 based on observed values at every
    level of complexity
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig23_HTML.jpg](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig23_HTML.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-23
  prefs: []
  type: TYPE_NORMAL
- en: Comparing of PRC for C1 through C4
  prefs: []
  type: TYPE_NORMAL
- en: To comprehend the results of the safety monitor’s predictions, we had to personally
    perform and watch some of the best runs for Checkpoints 2 in phase 1\. High speed
    is the major reason for monitor failures for C2.7, according to the percentage
    of false-positive causes shown in the table. Another intriguing aspect of this
    type of data collection is that it appears that as the network learns to handle
    new situations, the security produces more false alarms, and, more importantly,
    the scenarios that will eventually result in a crash are of a new type that the
    watch is unable to detect.
  prefs: []
  type: TYPE_NORMAL
- en: The table in Figure [17-24](#Fig24) shows how this approach works and how a
    great quantity of information and proof about the system’s behavior can be gathered
    using only a few observations. Depending on the real hardware available and the
    simulation model, further measurements can be acquired and combined to supplement
    this information.![../images/520777_1_En_17_Chapter/520777_1_En_17_Fig24_HTML.png](../images/520777_1_En_17_Chapter/520777_1_En_17_Fig24_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17-24
  prefs: []
  type: TYPE_NORMAL
- en: 'C1 generates a high percentage of false - negative: : : C4'
  prefs: []
  type: TYPE_NORMAL
- en: The monitor’s effectiveness appears to be impacted by how the controller acts,
    as we anticipated at the start of this project. Most significantly, evidence shows
    that the monitor’s prediction accuracy is unrelated to the length of time the
    controller has been taught. Retraining the network with various techniques and
    using the same monitoring approach for the operators may confirm or refute this
    finding.
  prefs: []
  type: TYPE_NORMAL
- en: 17.14.3 Retraining and Rechecking (Phase 3)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If Phase 2 suggested that the efficiency of the same security varies when applied
    to different checkpoints, restructuring with different strategies and comparing
    the performance of controllers trained in these methods to the one we used as
    the default approach will be critical in confirming or refuting this theory. In
    this phase, we retrained C4 using the five techniques listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: S0) The Coach framework’s basic technique, which was used to train C1... 4,
    with the optimization method calculated depending on the total neural network
    quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S1) When the automobile strikes anything, the functional form is now more severe,
    but braking is somewhat more rewarding, as long as the car does not stop going.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S2) The security monitor is connected to the system, and if an alarm is raised,
    the sensor’s response takes precedence over the network’s.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3) If the monitoring raises an alarm, the network’s activity is substituted
    by the monitor’s. For acting like the monitors, the network receives a good incentive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S4) If an alarm is triggered, the training phase is terminated and the system
    is given a poor incentive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result is {C5S0, C5S1, C5S2, C5S3, C5S4}.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the failure of the S2, S3, and S4 techniques, regrettably, the safety monitor-based
    methods did not yield the expected results. S3 and S4 stop the controller from
    moving, while S2 causes it to stop after a few meters if there is something in
    front of the vehicle, even if it is a considerable distance away. Because not
    moving is a poor reward, none of these checkpoints receive an unintentionally
    large reward for their behavior. As a consequence, the scenario in which the neural
    net becomes stuck between a local minimum and maximum may remain a theory, but
    we feel the training time was too short.
  prefs: []
  type: TYPE_NORMAL
- en: The monitor’s extremely large false positive rate is probably certainly to blame.
    Extended training in this manner taught the vehicle not to move in order to avoid
    the safety alarms and monitors. Even though these techniques account for the possibility
    of the car not moving by penalizing it with a -ve benefit, it was not sufficient
    to keep the automobile from remaining still.
  prefs: []
  type: TYPE_NORMAL
- en: We think that the DDPG algorithm’s reward function is well-defined, in the sense
    that it accounts for all good and bad behaviors, and that immediate rewards are
    well-balanced, as there are no inadvertently huge payments. However, adjusting
    all of the reward factors to prevent results like the one we witnessed is exceedingly
    tough owing to the difficulty and diversity of circumstances that a network should
    be allowed to drive a vehicle. Most important, we have no way of knowing if this
    behavior is a product of the reward function’s design or if it requires more training
    to observe intriguing behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: The most important issue with training is the time commitment; it is difficult
    to teach n agents using n techniques and then wait for them to be taught before
    determining which approach produced the best results.
  prefs: []
  type: TYPE_NORMAL
- en: The tuning of the network’s parameters is primarily dependent on regulations
    and heuristic approaches, as demonstrated by many publications. These methods,
    on the other hand, have shown to be time-consuming and error-prone.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, at this time, trial-and-error appears to be the sole way to measure
    the reward function’s efficiency. Because training data is still being investigated
    and there are no defined laws for self-driving vehicles, this problem, which is
    unique to these algorithms, remains unresolved. In academia, the design of the
    reward function for these relatively new algorithms is a hot issue, with academics
    trying to find out how to offer an appropriate learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Simultaneously, as previously said, AV training needs a huge amount of data,
    which is frequently inaccessible or insufficient as compared to traditional training
    methods. There are several frameworks for designing decision-making rules for
    relevance feedback, but most of them seem to focus on how to speed up the learning
    process rather than how to add learning factors to these networks.
  prefs: []
  type: TYPE_NORMAL
- en: Creating probabilistic models to aid in the calibration of reward system parameters
    prior to the controller’s retraining phase would be a fascinating approach. Because
    the protection performance measures were effectively approximated, the technique
    presented in this book would have a substantial impact on the construction of
    these models.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, the reward function has no means of knowing ahead of time what value
    each state should be assigned. Keep in mind that we are in a full spatial condition,
    which is difficult to depict and makes it hard to evaluate all potential events
    and transitions, making our task much more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, because of the complexity and uniqueness of these systems and
    techniques, more research is required to address these issues, and the development
    of a simulation process to aid fine-tuning will be critical in future work.
  prefs: []
  type: TYPE_NORMAL
- en: 17.15 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter looked at how to do a checking activity on self-driving automobiles
    managed by a controller and a sys admin.
  prefs: []
  type: TYPE_NORMAL
- en: Such systems demand special attention due to their complexity and the main problems
    of the ecosystem in which they function. This research is meant to be a first
    step in defining how such systems should be monitored. Because they are regularly
    employed in military activities, automatic vehicles are not a novel sort of equipment.
    However, because self-driving cars will be operating in close proximity to humans
    and in an urban environment, greater caution will be necessary in coping with
    the wide variety of events that may occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also questioned if there were any links between the efficacy of the relative
    safety and that of the control element when the latter was taught for long periods
    of time using various approaches. We covered the basic principles of these activities
    in the initial parts, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: The definitions of reliability and safety, which are both critical components
    of safety. In the context of self-driving automobiles, critical systems were examined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems that prohibit us from deploying self-driving automobiles in metropolitan
    areas at this point in time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How an incorrect/poor/optimistic evaluation of these systems’ reliability might
    have catastrophic repercussions when they are implemented.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems in analyzing observed emerging behavior as a result of the interaction
    of two main constituent components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the efficacy of the safety monitor impacted by the controller’s behavior,
    and if so, how? By the duration of the training and the strategies used throughout
    the training.
  prefs: []
  type: TYPE_NORMAL
- en: The instruments accessible for this type of activity were considered in the
    creation of this initial technique. We have to seek open-source alternatives because
    most solutions in this sector are private and proprietary, indicating that performing
    these studies in non-professional contexts is difficult.
  prefs: []
  type: TYPE_NORMAL
- en: The Lidar sensor issue would have rendered our study unfeasible, and it could
    have been impossible if it hadn’t been for Zhuang’s effort, because our only alternative
    would have been to rely on ground observations, which would have been useless
    for our research.
  prefs: []
  type: TYPE_NORMAL
- en: While the Coach framework was created by Intel AI Labs, it has the usual problems
    that plague open-source projects, the most notable of which being the failure
    of one of the two CARLA agents to move after a few rounds of training in Phase
    1\. This problem makes us question if there were other flaws in the training algorithm’s
    architecture that contributed to the C5S2:::4 behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue that obstructs the framework’s use is that the memory is not properly
    released, leading it to overflow and crash the running processes. As a result,
    we had to maintain a close check on the training process and redo it after each
    mishap.
  prefs: []
  type: TYPE_NORMAL
- en: 'The monitoring activity was created with the following major characteristics
    in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency:** By saving the controller’s activity in each frame, as well
    as the RNGs1 seeds, goal objectives, and the environment variable of every circumstance
    in files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The ability to be unobtrusive:** Because of the client-server design, code
    instrumentation may provide non-real-time data or incorrect measurements. This
    difficulty is solved by CARLA, which allows users to conduct simulations at a
    set time step. But the h/w should be capable of achieving the specified time step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adequate representation:** One of the main issues with monitoring self-driving
    cars is the representativeness of test cases, because dangerous occurrences might
    be so complicated and numerous that it is difficult to evaluate them all. In this
    regard, defining scenarios and difficulty levels helps provide a variety of scenarios
    based on the same situation. Concerns regarding the dataset’s nature arose from
    the predictive value of the metrics generated for the security monitor. The selection
    of the metrics to be utilized in the confusion matrix, as well as the exclusion
    of the others, were explained and justified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability:** The search for the appropriate tools to employ for this task
    was difficult owing to the large number of options presented, many of which had
    significant practical constraints. However, the technologies we utilized in this
    project allowed us to complete this task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the same time, we cannot overlook the problems that these initiatives have.
    This approach has shown to be a helpful tool for analyzing the performance of
    the entire system, with a focus on well-known metrics for evaluating a system’s
    reliability and the security monitor’s effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: The creation of situations and problems is essential for establishing whether
    external factors impair performance of the system and for creating and evaluating
    “uncommon” circumstances that cannot always be covered when building ad hoc safety
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: The main hypothesis that inspired us to start this study was to examine if the
    performance of “static” error-checkers was impacted by neuronal network performance,
    and if ad hoc training approaches had an impact as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the data gathered, the monitor’s efficacy with expert agents is
    expected to alter, encouraging us to further investigate this hypothesis. The
    failure of those methods (S2, S3, S4) that employed the safety monitor to direct
    the training did not provide us with proof of this reality, and this issue was
    highlighted at the beginning of the chapter. There are two possible causes for
    this failure:'
  prefs: []
  type: TYPE_NORMAL
- en: A reward function that isn’t defined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training duration was insufficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We believe the neural net required more time to comprehend what the security
    was attempting to educate since the trained agents did not receive an accidentally
    huge reward for keeping motionless. A bigger simulation time step would have allowed
    us to run simulation time faster than real time, which would have helped us save
    time.
  prefs: []
  type: TYPE_NORMAL
- en: We were unable to train the models for extended periods of time or retrain the
    networks by altering the reward parameters since this required hyper hardware.
    Around the same time, the security proved to be less effective when used in conjunction
    with the checkpoint that produced the best results, which was obtained from Checkpoint
    C4 by modifying the training method, whereas and it has the same TPR (0.75) when
    used in conjunction with the security checks trained with method S0, except perhaps
    Checkpoint C2.
  prefs: []
  type: TYPE_NORMAL
- en: While Checkpoint C2 was unquestionably a unique instance for the reasons outlined
    in earlier chapters, the same theory cannot be ruled out for C5S1\. We want to
    continue training C5S1 in the future to validate this reality. This study demonstrated
    the benefit of analyzing self-driving automobiles in a simulated environment,
    taking into account and overcoming many of the difficulties that emerge when calculating
    relevant metrics for such systems, as well as usual monitoring concerns.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter wraps up the book and gives a broad overview of blockchain
    technology and distributed system environment.
  prefs: []
  type: TYPE_NORMAL
