- en: CHAPTER 7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dally’s Parallel Paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Is this Life after Google or what?
  prefs: []
  type: TYPE_NORMAL
- en: Bill Dally is about to take me to the Palo Alto Caltrain station in his self-driving
    Tesla Model S.[¹](notes.html#ch07note-1)
  prefs: []
  type: TYPE_NORMAL
- en: In the Nvidia garage in Santa Clara, I board the sleek gray boron steel and
    titanium missile, noting its futuristic payload of a 1,200-pound lithium-ion battery.
    Should be enough to get me to the station. Fully charged, it can almost replace
    sixty pounds of gasoline in the tank of an internal combustion engine. That might
    not seem like much, but in Google-era mathematics it can save the world.
  prefs: []
  type: TYPE_NORMAL
- en: In calculating the energy budgets of its data centers, Google, like the rest
    of Silicon Valley, is as rigorous as a Kenyan marathoner. But you had better recheck
    its numbers when it begins rolling out cars gleaming in the sun of solar subsidies.
    They may cost “waymo” than they say.
  prefs: []
  type: TYPE_NORMAL
- en: This is a Tesla, though, and its self-driving aspiration comes from Nvidia’s
    industry-leading Drive PX system. To buckle myself into the bucket seat, I have
    to push aside a flier from the annual Hot Chips conference in nearby Cupertino.
    While I was analyzing semiconductors for Ben Rosen and Esther Dyson some three
    decades ago, when chips were still hot, I used to go to Hot Chips conferences
    to stay up to date. Silicon, then as now, was the foundation, the physical layer,
    underlying the entire edifice of information technology. I am reassured that Hot
    Chips lives on even though Google and others assert that “software eats everything.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Nick Tredennick, the designer of a favorite “hot chip” of yore, the Motorola
    68,000 microprocessor behind Steve Jobs’s Macintosh computer, used to say that
    the industry seeks to exploit the “leading edge wedge.” Three overlapping design
    targets converged in this fertile crescent of chip design: zero delay (fast hot
    chips), zero power (cool low-energy devices), and zero cost (transistors going
    for billionths of a penny).[²](notes.html#ch07note-2) Between the 1980s and 2017,
    chips have been migrating from the hot fast end toward the cool cheap end, a trend
    Dally has led.'
  prefs: []
  type: TYPE_NORMAL
- en: In the Tesla’s front seat, I face a two-foot-high screen displaying pale green
    and striated Google maps. Dally points out that self-driving vehicles “don’t care
    where the road lanes are. They navigate on maps, register their place on a map.
    If they have an empty road, they just take a line down the middle, like they are
    riding a rail. It is only the presence of moving objects, such as pedestrians
    and other cars, that requires them to use all their motion-sensing capabilities.”
  prefs: []
  type: TYPE_NORMAL
- en: While the maps come from Google, the processing comes from Nvidia GPUs. These
    chips compute the car’s response to lidar, radar, ultrasound, and camera signals
    that free the missile to descend from the outer space of Elon Musk’s domains and
    enter the ever-changing high-entropy world beyond Google Maps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dally barks his command: “Navigate to California Avenue Caltrain station,”
    and the car crisply responds. Dally comments, “In the last couple years speech
    recognition has become dramatically better. Thirty-percent better. Two years ago
    it was not really capable of getting it right. But now with machine learning on
    our Tegra chips, it gets it right every time.” Benefiting are all the users of
    Amazon’s Alexa, Apple’s Siri, Microsoft’s Cortana, Google’s Go.'
  prefs: []
  type: TYPE_NORMAL
- en: Dally has his hands on the steering wheel now as he negotiates the back streets.
    “It’s only level-two autonomy,” he explains, using the Society of Auto Engineers’
    classifications, which range from level one, a mere driver assistant, to level
    five, full self-driving. Musk promises to get Tesla to level five in two years.
    That’s Elon for you. But for now Dally keeps his eye on the road as the Tesla
    makes its way, with several high-voltage bursts, up the ramp onto 101\. Now Tesla’s
    self-driving mode enables him to turn and show me his film of the recent solar
    eclipse—a series of vivid high-contrast images of the rare event.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning, Dally points out, is mostly accomplished by graphics processing
    chips from Nvidia. Some advances in artificial intelligence spring from improvements
    in algorithms, but the real source of these capabilities is the explosive improvement
    in computer speed achieved through a combination of Moore’s Law and parallel processing.
    Nvidia’s graphics processors are the climax of Dally’s long career as a prophet
    of parallel processing, which began thirty years ago at Virginia Tech, where he
    studied the virtues of multiple processors functioning together.
  prefs: []
  type: TYPE_NORMAL
- en: At a Hot Chips conference at Stanford in August 1991, Dally and Norm Jouppi
    first emerged as foils in fashioning the future philosophies of computation. Dally
    introduced his revolutionary massively parallel J-machine, and Jouppi, now at
    Google, then at Digital Equipment, touted the promise of revving up existing processor
    pipelines to “five instructions per-clock” cycle.[³](notes.html#ch07note-3)
  prefs: []
  type: TYPE_NORMAL
- en: 'Those two papers of 1991 polarize all computer science: do you make existing
    serial von Neumann processors go faster, seeking zero delay, stepping and fetching
    instructions and data from ever faster remote memories? Or do you diffuse the
    memory and processing all through the machine? In a massively parallel spread
    like Dally’s J-machine, the memory is always close to the processor.'
  prefs: []
  type: TYPE_NORMAL
- en: Twenty-six years later, Dally and Jouppi are still at it. At the August 2017
    Hot Chips in Cupertino, all the big guys were touting their own chips for what
    they call “deep learning,” the fashionable Silicon Valley term for the massive
    acceleration of multi-layered pattern recognition, correlation, and correction
    tied to feedback that results in a cumulative gain in performance. What they call
    “learning” originated in earlier ventures in AI. Guess, measure the error, adjust
    the answer, feed it back are the canonical steps followed in Google’s data centers,
    enabling such applications as Google Translate, Google Soundwriter, Google Maps,
    Google Assistant, Waymo cars, search, Google Now, and so on, in real time.[⁴](notes.html#ch07note-4)
  prefs: []
  type: TYPE_NORMAL
- en: As recently as 2012, Google was still struggling with the difference between
    dogs and cats. YouTube was famous for its cat videos, but it could not efficiently
    teach its machines to recognize the cats. They could count them; the data center
    dogs could dance; but it took sixteen thousand microprocessor cores and six hundred
    kilowatts.[⁵](notes.html#ch07note-5) And it still was a dog, with a 5 percent
    error rate—not an impressive portent for Google’s human face-recognition project
    or for car vision systems that need flawlessly to identify remote objects in real
    time.
  prefs: []
  type: TYPE_NORMAL
- en: As Claude Shannon showed, these success rates of 95 percent, or even 99.999
    percent, are deceptive, because you have no way of telling which instances are
    the errors.[⁶](notes.html#ch07note-6) The vast majority of the home loans in the
    mortgage crisis were sound, but because no one knew which ones were not, all the
    securities crashed. You don’t want that problem with self-driving cars.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a joint appearance in 2012 in Aspen, Peter Thiel chided Eric Schmidt: “You
    don’t have the slightest idea of what you are doing.” He pointed out that the
    company had amassed some $50 billion in cash at the time and was allowing it to
    sit in the bank at near-zero interest rates while its vast data centers still
    could not identify cats as well as a three-year-old could.[⁷](notes.html#ch07note-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Thiel is the leading critic of Silicon Valley’s prevailing philosophy of “inevitable”
    innovation. Page, on the other hand, is a machine-learning maximalist who believes
    that silicon will soon outperform human beings, however you want to define the
    difference. If the haphazard Turing machine of evolution could produce human brains,
    just imagine what could be accomplished by Google’s constellation of eminent academics
    devoting entire data centers full of multi-gigahertz silicon to training machines
    on petabytes of data. In 2012, though, the results seemed underwhelming.
  prefs: []
  type: TYPE_NORMAL
- en: Simultaneously with the dogs and cats crisis in 2012, the leader of the Google
    Brain research team, Jeff Dean, raised the stakes by telling Urs Hölzle, Google’s
    data center dynamo, “We need another Google.” Dean meant that Google would have
    to double the capacity of its data centers just to accommodate new demand for
    its Google Now speech recognition services on Android smartphones.
  prefs: []
  type: TYPE_NORMAL
- en: Late in the year, Bill Dally provided an answer. Over breakfast at Dally’s favorite
    Palo Alto café, his Stanford colleague Andrew Ng, who worked with Dean at Google
    Brain, was complaining about the naming of cats. Sixteen thousand costly microprocessor
    cores seemed inefficient. Dally suggested that Nvidia GPUs could help. Graphics
    processors specialize in the matrix multiplication and floating-point mathematical
    operations that teach machines to recognize patterns. A graphical image is an
    array of values readily mapped to a mathematical matrix. Running images through
    as many as twelve layers of matrices, machine learning could be seen as another
    form of iterative graphics processing.
  prefs: []
  type: TYPE_NORMAL
- en: Prove it, Ng told Dally, and Google would buy his chips.
  prefs: []
  type: TYPE_NORMAL
- en: 'The man who built the first crude graphics processor, the precursor of all
    of Google’s data center neural networks, was Frank Rosenblatt, a psychology professor
    at Cornell. In 1958 he described his “perceptron” to the New Yorker: “If a triangle
    is held up to the perceptron’s eye [photosensor], the association units connected
    with the eye pick up the image of the triangle and convey it along a random succession
    of lines to the response units [now called neurons], where the image is registered. . . .
    [A]ll the connections leading to that response are strengthened [i.e., their weights
    are increased], and if a triangle of a different size and shape is held up to
    the perceptron, its image will be passed along the track that the first triangle
    took. If a square is presented, however, a new set of random lines is called into
    play. . . . The more images the perceptron is permitted to scan, the more adroit
    its generalizations. . . . It can tell the difference between a dog and a cat.”[⁸](notes.html#ch07note-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Four years later, Ray Kurzweil, then sixteen, visited Rosenblatt after Kurzweil’s
    MIT mentor Marvin Minsky exposed the limitations of the one-layer perceptron that
    Rosenblatt had built. Rosenblatt told Kurzweil that he could surmount these limitations
    by stacking perceptrons on top of one another in layers. “The performance improves
    dramatically,” he said. Rosenblatt died in a boating accident eight years later,
    never having built a multilayered machine.
  prefs: []
  type: TYPE_NORMAL
- en: Now at Google, that omission was being remedied. Dally assigned Nvidia’s software
    guru Frank Canizaro to work with Ng on upgrading Nvidia’s proprietary software
    CUDA (Compute Unified Device Architecture) for use in its CUDA Deep Neural Network
    library (cuDNN). The Stanford-Google-Nvidia team solved the cat-and-dog problem
    with merely twelve GPUs burning just four kilowatts, all for just thirty-three
    thousand dollars.
  prefs: []
  type: TYPE_NORMAL
- en: Dally was proud of this achievement. The Nvidia machine was roughly 150 times
    as cost-effective as Google’s previous setup. And that’s not even taking into
    account the GPUs’ enormous advantage in energy efficiency. Nvidia processors soon
    pervaded Google’s data centers, giving unprecedented performance in the matrix
    multiplications and accumulations at the heart of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Google now deploys ten-to-twelve-layer neural networks generating thirty exa-flops
    of floating point mathematical computing capacity—and matrix multiplications galore.
    In accord with Rosenblatt’s prediction that the “more images the perceptron is
    permitted to scan, the more adroit its generalizations,” the Google machine sorts
    tens of millions of images according to some one billion parameters. It prompts
    Google Brain routinely to claim to be “outperforming humans.” Gee, a billion parameters,
    beats me! In Silicon Valley, where human beings program these machines, it is
    considered cranky to question the claim of “superhuman” powers.
  prefs: []
  type: TYPE_NORMAL
- en: None of this would faze Dally, except for one crucial change at Google. At the
    2017 Hot Chips conference, the company, in a do-it-yourself mood, indicated that
    it would henceforth replace Nvidia’s devices with its own special-purpose silicon.
    Jeff Dean celebrated Jouppi’s souped-up “Tensor” “matrix multiplier,” which eschewed
    graphics and floating point, focusing on the machine learning functions alone.
    It’s a matrix multiplier ASIC (application-specific integrated circuit). Without
    their Tensor processing unit, say the Google guys, they would have had to double
    the size of their data centers.
  prefs: []
  type: TYPE_NORMAL
- en: Dally points out that it is always possible to make huge temporary gains by
    putting entire systems onto single slivers of ASIC silicon, special-purpose chips
    hard-wired to perform one complex function. As Dally tells me, in performing parallel
    operations, graphics processors are ten times more cost-effective than general-purpose
    central processing units (CPUs), and ASICs are ten to a hundred times more cost-effective
    than ordinary GPUs. But with ASICs, your market is reduced to just your chosen
    special purposes, and your data centers are no longer all-purpose Turing machines.
    They are ossifying into special-purpose factories like the aluminum plants they
    succeeded in The Dalles.
  prefs: []
  type: TYPE_NORMAL
- en: Google can afford to make its own custom ASICs for particular slots in its data
    centers, but Nvidia is dominating the entire domain of massively parallel processing.
    In the third quarter of 2017, after the Hot Chips “setbacks,” Nvidia announced
    a 109 percent increase in revenues from cloud computing sales, to $830 million,
    lifting the company’s market value to almost $130 billion.
  prefs: []
  type: TYPE_NORMAL
- en: Now Nvidia is a potent force providing parallel processors across global industry
    and providing new platforms for life after Google. Is all this to come to an end
    with Google’s new prowess in making hardware as well as software, hiring industry
    hardware titans such as Dave Patterson and Norm Jouppi to contrive world-leading
    chip architectures?
  prefs: []
  type: TYPE_NORMAL
- en: I was visiting Dally to find out. A fifty-seven-year-old, brown-haired engineer
    with a black hat and backpack and hiking boots, he is dressed Silicon-Valley-mountaineer
    style to take me on a high-altitude adventure in microchips and software, ideas
    and speculations, Google maps and Elon Musk “reality distortion fields” down Route
    101 at five o’clock on a late-August Friday evening.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not quite Doctor Brown’s Back to the Future ride in a DeLorean, but it
    will suffice for some modest time-travel in the history of computing.
  prefs: []
  type: TYPE_NORMAL
- en: Since writing his college thesis in the late 1970s, Dally has rebelled against
    the serial step-by-step computing regime known as the von Neumann architecture.
    After working on the “Cosmic Cube” under Chuck Seitz for his Ph.D. at Caltech
    (1983), Dally has led design of parallel machines at MIT (the J-machine and the
    M-machine), introduced massive parallelism to Cray supercomputers (the T-3D and
    3E), and pioneered parallel graphics at Stanford (the Imagine project, a streaming
    parallel device incorporating programmable “shaders,” now ubiquitous in the industry’s
    graphic processors from Nvidia and others).
  prefs: []
  type: TYPE_NORMAL
- en: In all these projects Dally was warring against the conventional computer architecture
    of step-by-step serial processing—associated with a memory problem called the
    “von Neumann bottleneck.” You live in the real world, right? The real world offers
    intrinsically parallel problems such as images that flood the eye all at once,
    whether you’re driving a car in the snow or summoning a metaverse with computer-generated
    graphics or pattern-matching in “machine learning” argosies across the seas of
    big data.
  prefs: []
  type: TYPE_NORMAL
- en: The von Neumann bottleneck was recognized by von Neumann himself. In response,
    he proposed a massively parallel architecture called cellular automata, which
    led to his last book before his death at age fifty-seven. In The Computer and
    the Brain, he contemplated a parallel solution called neural networks, which were
    based on a primitive idea of how billions of neurons might work together in the
    human neural system.
  prefs: []
  type: TYPE_NORMAL
- en: Von Neumann concluded the brain is a non-von machine nine orders of magnitude
    slower than the gigahertz he prophesied for computers back in 1957\. Amazingly,
    von Neumann anticipated the many-million-fold “Moore’s Law” speedup that we have
    experienced. But he estimated that the brain is nine orders of magnitude (a billion
    times) more energy-efficient than a computer. This is a delta larger even than
    that claimed by the guys from Google Brain for their Tensor chip. In the age of
    Big Blue and Watson at IBM, the comparison remains relevant. When a supercomputer
    defeats a man in a game of chess or Go, the man is using maybe fourteen watts
    of power, while the computer and its networks are tapping into the gigawatt clouds
    on the Columbia.
  prefs: []
  type: TYPE_NORMAL
- en: In the age of Big Data, the von Neumann bottleneck has philosophical implications.
    The more knowledge that is put into a von Neumann machine, the bigger and more
    crowded its memory, the further away its average data address, and the slower
    its functioning. Danny Hillis, of the erstwhile Thinking Machines, writes, “This
    inefficiency remains no matter how fast we make the processor, because the length
    of the computation becomes dominated by the time required to move data between
    processor and memory.” That span, traveled in every step in the computation, is
    governed by the speed of light, which on a chip is around nine inches a nanosecond—a
    significant delay on chips that now bear as much as sixty miles of tiny wires.
  prefs: []
  type: TYPE_NORMAL
- en: What Dally saw is that the serial computer has reached the end of the line.
    Most computers (smartphones and tablets and laptops and even self-driving cars)
    are not plugged into the wall any more. Even supercomputers and data centers suffer
    from power constraints, manifested in the problems of cooling the machines, whether
    by giant fans and air conditioners or by sites near rivers or glaciers. As Hölzle
    comments, “By classic definitions, there is little ‘work’ produced by a datacenter
    since most of the energy is converted to heat.”
  prefs: []
  type: TYPE_NORMAL
- en: Hitting the energy wall and the light-speed barrier, the chip’s architecture
    will necessarily fragment into separate modules and asynchronous and more parallel
    structures. We might term these processors time-space “mollusks”—Einstein’s word
    for entities in a relativistic world. Setting the size of the integrated circuit
    cell will be a measure comparable in the microcosm to light-years in the cosmos.
    It will enforce a distribution of computing capabilities analogous to the distribution
    of human intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, says Dally, harking back to Tredennick, leading-edge-wedge computer
    performance must now be measured not by the conventional metrics of operations
    per second or silicon area but by operations per watt. Based on the natural parallelism
    of images hitting the eye at once, graphics processors are not only as ubiquitous
    as vision but supremely parallel. Thus many of the “cool chips” of today tend
    to be made by Nvidia.
  prefs: []
  type: TYPE_NORMAL
- en: Still, in operations per watt, the prevailing champion is made not of silicon
    but of carbon. It is the original neural network, the human brain and its fourteen
    watts, which is not enough to illuminate the lightbulb over a character’s head
    in a cartoon strip. In the future, computers will pursue the energy ergonomics
    of brains rather than the megawattage of Big Blue or even the giant air-conditioned
    expanses of data centers. All computers will have to use the power-saving techniques
    that have been developed in the battery-powered smartphone industry and then move
    on to explore the energy economics of real carbon brains.
  prefs: []
  type: TYPE_NORMAL
- en: There is a critical difference between programmable machines and programmers.
    The machines are deterministic and the programmers are creative.
  prefs: []
  type: TYPE_NORMAL
- en: 'That means that the AI movement, far from replacing human brains, is going
    to find itself imitating them. The brain demonstrates the superiority of the edge
    over the core: It’s not agglomerated in a few air-conditioned nodes but dispersed
    far and wide, interconnected by myriad sensory and media channels. The test of
    the new global ganglia of computers and cables, worldwide Webs of glass and light
    and air, is how readily they take advantage of unexpected contributions from free
    human minds in all their creativity and diversity, which cannot even be measured
    by the metrics of computer science.'
  prefs: []
  type: TYPE_NORMAL
- en: As the Silicon Valley legend Carver Mead of Caltech has shown in his decades
    of experiments in neuro-morphic computation, any real artificial intelligence
    will likely have to use not silicon substrates but carbon-based materials. With
    some 200,000 compounds, carbon is more adaptable and chemically complex than silicon
    by orders of magnitude. Recent years have seen an efflorescence of new carbon
    materials, such as the organic light-emitting diodes and photodetectors now slowly
    taking over the display market. Most promising is graphene, a one-atom-deep sheet
    of transparent carbon that can be curled up in carbon nanotubes, layered in graphite
    blocks, or architected in C-60 “Buckyballs.”
  prefs: []
  type: TYPE_NORMAL
- en: Graphene has many advantages. Its tensile strength is sixty times that of steel,
    its conductivity two hundred times that of copper. There is no band gap to slow
    it down, and it provides a relatively huge sixty-micron mean-free path for electrons.
    As the nanotech virtuoso James Tour of Rice University has demonstrated in his
    laboratory, graphene, carbon nanotube swirls, and their compounds make an array
    of nano-machines, vehicles, and engines possible. They offer the still-remote
    promise of new computer architectures such as quantum computers that can actually
    model physical reality and thus may finally yield some real intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: The current generation in Silicon Valley has yet to come to terms with the findings
    of von Neumann and Gödel early in the last century or with the breakthroughs in
    information theory of Claude Shannon, Gregory Chaitin, Anton Kolmogorov, and John
    R. Pierce. In a series of powerful arguments, Chaitin, the inventor of algorithmic
    information theory, has translated Gödel into modern terms. When Silicon Valley’s
    AI theorists push the logic of their case to explosive extremes, they defy the
    most crucial findings of twentieth-century mathematics and computer science. All
    logical schemes are incomplete and depend on propositions that they cannot prove.
    Pushing any logical or mathematical argument to extremes—whether “renormalized”
    infinities or parallel universe multiplicities—scientists impel it off the cliffs
    of Gödelian incompleteness.
  prefs: []
  type: TYPE_NORMAL
- en: Chaitin’s “mathematics of creativity” suggests that in order to push the technology
    forward it will be necessary to transcend the deterministic mathematical logic
    that pervades existing computers. Anything deterministic prohibits the very surprises
    that define information and reflect real creation. Gödel dictates a mathematics
    of creativity.
  prefs: []
  type: TYPE_NORMAL
- en: This mathematics will first encounter a major obstacle in the stunning successes
    of the prevailing system of the world not only in Silicon Valley but also in finance.
  prefs: []
  type: TYPE_NORMAL
