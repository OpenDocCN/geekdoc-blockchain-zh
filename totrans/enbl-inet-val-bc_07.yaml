- en: Part VIIInternet of Value and Systemic Risk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2022N.
    Vadgama et al. (eds.)Enabling the Internet of ValueFuture of Business and Finance[https://doi.org/10.1007/978-3-030-78184-2_15](https://doi.org/10.1007/978-3-030-78184-2_15)
  prefs: []
  type: TYPE_NORMAL
- en: Structure, Robustness and Efficiency of Networked Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fabio Caccioli^([1](#Aff4) [ ](#ContactOfAuthor1))(1)University College London,
    London, UKFabio CaccioliEmail: [f.caccioli@ucl.ac.uk](mailto:f.caccioli@ucl.ac.uk)Fabio
    Caccioli'
  prefs: []
  type: TYPE_NORMAL
- en: is an associate professor at the Department of Computer Science at University
    College London. He has been a research associate in the Centre for Risk Studies
    (University of Cambridge) and a postdoctoral fellow at the Santa Fe Institute.
    He holds a Ph.D. in Statistical Physics from SISSA (Trieste, Italy). His research
    focuses on systemic risk, complex systems and network theory.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The development of blockchain and distributed ledger technologies (DLTs) aims
    to transform economic interactions by facilitating the method by which value is
    stored and transferred between individuals. This is achieved through the emergence
    of peer-to-peer systems, where users directly interact with one another rather
    than through intermediaries, thus reducing the cost associated with their transactions
    (Ibañez et al. [2021](#CR13)).
  prefs: []
  type: TYPE_NORMAL
- en: Users will contribute to the development and maintenance of such platforms,
    which should, therefore, emerge with no requirements for central planning, apart
    from some engagement rules that define the collective decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: Such engagement rules must provide the correct structure of incentives for the
    system to correctly develop as a distributed system where all users are equal.
    This is non-trivial, as networked systems that develop over time in a self-organised
    manner with no central planning tend to become more centralised and witness the
    emergence of special users on which interactions tend to concentrate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The emergence of special users occurs because of a simple self-reinforcing
    mechanism: if a user is more important than others, other users will tend to interact
    with that user, thus making it more important (Yule [1925](#CR20); Simon [1955](#CR18);
    Price [1976](#CR17); Barabási and Albert [1999](#CR2)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the perspective of the system as a whole, there are benefits and risks
    associated with the emergence of special users: their existence can facilitate
    the propagation of information and overall efficiency of the system. This, however,
    comes at the expenses of the increased fragility of the system to attacks targeted
    to the special users. This implies the existence of trade-offs between efficiency
    and robustness that must be accounted for when designing a networked system.'
  prefs: []
  type: TYPE_NORMAL
- en: The study of networks, a very active field of research in complexity science
    since the 90s, provides a useful insight concerning these mechanisms and trade-offs.
    This chapter reviews—in an idiosyncratic manner and without a claim to completeness—some
    lessons that have been learned in relation to the stability and efficiency of
    complex networks, and its attempts to derive some insights for Internet of Value
    (IoV) networks. The interested reader can refer to the following references for
    comprehensive reviews on complex networks (Albert and Barabási [2002](#CR1); Caldarelli
    [2007](#CR5); Barrat et al. [2008](#CR3); Dorogovtsev et al. [2008](#CR11); Newman
    [2010](#CR15)).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Random Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Networks are a mathematical abstraction used to represent systems composed of
    many units that interact in pairs. The mathematical representation of a network
    is given in terms of a set of nodes (representing the units) and links that connect
    nodes (representing interactions).
  prefs: []
  type: TYPE_NORMAL
- en: The typical question asked when studying a network concerns the relationship
    between the structure of the network and its function. We want to find general
    relationships that are independent of the fine-grained details of the network
    at hand, in order to draw conclusions that are valid for a class of networks rather
    than a specific realisation, which may, for instance, be affected by noise. To
    this end, different classes of random networks have been introduced in the literature
    (Albert and Barabási [2002](#CR1); Caldarelli [2007](#CR5); Barrat et al. [2008](#CR3);
    Dorogovtsev et al. [2008](#CR11); Newman [2010](#CR15)). For each class, some
    specific features are (statistically) fixed, the rest being as random as possible.
    This allows us to understand the effect on the system of those features that are
    being fixed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most famous class of random networks are Erdős–Rényi networks (Erdös and
    Rényi [1959](#CR12)). An Erdős–Rényi network with N nodes can be constructed by
    considering all possible pairs of nodes and drawing a link between each pair with
    probability p. The parameter p controls the level of connectivity of the network:
    each node will on average be connected to ![$$&lt;k&gt; = p(N - 1)$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq1.png)
    other nodes. The number of neighbours of a node is called its degree. A network’s
    degree distribution is the distribution of degrees across its nodes. We see in
    the following that the degree distribution strongly affects the property of random
    networks.'
  prefs: []
  type: TYPE_NORMAL
- en: In the limit of large networks (![$$N \gg 1$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq2.png)),
    the degree distribution of Erdős–Rényi random networks is a Poisson distribution
    with average ![$$&lt;k&gt;$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq3.png).
    For this type of distribution, the probability of observing a node with a degree
    much farther from the average degree decays quickly, so that Erdős–Rényi networks
    are typically used to describe systems where all nodes have similar connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also in a regular lattice (like for instance two-dimensional grid), nodes have
    similar connectivities (in fact they have the same degree). Erdős–Rényi networks
    are, however, quite different from a regular lattice because the average distance
    between nodes is much smaller: In a regular d-dimensional lattice, the average
    distance between nodes grows as ![$$N^d$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq4.png),
    whereas on Erdős–Rényi networks it grows only as ![$$\ln N$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq5.png).
    This means that Erdős–Rényi networks, at odds with regular lattices, are quite
    compact objects: it is possible to navigate between nodes quickly, and this remains
    true as we increase the size of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: Because random networks appear to be easy to navigate, and because all nodes
    of an Erdős–Rényi network have similar importance (when measured in terms of connectivity),
    we can consider Erdős–Rényi networks as prototypical examples of distributed networks.
  prefs: []
  type: TYPE_NORMAL
- en: Although very useful from a theoretical perspective, the analysis of many real
    networked systems (technological, biological, social) makes it clear that Erdős–Rényi
    networks are, however, not a good model to describe the connectivity of real networks.
    In fact, the latter tend to be characterised by heterogeneous distributions of
    degrees, with a few nodes acting as highly connected hubs among a majority of
    less connected nodes. There are many more differences between real networks and
    random networks; here we focus for simplicity on the degree distribution, as this
    can be linked to the centralisation of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration model is a way to generalise Erdős–Rényi networks to networks
    with any degree distribution. Rather than just fixing the average degree, as in
    Erdős–Rényi networks, we now fix the entire degree distribution *P*(*k*). A network
    of the configuration model can be generated as follows: First, we assign to each
    node *i* several half links ![$$k_i$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq6.png)
    drawn from the probability distribution *P*(*k*). Then, we randomly match half
    links to form links between pairs of nodes. In this way, it is possible to generate
    networks with any given degree of distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: To mimic the connectivity pattern of real networks, the class of scale-free
    random networks has, in particular, been considered in the literature (Albert
    and Barabási [2002](#CR1)). These networks are characterised by a power-law distribution
    of degrees ![$$P(k) \sim k^{-\gamma }$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq7.png).
    When the exponent ![$$\gamma $$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq8.png)
    satisfies ![$$2 &lt; \gamma \le 3$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq9.png),
    the variance of the degree distribution diverges, which makes it apparent that
    the fluctuations of degrees across nodes are large. The existence of hubs in scale-free
    networks facilitates the navigability of the network, as they can act as shortcuts
    to move from one node to another. In fact, the typical distance between nodes
    in such networks scales as ![$$\ln (\ln N)$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq10.png)
    (Cohen and Havlin [2003](#CR6)), even more slowly than for Erdős–Rényi networks.
  prefs: []
  type: TYPE_NORMAL
- en: Given the inequality in terms of nodes’ degrees that characterises scale-free
    networks, they are considered in the following as examples of effectively-centralised
    systems (a purely centralised system would be represented by a star network, with
    the provider of the service at the centre, and all users connected to it. In a
    scale-free network, there is no single central node, yet most of the activity
    is concentrated on a few nodes).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Large-Scale Connectivity of Random Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Intuitively, we would expect a networked system to be able to perform its function
    if it is “well connected”, that is if it is possible to reach a sizable fraction
    of nodes from any other node. This is, for instance, the case if the network is
    supposed to enable the transfer of information between its parts.
  prefs: []
  type: TYPE_NORMAL
- en: This intuition can be formalised in mathematical terms through the concept of
    a giant component. A component of a network is a set of nodes that are all connected
    between them, in the sense that it is possible to reach any node of the set starting
    from any other node in the set following a path of links. If we consider the largest
    component of a network, we can say that there is a giant component if the number
    of nodes ![$$N_{LC}$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq11.png)
    of the largest component scales with the size of the network *N* as ![$$N_{LC}
    \sim N$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq12.png).
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the mathematical details, the existence of a giant component signals
    the fact that a sizable fraction of nodes belongs to the same component, which
    we take as a proxy that the network can perform its function (for instance, it
    means that information could travel among a relatively large set of nodes). In
    the absence of a giant component, the network is just a collection of components
    that are very small compared to the total number of nodes and are disconnected
    between them. Therefore, the network is not well-formed, and it does not display
    any large-scale coherence.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a network generated through the configuration model, it is possible to
    derive the following condition for the emergence of a giant component (Molloy
    and Reed [1995](#CR14)):![$$\begin{aligned} \frac{\langle k(k-1)\rangle }{ \langle
    k\rangle }&gt;1\end{aligned}$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_Equ1.png)(1)where
    ![$$\langle \cdot \cdot \cdot \rangle $$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq13.png)
    denotes an average over the degree distribution of the network. The equation above
    can be specialised to the case of Erdős–Rényi and scale-free networks. For Erdős–Rényi
    networks, the condition reduces to ![$$\langle k \rangle &gt; 1$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq14.png),
    which means that if the average degree is larger than one, the network displays
    a giant component. Note that this implies that the giant component appears already
    for sparse networks (in a network of *N* nodes, there can be up to ![$$N (N -
    1)/2$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq15.png).
    A network is said to be dense if the number of links scales with the number of
    nodes *N* as ![$$O(N^2)$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq16.png):
    If the number of nodes doubles, the number of links quadruples. A sparse network
    is, instead, one for which the number of links scales instead as *O*(*N*): If
    the number of nodes doubles, the number of links also doubles).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For an Erdős–Rényi network there are then two regimes: If ![$$\langle k \rangle
    &lt; 1$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq17.png)
    there is no giant component, whereas if ![$$\langle k \rangle &gt; 1$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq18.png)
    there is a giant component and the network in the abstraction under consideration,
    can perform its function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a scale-free network with ![$$2 &lt; \gamma \le 3$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq19.png),
    the situation is quite different: the second moment of the degree distribution
    diverges, which implies that the condition for the existence of a giant component
    is always satisfied. The intuition behind this behaviour is that the presence
    of hubs facilitates the connectivity of the network, which is always able to perform
    its function.'
  prefs: []
  type: TYPE_NORMAL
- en: Given that we are taking Erdős–Rényi and scale-free random networks as toy models
    of decentralised and centralised systems, the lesson we gain from this analysis
    is that, whereas it may be easier for centralised systems to function, decentralised
    systems can also operate well in the sparse regime, provided a sufficient number
    of links is present. Thus, IoV system designers would need to account for these
    network properties.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Robustness of Random Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, this chapter has discussed what happens when pairs of nodes are randomly
    connected and asked the question of under what conditions a giant component emerges
    in the network. The opposite problem is now considered: given a network with a
    giant component, how many nodes should be removed for the giant component to disappear.
    This exercise is useful to understand how robust a network with respect to the
    failure of its nodes is.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer to this question depends on the protocol used to remove the network
    nodes. If nodes are removed randomly with uniform probability, scale-free networks
    are more robust than Erdős–Rényi networks. If nodes are removed starting with
    the most connected ones, Erdős–Rényi networks are more robust than scale-free
    ones. The reason for this behaviour—often referred to as robust-yet-fragile behaviour—is
    simple: whereas in Erdős–Rényi networks nodes have similar degrees, scale-free
    networks are characterised by the presence of few hubs—which facilitate the connectivity
    between different parts of the network—and many “poorly” connected nodes. In that
    configuration, to break the network, hubs must be removed. However, if nodes are
    removed randomly with uniform probability, it is not likely that the hubs will
    be removed, because they are few and, with higher probability, a poorly connected
    node will be selected for removal. However, if the removal protocol favours the
    removal of highly connected nodes, then by removing a few selected nodes (the
    hubs), it is very easy to break scale-free networks (Cohen et al. [2001](#CR7)).'
  prefs: []
  type: TYPE_NORMAL
- en: This intuition is extremely important for IoV systems. This confirms that a
    distributed network will perform worse than centralised ones in the case of random
    failures of nodes, whereas they will be better off in the case of targeted attacks
    aimed at breaking the system.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Spreading Processes of Random Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, the structural properties of networks have been considered. However,
    how does the structure of the network affect the outcome of dynamic processes
    that take place on the network? This question is useful to understand, for instance,
    how information spreads on networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, the degree of distribution of the network can strongly affect the outcome
    of dynamical processes taking place on networks. A typical example of this can
    be observed, for instance, in epidemic spreading models. Consider, for instance,
    the SIS model, where each node can be susceptible or infected. The dynamic is
    very simple: At rate ![$$\alpha $$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq20.png),
    each infected node infects its susceptible neighbours, whereas at rate ![$$\beta
    $$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq21.png), each
    infected node becomes susceptible (i.e. it recovers, but it does not become immune).
    In this spreading process, the question raised is of whether, in the stationary
    regime, the fraction of infected nodes can be larger than zero. By solving the
    model, it is possible to find a condition for this to occur (Pastor-Satorras and
    Vespignani [2001](#CR16))![$$\begin{aligned} \frac{\alpha \left\langle k^{2}\right\rangle
    }{\beta \langle k\rangle }&gt;1 \end{aligned}$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_Equ2.png)(2)This
    condition depends on the moments of the degree distribution, and it defines the
    so-called epidemic threshold, which discriminates between a regime where no infected
    nodes remain in the system and a regime where a finite fraction of nodes remain
    infected. The epidemic threshold is equal to ![$$\langle k \rangle / \langle k^2\rangle
    $$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq22.png) if
    ![$$\alpha / \beta &lt; \langle k \rangle /\langle k^2\rangle $$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq23.png)
    there is no epidemic, otherwise there is. The epidemic threshold crucially depends
    on the moments of the degree of distribution. In particular, we see that—at odds
    with the case of Erdős–Rényi networks where the two regimes exist—for a scale-free
    network, it goes to zero, meaning that a finite fraction of the population will
    always be infected as long as the transmission rate ![$$\alpha $$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq24.png)
    is larger than zero (Pastor-Satorras and Vespignani [2001](#CR16); Barrat et al.
    [2008](#CR3)). Again, this is due to the presence of hubs that facilitate the
    spreading of the infection.'
  prefs: []
  type: TYPE_NORMAL
- en: Epidemic spreading models cannot be directly used to model the propagation of
    information in networked systems, as the propagation mechanism is context dependent.
    For instance, uninformed individuals may require multiple exposures to informed
    individuals to acquire the information, or informed individuals may decide to
    stop spreading the information if enough of their neighbours are already informed
    (Daley and Kendall [1964](#CR8); Daley and Gani [1999](#CR9); Barrat et al. [2008](#CR3)).
    Therefore, results concerning the efficiency of different networks may depend
    on the specific mechanism at hand. For instance, in the case whereby informed
    nodes stop propagating information if enough of their neighbours are informed,
    the presence of hubs may reduce the capability of the information to reach the
    entire network, should these stop propagating the information.
  prefs: []
  type: TYPE_NORMAL
- en: A real case application of epidemic spreading models to distributed systems
    is the study of forks in the blockchain, which may impact IoV systems. In this
    case, nodes represent miners who validate transactions and spend computational
    power to form new blocks to be added to the blockchain. The mining process can
    be modelled as a Poisson process on each node, whereby a new block is discovered
    at a given rate. When a new block is found by a node, this is broadcasted at a
    given rate to its neighbours, who, in turn, broadcast the new block to their neighbours
    and start mining on the new block, and so on. This propagation process can be
    modelled as an SIS model with ![$$\beta = 0$$](../images/508554_1_En_15_Chapter/508554_1_En_15_Chapter_TeX_IEq25.png)
    (Decker et al. [2013](#CR10)), i.e. infected nodes (nodes that have been informed
    about the new block) never recover from the infection. A blockchain fork occurs
    whenever the new block does not reach the whole network before an alternative
    block is found by an uninformed node. The probability of a fork increases with
    the number of nodes in the network, which may lead to scalability problems.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, the efficiency of the system can be, for instance, measured
    in terms of the probability of reaching the majority of the network before a fork
    occurs. A comparison between Erdős–Rényi and scale-free networks shows that the
    latter scale better (Caccioli et al. [2016](#CR4)).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The comparison between Erdős–Rényi and scale-free networks discussed above
    provides some ideas concerning the robustness and efficiency of IoV networked
    systems. The main points that emerge are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In Erdős–Rényi random networks, nodes are homogeneous in terms of their degree,
    whereas in scale-free networks, nodes are unequal because links concentrate on
    a few hubs. We can use these models as abstract examples of distributed systems
    and effectively-centralised systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From a structural perspective, scale-free networks tend to be more efficient:
    They are more compact, and the presence of hubs make it easy to navigate through
    them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This increased efficiency comes at the expense of a high fragility to targeted
    attacks: if a few hubs are shut down, the system breaks and will fail to perform
    its function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More centralised systems, where special nodes serve as hubs, appear to be more
    efficient. This is, perhaps, the reason why many real networks that have no central
    planning (such as the Internet, cryptocurrency open networks or social networks)
    spontaneously evolved towards a structure that can be better approximated by scale-free
    networks than Erdős–Rényi networks. See Tasca ([2015](#CR19)) for the centralisation
    of the Bitcoin network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason for this tendency can intuitively be understood by the desire of
    nodes to establish connections with important nodes to facilitate their access
    to the system. This is at the basis of the celebrated Barabasi-Albert model of
    scale-free networks, which explains how a scale-free network may be the result
    of individual nodes’ decisions. The decision-making process they use in their
    model is simple: As new nodes come to the network, they need to connect to already
    existing nodes. The choice of their neighbours is random, but with a bias such
    that the probability of connecting to a node increases with the degree of that
    node. This preferential attachment, rich-get-richer rule leads to a self-reinforcing
    mechanism by which the more connected a node is, the newer connections it tends
    to attract. This eventually leads to the emergence of hubs in the system and consequently
    power-law distributions of degree. This implies the existence of special nodes
    on which most of the activity in the network concentrates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The same special nodes that improve efficiency, however, make the system vulnerable
    to attacks. Once the special nodes are identified, it is sufficient to focus on
    them to take the system down, so that the most efficient network structures are
    not necessarily the most robust to attacks. This represents a potential trade-off
    between efficiency and robustness to attacks, which is of paramount importance
    for IoV systems: the network topology which is desirable for efficiency gains
    does not generally match the one that is preferable in terms of robustness against
    attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2022N.
    Vadgama et al. (eds.)Enabling the Internet of ValueFuture of Business and Finance[https://doi.org/10.1007/978-3-030-78184-2_16](https://doi.org/10.1007/978-3-030-78184-2_16)
  prefs: []
  type: TYPE_NORMAL
- en: Potential Sources of Internet of Value Systemic Risk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Josep Lluis de la Rosa Esteva^([1](#Aff4) [ ](#ContactOfAuthor1))(1)Girona,
    SpainJosep Lluis de la Rosa EstevaEmail: [peplluis@eia.udg.edu](mailto:peplluis@eia.udg.edu)Josep
    Lluis de la Rosa Esteva'
  prefs: []
  type: TYPE_NORMAL
- en: focuses on research on DLT and its applications to intelligent agents, virtual
    currencies and digital preservation. Notable blockchain projects he has been associated
    with are licens3d.com, venushina.org, welicense.io and reciclos.cat. He is also
    a professor at the University of Girona, a visiting professor at ETH Zurich and
    a research associate of the UCL CBT.
  prefs: []
  type: TYPE_NORMAL
- en: With the continuous development of the IoV, the lack of interoperability between
    networks of trust can become an important source of systemic risk.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the main problem that the blockchain technology faces at the moment
    for the sake of the IoV is the lack of interoperability and the obstacles faced
    in building a second and third layer network.
  prefs: []
  type: TYPE_NORMAL
- en: There are thousands of distributed ledgers out there today, and they perform
    or improve several functions. Each ledger is working separately like a local area
    network (LAN) of the 1970s that did not communicate with others around it; closed
    off because there is no adoption as yet of industry-wide standardisation of protocols.
    If they could all interoperate, then moving several digital assets and converting
    from one to another could be fast, cheap and more secure, especially as there
    would no longer be a need to rely on risky centralised exchanges.
  prefs: []
  type: TYPE_NORMAL
- en: Even if the current financial system started adopting distributed ledgers on
    a large scale, with each institution using its own private or public ledger, not
    all of the challenges faced by today’s siloed systems would be solvable. Paradoxically,
    in this scenario, the situation with cross-border and inter-bank transactions
    might remain the same, as it would require time-consuming processes for value
    to move from one ledger to another.^([1](#Fn1))
  prefs: []
  type: TYPE_NORMAL
- en: Thus, interoperability is a must, and its lack is a potential source of systemic
    risk. This is of paramount importance, especially in the current expansion phase
    where the IoV’s new business models come up to erase long-standing barriers and
    enable the democratisation of finance and property by empowering its users to
    transact instantly not only across borders, but also across currencies while closing
    cultural and socioeconomic gaps.
  prefs: []
  type: TYPE_NORMAL
- en: The reasons for the lack of interoperability between ledgers are many; not only
    the lack of agreed standard protocols to make it possible, but the fact that every
    ledger has different goals when it comes to how data and the digital assets are
    handled. They are also built using different distributed ledger technologies,
    languages, protocols and consensus mechanisms. For more insights about the blockchain
    interoperability issue, we refer the readers to Tasca and Piselli ([2019](#CR8)).
  prefs: []
  type: TYPE_NORMAL
- en: The lack of interoperability—driven by the diversity of missions and technological
    proposals for ledgers—is not the only source of systemic risk envisaged here.
    In addition, the lack of scalability of these new network technologies could worsen
    the risks of assets and value, in general, being lost in endless sidechains.
  prefs: []
  type: TYPE_NORMAL
- en: Sidechains, state channels and payment channels are off-chain solutions to specialise
    the validation of transactions in subsets of nodes. An off-chain transaction is
    the movement of value outside of the main ledger, despite being very likely registered
    in a local or minor ledger validated by subsets of nodes from the main net, which,
    in turn, tend to be trusted parties among each other and work out in a permissioned
    minor ledger (Back et al. [2014](#CR1)).
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, only a “summary” of this minor ledger is uploaded on the main
    ledger in any form that is eventually validated by the main net. Risk is again
    at the core of this specialisation. If this is the technology which will be used
    for a hyperconnected IoV, we may expect that from time to time, tokens might get
    trapped in these new siloes or take the wrong direction in the network, and therefore,
    their value might fade according to their distance to the main net and the number
    of summary hops required to get the greatest level of confirmation.
  prefs: []
  type: TYPE_NORMAL
- en: To summarise, considering the current growing trend of open communities along
    with all types of permissioned ledgers, and the developments of side-channels
    for speeding up consensus for those who transact at high volume or demand higher
    privacy or speed, the lack of scalability and interoperability are major risks.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the network structure also plays an important role, especially if
    we consider networks of networks—see coloured coin applications (Bitcoin Wiki
    [2020](#CR2)) for example. This source of systemic risk is of paramount importance,
    as numerous new initiatives are proposing services backed by collateral in whatever
    asset that is tokenised (i.e. lending, factoring, insurance, guarantees of service,
    creative industries and more). The trend of collateralising the risk of digital
    currencies or any of the new digital assets may lead to systemic risk.
  prefs: []
  type: TYPE_NORMAL
- en: The crypto-collateralised stablecoins, conceived as a workable, truly decentralised
    approach to stabilise prices, they are not that stable (Perez et al. [2021](#CR6)).
    The same might occur with crypto-collateralised loans, which are paid by smart
    contracts collateralised by cryptocurrencies (see Chapter “[From Banks to DeFi:​
    The Evolution of the Lending Market](508554_1_En_6_Chapter.xhtml)”). For example,
    *“when you want to take a Dai loan from MakerDAO, you freeze some Ether in MakerDAO
    as a guarantee that you will repay your loan. As the US mortgage market collapse
    of the 2000s has shown, Collateral is an unstable asset whose price may decrease
    significantly”* (Heydari [2018](#CR5)). Under these assumptions, the stablecoins’
    smart contracts might demand several times more collateral than the original value
    of the loan.
  prefs: []
  type: TYPE_NORMAL
- en: The phenomena of the IoV presents an interesting avenue to the application of
    tokenisation to new business models that may lead to systemic risk. The IoV facilitates
    value moving as quickly and as easily as information does. As value is something
    that is up to a society to determine, there is practically no limit to what can
    be exchanged over the Internet with value for individuals or institutions. According
    to Heydari ([2018](#CR5)), already in 2018, cryptocurrencies had attracted 5%
    of gold market customers to reach a USD 400 billion market size, and since then,
    stablecoins with all type of collateral, and above all with Bitcoin, have greatly
    grown, with a peek in mid-2020.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, another source of systemic risk may come precisely from the nature
    of digital assets used as collateral. Tokenisation and collateralisation might
    create new types of much more complex asset interdependencies, and the collapse
    of even a small asset used as collateral could trigger the domino effect of a
    failure cascade.
  prefs: []
  type: TYPE_NORMAL
- en: The value of some collateral might fade by their lack of usage, trust, lost
    in a minor ledger, and indeed because they are poorly preserved. Finally, the
    preservation of collateral value over time must be considered. The preservation
    of value tackles the problem of value loss through time and exchanges (de la Rosa
    [2020](#CR3)). Through time, the accumulation of errors that occur when updates
    and migration into new technologies take place tend to erode the usability and
    integrity of digital assets. Thus, their value may reflect this situation and
    suffer a devaluation, as they would not be ready for any value transaction as
    the receiver will not receive the assets at their full integrity in the form and
    time that is required. Similarly, the accumulation of errors in the transmission
    of value to inappropriate receivers impacts the value that goes on erosion. The
    two factors, obsolescence and bad exchanges multiply, accelerating the decrease
    in the value of digital assets.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the development of techniques that look after the curation and integrity
    of digital assets along with their proper ownership management across all exchanges
    is all about the preservation of value, as an ultimate safeguard to avoid systemic
    risks triggered by a failure to preserve the value of digital assets.
  prefs: []
  type: TYPE_NORMAL
- en: The value preservation will be enormous as I also foresee massive migration
    of value onto DLT, a sort of Value Deluge, that requires fine value preservation
    of digital assets to avoid Value Blackouts which would amplify systemic risks.
    A Value Blackout would harm collateral and undermine the long term storage of
    value of digital assets.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, there has been a discussion of some potential sources of systemic
    risk to the emergence of the IoV. However, it is difficult if not impossible to
    be exhaustive regarding all possible sources. Indeed, the recent 2007–2008 financial
    crisis showed that with increasing connectedness comes increasing complexity,
    which is understood as greater interdependence. Any increasingly complex system
    is also characterised by higher unpredictability and speed, and presents emergent
    properties not observable at the micro level, leading to higher fragility. For
    further discussion on economic complexity, please see Sahdev ([2016](#CR7)).
  prefs: []
  type: TYPE_NORMAL
