- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2021R.
    Kumar et al. (eds.)Internet of Things, Artificial Intelligence and Blockchain
    Technology[https://doi.org/10.1007/978-3-030-74150-1_12](https://doi.org/10.1007/978-3-030-74150-1_12)
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者，独家许可给 Springer Nature Switzerland AG 2021R. Kumar 等，互联网、人工智能和区块链技术[https://doi.org/10.1007/978-3-030-74150-1_12](https://doi.org/10.1007/978-3-030-74150-1_12)
- en: '12. Bi-GRU Model with Stacked Embedding for Sentiment Analysis: A Case Study'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12. 双向 GRU 模型与堆叠嵌入用于情感分析：一个案例研究
- en: Sanjana Kavatagi^([1](#Aff5)) and Vinayak Adimule^([2](#Aff6)  )(1)Department
    of Computer Science and Engineering, Angadi Institute of Technology and Management
    (AITM), Belagavi, Karnataka, India(2)Dean R&D, Angadi Institute of Technology
    and Management (AITM), Belagavi, Karnataka, IndiaKeywordsMachine learningSentiment
    analysisLSTMGRUCNNPolarity-based techniqueEmbeddingProf. Sanjana M. Kavatagi
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Sanjana Kavatagi^([1](#Aff5)) 和 Vinayak Adimule^([2](#Aff6)  )(1)卡纳塔克邦贝拉加维的
    Angadi 技术与管理学院计算机科学与工程系，印度(2)卡纳塔克邦贝拉加维的 Angadi 技术与管理学院研究与开发院院长，印度关键词机器学习情感分析LSTMGRUCNN基于极性的技术嵌入式Prof.
    Sanjana M. Kavatagi
- en: received her bachelor’s degree and master’s degree from Visvesvaraya Technological
    University, Belagavi, Karnataka. She is currently working as an Assistant Professor
    in the Department of Computer Science and Engineering, Angadi Institute of Technology
    and Management, Belagavi. She has published two papers in national and international
    journals. Her research interests are artificial intelligence, machine learning,
    and big data. ![../images/504166_1_En_12_Chapter/504166_1_En_12_Figa_HTML.jpg](../images/504166_1_En_12_Chapter/504166_1_En_12_Figa_HTML.jpg)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 她在卡纳塔克邦贝拉加维的维斯韦沙拉雅工业大学获得学士学位和硕士学位。她目前在贝拉加维的 Angadi 技术与管理学院计算机科学与工程系担任助理教授。她在国内外期刊上发表了两篇论文。她的研究兴趣包括人工智能、机器学习和大数据。![../images/504166_1_En_12_Chapter/504166_1_En_12_Figa_HTML.jpg](../images/504166_1_En_12_Chapter/504166_1_En_12_Figa_HTML.jpg)
- en: Prof. Dr. Vinayak Adimule
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Prof. Dr. Vinayak Adimule
- en: has 13 years of research experience as a Senior Scientist and an Associate Research
    Scientist in R&D organizations of TATA (Advinus), Astra Zeneca India, Trans. Chem.
    Ltd. He is expert in the area of medicinal chemistry (anticancer drugs), nanoscience
    and technology, material chemistry, and biochemistry. He published more than 30
    research articles and book chapters in Scopus and Google Scholar (SJR)-indexed
    journals; attended and presented papers in national and international conferences;
    received best oral/poster awards; published few books in Notion Press International
    Inc.; chaired few international and national conferences symposia related to material
    electronics. He is an Editorial Board Member, Life Member, and Associate Member
    of many international societies¸ research institutions. He is recognized as Research
    Guide VTU, Belagavi. Presently, he is guiding one research scholar for his PhD
    degree. His research interests include nanoelectronics, material chemistry, sensors
    and actuators, bionanomaterials, and medicinal chemistry.![../images/504166_1_En_12_Chapter/504166_1_En_12_Figb_HTML.jpg](../images/504166_1_En_12_Chapter/504166_1_En_12_Figb_HTML.jpg)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有13年的研究经验，曾在TATA（Advinus）、Astra Zeneca India、Trans. Chem. Ltd等研发组织担任高级科学家和副研究科学家。他在药物化学（抗癌药物）、纳米科学与技术、材料化学和生物化学领域是专家。他在Scopus和Google
    Scholar（SJR）索引期刊上发表了30多篇研究文章和书籍章节；参加并在国内外会议上发表论文；获得最佳口头/海报奖；在Notion Press International
    Inc.出版了几本书籍；主持了几次与材料电子相关的国际和国内会议研讨会。他是许多国际学会、研究机构的编辑委员会成员、终身会员和副会员。他被认可为VTU贝拉格维的研究指导教师。目前，他正在指导一位研究生攻读博士学位。他的研究兴趣包括纳米电子学、材料化学、传感器和执行器、生物纳米材料和药物化学。![../images/504166_1_En_12_Chapter/504166_1_En_12_Figb_HTML.jpg](../images/504166_1_En_12_Chapter/504166_1_En_12_Figb_HTML.jpg)
- en: 12.1 Introduction
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 引言
- en: With the growth of Internet in today’s world, the data is getting generated
    exponentially. e-Commerce is one such area where people make most use of it. The
    e-commerce applications like Flipkart, Amazon, Snapdeal, Grofers, big basket,
    and others are the highly used platforms for online shopping. Due to the exponential
    development and popularization of e-commerce technologies, everyone loves shopping
    on numerous e-commerce pages (Liang & Wang, [2019](#CR26)).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 随着当今世界互联网的增长，数据呈指数级增长。电子商务是一个人们最常使用的领域之一。像Flipkart、Amazon、Snapdeal、Grofers、big
    basket等电子商务应用程序是在线购物的高度使用平台。由于电子商务技术的指数级发展和普及，每个人都喜欢在众多电子商务页面上购物（Liang & Wang，[2019](#CR26)）。
- en: Consumers have been increasingly keen to share their opinions and feelings on
    the Internet with the rise of online shopping. Extraction of emotions from the
    users in online reviews is of great importance, which not only attracts potential
    users by helping them make purchase decisions but also allows organizations to
    gain feedback on the product. On goods or services, customers may share their
    views, opinions, and user experiences. They may also provide other people with
    clear suggestions (Chen et al., [2018a](#CR5), [b](#CR6)). Nowadays, reviews given
    online by the customers are considered to be the best and successful sources for
    supporting the online buying decisions of prospective customers on goods or services
    (Gao et al., [2017](#CR11); Gavilan et al., [2018](#CR12)). In Natural Language
    Processing (NLP), sentiment analysis plays a critical role in collecting and processing
    relevant information from user feedback shared through online communities and
    collaborative media. From an applicative point of view, the viewed data is becoming
    increasingly relevant, solely in intelligent science and business systems to overcome
    and enhance the quality of their services and products (La’ercio Dias, [2018](#CR25);
    Xing et al., [2018](#CR38)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网络购物的兴起，消费者越来越热衷于在互联网上分享他们的意见和感受。从在线评论中提取用户的情感至关重要，这不仅可以通过帮助他们做出购买决策吸引潜在用户，还可以让组织获得产品的反馈。在商品或服务上，客户可能会分享他们的观点、意见和用户体验。他们还可能向其他人提供明确的建议（陈等人，[2018a](#CR5)，[b](#CR6)）。如今，顾客在线给出的评论被认为是支持潜在客户对商品或服务的在线购买决策的最佳和成功的来源（高等人，[2017](#CR11)；加维兰等人，[2018](#CR12)）。在自然语言处理（NLP）中，情感分析在从在线社区和协作媒体中收集和处理用户反馈相关信息方面起着至关重要的作用。从应用角度来看，观察到的数据在智能科学和商业系统中变得越来越重要，以克服并提高其服务和产品的质量（拉西奥·迪亚斯，[2018](#CR25)；邢等人，[2018](#CR38)）。
- en: Usually, the data of online reviews is complex and comprehensive, and it also
    contains multiple characteristics of a product or service being reviewed (also
    referred to as aspects of literature) (Zhang et al., [2010](#CR43)). The words
    in the review can be interpreted as either positive or negative. The words describing
    the quality of products as “good” can be considered as a positive perception on
    the product, whereas the word “high” used in expressing the cost can be considered
    to give some negative perception on the products.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在线评论的数据是复杂而全面的，它还包含了对被评论产品或服务的多个特征（也称为文学方面）（张等人，[2010](#CR43)）。评论中的词语可以被解释为积极或消极。描述产品质量为“好”的词语可以被认为是对产品的积极看法，而用于表达成本的“高”这个词则可能被认为对产品有些消极看法。
- en: Every statement in the review will assume some knowledge of positive perception,
    negative perception, or some neutral perception altogether because of the nuances
    and inherent fuzziness that is inherent in preferences and decisions of human
    beings. So, the assessment of such a broad range of online ratings is challenging.
    If we just take into account the general product sentiment values and disregard
    the fuzziness and randomness implicit in online feedback of different features,
    valuable details can be overlooked or distorted.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在评论中的每个陈述都会假设对正面感知、负面感知或某种中立感知有一定了解，因为人类偏好和决策中固有的微妙和模糊性。因此，评估如此广泛的在线评级是具有挑战性的。如果我们只考虑一般产品情感价值，并忽视在线反馈中不同特征的模糊性和随机性，就会忽视或扭曲宝贵的细节。
- en: Till date, a huge research has been conducted on sentiment analysis. A lot of
    tools and algorithms on sentiment analysis have been implemented by the researchers.
    Sentiment analysis research can not only be carried out on consumer review results,
    but it can also be conducted on ads, data from social media, etc. The reviews
    that are given by the users can have sarcasm, and some reviews may be irrelevant
    to the product, which means customer relationships, product distribution, etc.
    may be the reviews that need to be considered as important ones.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 至今，对情感分析进行了大量研究。研究人员已经实施了许多关于情感分析的工具和算法。情感分析研究不仅可以针对消费者评论结果进行，还可以针对广告、社交媒体数据等进行。用户提供的评论可能含有讽刺，一些评论可能与产品无关，这意味着客户关系、产品分销等可能是需要考虑的重要评论。
- en: We have focused on the study of sentiment at the aspect level (Kajal & Vandana,
    [2017](#CR21)) initially, whether the argument is true or speculative should be
    decided. Sentence-level SA has to decide if the sentence is subjective, and whether
    the sentence represents positive or negative views (Liu, [2012](#CR27)) claimed
    that representations of feeling do not always have an arbitrary essence. However,
    there is no fundamental difference between the classification of the text and
    the classification of the sentence level, as the sentences are only brief documents
    (Yu Liang-Chih et al., [2013](#CR41)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初集中研究了**Kajal & Vandana**（[2017](#CR21)）提出的方面级情感，首先需要确定论点是否真实或是推测性的。句子级 SA
    必须确定句子是否主观，以及句子是否代表积极或消极观点。**Liu**（[2012](#CR27)）声称，情感的表达并不总是具有任意性质。然而，文本分类与句子级分类之间并没有根本的区别，因为句子只是简短的文档。**Yu
    Liang-Chih** 等人（[2013](#CR41)）。
- en: At the aspect level, it seeks to classify the feelings expressed in terms of
    words in the review by people. For different features of the same product, each
    individual may have a different opinion. For example, the statement “the display
    quality of the phone is good but the battery is not long-lasting.” The first two
    forms of SA are covered by this survey.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在方面级别上，它试图根据人们在评论中用词表达的情感对其进行分类。对于同一产品的不同特征，每个人可能有不同的看法。例如，表述“手机的显示质量很好但电池不耐用。”
    这项调查涵盖了前两种情感分析形式。
- en: In addition to product ratings, SA applies to capital markets (Hagenau et al.,
    [2013](#CR14); Xu et al., [2012](#CR39)), news stories (Kim et al., [2018](#CR24)),
    and parliamentary discussions (Graham, [2009](#CR9)). The desire to learn, mine,
    and analyze this knowledge has increased dramatically due to the massive data
    evolution and the amount of data being exchanged and generated every second. And
    since standard machine learning techniques and neural networks were not enough
    for this big data to be collected, deep learning was the key to the era of big
    data (Chitkara et al., [2019](#CR7)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 除了产品评级外，情感分析也适用于资本市场（Hagenau等，[2013](#CR14)）；新闻报道（Kim等，[2018](#CR24)）；以及议会讨论（Graham，[2009](#CR9)）。由于海量数据的发展以及每秒交换和生成的数据量的增加，人们对学习、挖掘和分析这些知识的愿望急剧增加。由于标准机器学习技术和神经网络不足以收集这些大数据，深度学习成为大数据时代的关键（Chitkara等，[2019](#CR7)）。
- en: Deep learning is a discipline of machine learning and an alternation of neural
    networks. In other words, in addition to hidden layers in between, a standard
    neural network is a single network with an input and output layer, where computation
    is performed, whereas deep neural networks consist essentially of multiple neural
    networks, where one network’s output is an input to the next network and so on.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一门学科，也是神经网络的一种变体。换句话说，在隐藏层之间，标准神经网络是一个具有输入和输出层的单个网络，在其中进行计算，而深度神经网络基本上由多个神经网络组成，其中一个网络的输出是下一个网络的输入，依此类推。
- en: Deep learning networks learn the characteristics on their own, i.e., it has
    become apparent as a robust technique of machine learning that learns multiple
    layers of data characteristics and induces prediction performance. In various
    applications in the field of signal and information processing, deep learning
    has recently been used, especially with the evolution of big data (Zeng et al.,
    [2018](#CR42)). Furthermore, in sentiment analysis and opinion mining, deep learning
    networks have been used. In recent years, in many fields, it has created great
    achievements. Deep learning doesn’t require human intervention features compared
    to conventional machine learning approaches; big data is needed as support for
    deep learning, however. Deep learning approaches automatically derive characteristics
    from various neural network techniques and improve with their own mistakes (Yang
    et al., [2019](#CR40)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习网络能够自行学习特征，即已经成为一种强大的机器学习技术，能够学习多层数据特征并诱导出预测性能。在信号和信息处理领域的各种应用中，近年来深度学习已经被广泛应用，特别是随着大数据的发展（Zeng等人，[2018](#CR42)）。此外，在情感分析和观点挖掘中，深度学习网络已经被使用。近年来，在许多领域取得了巨大成就。与传统的机器学习方法相比，深度学习不需要人工干预特征；但是，深度学习需要大数据作为支持。深度学习方法可以自动从各种神经网络技术中提取特征，并通过自己的错误改进（Yang等人，[2019](#CR40)）。
- en: In this chapter we have discussed polarity-based model, gated recurrent unit
    (GRU) combined with convolution neural network (CNN), and long and short-term
    memory (LSTM) model. We have developed a novel model using stacked embedding to
    perform sentiment analysis on Amazon product reviews which improves the accuracy
    compared to the previous models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了基于极性的模型，将门循环单元（GRU）与卷积神经网络（CNN）相结合，以及长短期记忆（LSTM）模型。我们开发了一个新颖的模型，使用堆叠嵌入来对亚马逊产品评论进行情感分析，与先前的模型相比，提高了准确性。
- en: In Sect. [12.2](#Sec2) we have discussed literature review. Then we have explained
    various methods used for sentiment analysis in Sect. [12.3](#Sec6). Then in Sect.
    [12.4](#Sec11), we have introduced our novel model developed for Amazon product
    reviews. Results are discussed in Sect. [12.5](#Sec15) followed by summarization
    of the chapter and future scope in the last section.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[12.2](#Sec2)节中，我们讨论了文献综述。然后我们在第[12.3](#Sec6)节中解释了用于情感分析的各种方法。然后在第[12.4](#Sec11)节中，我们介绍了我们为亚马逊产品评论开发的新模型。结果在第[12.5](#Sec15)节中讨论，随后在最后一节总结了本章和未来的研究方向。
- en: 12.2 Literature Review
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 文献综述
- en: The study of sentiment analysis is a field that deals with the analysis and
    classification of the opinions, thoughts, and sentiment of the customer. At present,
    the research on techniques for sentiment analysis is grouped into three categories,
    namely, machine learning-based sentiment analysis technique, sentiment analysis
    based on lexicon techniques (Liu et al., [2017](#CR28); Zhou et al., [2017](#CR45)),
    and it is also based on some hybrid models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning strategies succeed in the optimization of parameters of a system
    which has a vast volume of training data. Positive and negative word sets are
    used in lexicon-based approaches in order to differentiate feelings, while mixed
    versions make use of both models in combination. In recent studies, approaches
    to machine learning involving supervised learning models (Agarwal et al., [2015](#CR2);
    Xia et al., [2014](#CR37)), semi-supervised learning models  , and unsupervised
    learning models (Guo et al., [2017](#CR13)) have been studied. The focus of this
    is the development of sentiment dictionaries (Loughran & Mcdonald, [2011](#CR30))
    by manual or automatic processes. Extant literature on lexicon-based sentiment
    analysis approaches mostly contains dictionary-based and corpus-based categories.
    While this one is used to identify patterns of the syntax and develop a list of
    words of opinion. Performance analysis of sentiments in Twitter dataset using
    SVM models was analyzed by (Ramasamy et al. [2021a](#CR33)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: In this review, we use the latter to distinguish sentiment of the product as
    either positive, neutral, or negative relevant to each of the features, provided
    that machine learning techniques face many limitations and portability and robustness
    benefits are shown by dictionary-based sentiment analysis techniques across different
    domains.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Sentiment Lexicon-Based Sentiment Analysis
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jurek et al. ([2015](#CR20)) showed that the words  in the dictionary with the
    indication of polarity and intensity of sentiment are used in this method to derive
    sentiment from the document. A lexicon-based sentiment analysis algorithm was
    proposed by (Ashhar et al. [2017](#CR3)), using evidence-based mixture features
    and sentiment normalization techniques. Ashhar et al. To analyze emotion processing
    of online user feedback, in addition to using terms for emotion. Some modifiers
    that are domain specific terms are added. A DSEL-based unigram mixture model (UMM)
    was suggested by (Bandhakavi et al. [2017](#CR4)). Emotion classification is done
    to extract effective features with the use of labeled and coarsely labeled emotion
    text. Dhaoui et al. ([2017](#CR8)) used the machine learning package of LIWC2015
    lexicon, and RTextTools are used to evaluate the process of the study of sentiments
    based on lexicon and machine learning (Ramasamy et al., [2021b](#CR34)). Selection
    of optimal hyper-parameter values of support vector machine for sentiment analysis
    tasks uses nature-inspired optimization methods. Khoo and Johnkhan ([2018](#CR23))
    introduced a common sentiment lexicon which is named the WKWSCI sentiment lexicon
    and is evaluated by comparing to the present sentiment lexicons (Zhang et al.,
    [2018](#CR44)). To test Chinese microblog text sentiment, used different sentiment
    lexicons. Keshavarz and Abadeh ([2017](#CR22)) produced an adaptive sentiment
    vocabulary to increase Weibo’s accuracy in the classification of sentiment by
    using a mixture of corpus and lexicons. A two-layer graph model was developed
    by (Feng et al., [2015](#CR10)), built using different sentiment terms of emoji
    and terms of candidate sentiment, and picked the popular terms present in the
    model as phrases of sentiment. Therefore, machine learning-based solution that
    impulsively generates sentiment features has become a safer choice for researchers
    with a little manual involvement  .
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Jurek等人（[2015](#CR20)）表明，字典中具有极性和情感强度指示的词语在这种方法中被用来从文档中提取情感。基于词典的情感分析算法是由（Ashhar等人，[2017](#CR3)）提出的，使用基于证据的混合特征和情感归一化技术。Ashhar等人。除了使用情感词汇之外，为了分析在线用户反馈的情感处理，还添加了一些领域特定的修饰词。（Bandhakavi等人，[2017](#CR4)）建议使用基于DSEL的单词混合模型（UMM）。情感分类是通过使用标记和粗略标记的情感文本来提取有效特征来完成的。Dhaoui等人（[2017](#CR8)）使用了LIWC2015词典的机器学习包，以及RTextTools来评估基于词典和机器学习的情感研究过程（Ramasamy等人，[2021b](#CR34)）。为了情感分析任务选择最优的支持向量机超参数值，使用了自然启发式优化方法。Khoo和Johnkhan（[2018](#CR23)）引入了一个通用情感词典，名为WKWSCI情感词典，并通过与现有情感词典（Zhang等人，[2018](#CR44)）进行比较来进行评估。为了测试中文微博文本的情感，使用了不同的情感词典。Keshavarz和Abadeh（[2017](#CR22)）通过使用语料库和词典的混合来产生自适应情感词汇，以提高微博情感分类的准确性。（Feng等人，[2015](#CR10)）开发了一个两层图模型，使用了不同的表情符号和候选情感术语，并从模型中选择了流行的术语作为情感短语。因此，基于机器学习的会自动生成情感特征的解决方案已成为研究人员的更安全选择，几乎没有人工参与。
- en: 12.2.2 Machine Learning-Based Sentiment Analysis
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.2 基于机器学习的情感分析
- en: By using the gini index and the classification  of the support vector machine
    (SVM), a function extraction method was proposed by (Manek et al. [2017](#CR32))).
    A novel probabilistic supervised joint emotion model (SJSM) was suggested by (Hai
    et al., [2017](#CR15)) which accepts semantic emotions from the data of comments
    given by users but also hypothesize the general sentiment of the data in comments.
    A theme model developed in joint with a multi-model for sentiment analysis was
    suggested by (Singh et al., [2017](#CR35)). With the focus on the personality
    traits of the user and words with effect of sentiment, this model used latent
    dirichlet allocation (LDA) to determine the sentiment of the user written in words.
    Huq et al. [2017](#CR18) studied various algorithums to evaluate twitter data
    sentiment. Long et al. ([2018](#CR29)) used SVM to use additional samples containing
    previous information to identify stock forum messages. While features can be extracted
    automatically by the machine learning-based process, it also relies on selection
    of features manually. The strategy based on deep learning doesn’t need manual
    intervention. Through the neural network framework, it can pick and extract features
    automatically, and it can master from its own mistakes  .
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用基尼指数和支持向量机（SVM）的分类，(Manek 等人 [2017](#CR32)) 提出了一种函数提取方法。一种新颖的概率监督联合情感模型（SJSM）由
    (Hai 等人，[2017](#CR15)) 提出，它接受用户评论数据中的语义情感，但也假设评论数据的总体情感。由 (Singh 等人，[2017](#CR35))
    提出了一个与情感分析的多模型联合开发的主题模型。该模型聚焦于用户的个性特征和具有情感影响的单词，使用隐狄利克雷分配（LDA）来确定用户在文字中表达的情感。Huq
    等人 [2017](#CR18) 研究了评估 Twitter 数据情感的各种算法。Long 等人（[2018](#CR29)）使用 SVM 利用包含先前信息的额外样本来识别股票论坛消息。虽然机器学习过程可以自动提取特征，但也依赖于手动选择特征。基于深度学习的策略不需要手动干预。通过神经网络框架，它可以自动选择并提取特征，并且可以从自己的错误中学习。
- en: 12.2.3 Deep Learning-Based Sentiment Analysis
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.3 基于深度学习的情感分析
- en: A qualitative analysis  of the characteristics of semantics and some co-occurrence
    characteristics of statistics in the words of tweet and method reference for the
    n-gram convolution neural network to test the emotion polarity was used by (Jianqiang
    et al., [2018](#CR19)). Ma et al. ([2018](#CR31)) developed a model using the
    target-dependent convolution neural networks in which the words surrounds the
    target word that influence it and the distance between the intended word and the
    words surrounding it. If we focus on the process, every word in the sentence will
    give some emotional impact on deciding the polarity of the sentence. A model which
    combines some of the dominant features and some of the features which are not
    dominant was developed by Ma et al. using the LSTM model. This requires a memory  at
    the token level to be injected at the output gate separately. A model based on
    the theory of mathematics for the regression network was proposed by (Chen et
    al. [2018a](#CR5), [b](#CR6)). A combined framework to incorporate CNN and RNN
    was developed by Abid et al., ([2014](#CR1)). Opinion of the customer on the products
    they buy can be identified with the reviews they give for it, and this can be
    achieved by using sentiment analysis. With the help of the above opinion analysis,
    we can take advantages of this and can build some policies and directives that
    can be incorporated for taking better decisions at the business.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在推特词汇中对语义特征和一些共现特征的定性分析，以及测试情感极性的 n-gram 卷积神经网络的方法参考（Jianqiang 等人，[2018](#CR19)）使用了统计特性。Ma
    等人（[2018](#CR31)）开发了一个模型，使用了目标相关的卷积神经网络，在这个模型中，词汇环绕着目标词，影响着它，以及目标词与其周围词汇之间的距离。如果我们关注过程，句子中的每个词都会对决定句子的极性产生一些情感影响。Ma
    等人使用 LSTM 模型开发了一个模型，结合了一些主导特征和一些非主导特征。这需要在输出门处分别注入标记级别的内存。基于回归网络数学理论提出了一种模型（Chen
    等人 [2018a](#CR5), [b](#CR6)）。Abid 等人开发了一个结合了 CNN 和 RNN 的综合框架（[2014](#CR1)）。顾客对他们购买的产品的意见可以通过他们对产品的评论来识别，这可以通过使用情感分析来实现。借助以上观点分析，我们可以利用这一点，制定一些可以纳入业务决策的政策和指导方针。
- en: Liu et al. defined sentiment strength  by degree and punctuation adverbs and
    created the sentiment feature vectors in 2014 to examine the emotional polarity
    of product comments by dependency parsing. Wang et al. used statistical and point
    mutual data to mine and identify the new meanings and emotions of SinaWeibo, defined
    principles of emotional computing at different semantic levels, and conducted
    Chinese sentiment analysis by constructing an emotional dictionary and collection
    of rules in 2015 to incorporate emotional computing. A relationship between the
    product and its various attributes was developed by Liu et al. who also prepared
    a model to match the parts of speech pattern to extract the words of function
    and the words that describe emotions using the domain emotion ontology. To classify
    the text based on properties, with labeled inputs, the model is trained, and this
    learned model will predict the patters of sentiment. Various machine learning
    algorithms like maximum entropy (ME), Naïve Bayes, and SVM were used to train
    the data, and the result was achieved. The probability of sentiment in the words
    and the polarity of sentiment is calculated by the techniques that focus on deep
    learning derive valuable artificial text information.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在2014年，刘等人通过程度和标点副词定义了情感强度，并创建了情感特征向量，以通过依赖解析来检验产品评论的情感极性。王等人利用统计和点互信息数据来挖掘和识别新的意义和情感，定义了不同语义层次的情感计算原则，并在2015年通过构建情感词典和规则集合来进行中文情感分析，将情感计算纳入其中。刘等人开发了产品与其各种属性之间的关系，还准备了一个模型来匹配词性模式，以提取功能词和描述情感的词，使用领域情感本体。为了基于属性对文本进行分类，用标记的输入训练模型，而这个学习过的模型将预测情感的模式。各种机器学习算法，如最大熵（ME）、朴素贝叶斯和支持向量机（SVM）被用来训练数据，并取得了成果。通过专注于深度学习的技术计算情感词中的情感概率和情感的极性，得出有价值的人工文本信息。
- en: A model developed  in the year 2017, for product review emotion analysis, the
    researcher attempted to use the convolution neural network (CNN) and achieved
    improved performance (Hu et al., [2017](#CR17)). A special model developed using
    RNN and LSTM can solve the problem of explosion of gradient values in RNN or the
    disappearance of gradient with one extra cell as compared to the traditional long
    short-term memory model and the RNN model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年开发的一个产品评论情感分析模型中，研究人员尝试使用卷积神经网络（CNN），并取得了改进的性能（胡等人，[2017](#CR17)）。使用RNN和LSTM开发的特殊模型可以解决RNN中梯度值爆炸或梯度消失的问题，相比传统的长短期记忆模型和RNN模型多了一个额外的单元。
- en: Thus, for e-commerce review sentiment analysis based on LSTM with bidirectional
    encoding, a model was developed in this paper. Coming to aspect-based sentiment
    analysis, mainly the tasks involved here are extraction of the aspect term, polarity
    recognition of terms in the aspect, recognition of aspect group, and finding the
    polarity of aspect categories (Hochreiter & Schmidhuber, [2017](#CR16)). A methodology
    based on frequency of occurrence of words was adopted by the authors  , which
    identifies and nominates the most frequent nouns as elements.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，针对基于LSTM的双向编码的电子商务评论情感分析，在本文中开发了一个模型。关于基于方面的情感分析，主要涉及到的任务是提取方面术语、识别方面术语中的极性、识别方面群组和找出方面类别的极性（Hochreiter
    & Schmidhuber，[2017](#CR16)）。作者采用了一种基于词频的方法，该方法将最频繁出现的名词标识并提名为元素。
- en: 12.3 Methodology
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 方法论
- en: In this section we have discussed polarity-based mechanism, GRU model combined
    with CNN, Bi-GRU layer, and LSTM model for sentiment analysis.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论了基于极性的机制、结合了CNN的GRU模型、双向GRU层和LSTM模型用于情感分析。
- en: 12.3.1 Polarity-Based Mechanism
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.1 基于极性的机制
- en: This method is widely used to identify the polarity of the text that is present
    in the reviews given by the user. The classification is done normally as positive,
    negative, and neutral. Positive statements are those which contain all the positive
    words in describing the products that they have purchased. Negative statements
    are those that consist of some words which give negative meaning. Neutral statements
    are those that are neither positive nor negative in meaning.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被广泛用于识别用户评论中存在的文本的极性。分类通常分为积极、消极和中性。积极的陈述是指在描述他们购买的产品时包含所有积极词的陈述。消极的陈述是指包含一些带有负面含义的词语的陈述。中性的陈述是指在含义上既不积极也不消极的陈述。
- en: The dataset used for this method is reviews of products given by the users.
    The reviews of the products are collected from the Python Crawler. In the data
    collection step, the dataset to be used for research is identified. In this case
    the reviews of products that are purchased online by the user are considered.
    Secondly, in the step of pre-processing, boundary of the sentence is identified
    in the first phase, and after having verified, the boundary of the sentence in
    the text is tokenized into individual words. Deletion of stop words, removal of
    white spaces, removal of html tags, removal of emotions, and removal of special
    symbols are also included in the process of data pre-processing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 用于该方法的数据集是用户提供的产品评论。产品的评论是从Python爬虫中收集的。在数据收集步骤中，确定用于研究的数据集。在这种情况下，考虑了用户在线购买的产品的评论。其次，在预处理步骤中，首先确定句子的边界，经过验证后，将文本中的句子边界标记为单个单词。数据预处理过程还包括删除停用词、删除空格、删除HTML标签、删除情感和删除特殊符号。
- en: In the tokenization step, each word should be assigned with a token, and the
    score of that word is taken from the library called SentiWordNet, based on the
    token assigned. In the process of stemming, identical words that are present in
    the dataset considered are removed to make sure that there is no repetition of
    identical words in the reviews. At last, parts of speech tagging is done. The
    reviews given by the user may contain different parts of speech that were tagged
    using Natural Language Tool Kit (NLTK). All adverbs are extracted from the reviews.
    After this, score of the sentence and score of the reviews must be calculated,
    and score of the sentence in the review is computed by the score of the individual
    words that are there in the given particular sentence. The score of the review
    can be computed by computing the scores of the sentences that are there in a review.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词步骤中，每个单词都应该被赋予一个标记，并且该单词的分数是从名为 SentiWordNet 的库中获取的，基于分配的标记。在词干提取过程中，会删除数据集中存在的相同单词，以确保评论中没有相同单词的重复。最后，进行词性标注。用户提供的评论可能包含使用自然语言工具包（NLTK）标记的不同词性。所有的副词都会从评论中提取出来。之后，必须计算句子的分数和评论的分数，评论中的句子的分数由给定特定句子中存在的单词的分数计算得出。评论的分数可以通过计算评论中存在的句子的分数来计算。
- en: To classify a review, it must be tagged using various adverbs and its variations.
    After the process of tagging, reviews and various forms of adverbs can be fetched.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要对一条评论进行分类，必须使用各种副词及其变体进行标记。在标记过程之后，可以获取评论和各种形式的副词。
- en: Once these are extracted, those can be merged to get scores, using SentiWordNet.
    Initially, in the level of sentence, scores are assigned also at the level of
    review scores. Finally, at the end, the final scores of the reviews are acquired,
    and they are categorized with a rating of 5 star which consists of strongly negative,
    negative, neutral, positive, and strongly positive using classification algorithms.
    Here, we have used Naïve Bayes classifier.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些被提取出来，就可以使用 SentiWordNet 合并它们以获得分数。最初，在句子级别，分数也会分配到评论分数的级别。最后，在结束时，获取评论的最终分数，并且使用分类算法对其进行评级为
    5 星，其中包括强烈负面、负面、中立、积极和强烈积极。在这里，我们使用了朴素贝叶斯分类器。
- en: 12.3.2 GRU Combined with CNN
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.2 GRU 结合 CNN
- en: 'In order to provide high accuracy of sentiment analysis on the reviews of products
    procured online by the user, a combined model of CNN and GRU is used. Initially
    a sentiment lexicon is used for the reviews to boost the features given by users.
    Then the main features of sentiment are extracted by the CNN and GRU networks,
    and for weights, attention mechanism is used. At last the features of weighted
    sentiment can be categorized. The structure of the model is shown in Fig. [12.1](#Fig1).
    The model is made up of six layers: an embedded layer, a convolution layer, a
    pooled layer, a layer of Bi-GRU, an attention layer, and a layer which is fully
    connected.![../images/504166_1_En_12_Chapter/504166_1_En_12_Fig1_HTML.png](../images/504166_1_En_12_Chapter/504166_1_En_12_Fig1_HTML.png)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在用户在线购买产品的评论中提供高准确率的情感分析，采用了CNN和GRU的结合模型。最初，使用情感词典对用户提供的特征进行增强。然后，通过CNN和GRU网络提取情感的主要特征，并使用注意力机制进行权重处理。最后，对加权情感的特征进行分类。模型的结构如图[12.1](#Fig1)所示。该模型由六层组成：嵌入层、卷积层、池化层、Bi-GRU层、注意力层和全连接层。![../images/504166_1_En_12_Chapter/504166_1_En_12_Fig1_HTML.png](../images/504166_1_En_12_Chapter/504166_1_En_12_Fig1_HTML.png)
- en: Fig. 12.1
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1
- en: Structure of GRU model combined with CNN
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 结合了CNN的GRU模型的结构
- en: We have considered the input text which will consists of statements S, and each
    statement S consists of words; therefore *S* = {*w*[1], *w*[2], *w*[3], *w*[4]………...*w*[n]}
    where *w*[i] indicates a word in the statement *S*. The work here is to predict
    the polarity of the sentiment in the dataset which is nothing but finding polarity
    *P* for the statement *S* in the text. The sentiment lexicon gives each word *w*[i]
    in *S* a corresponding weight SW[i]. Various open-source sentiment dictionaries
    are used for this purpose.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了输入文本，其中包含了语句S，每个语句S由单词组成；因此*S* = {*w*[1], *w*[2], *w*[3], *w*[4]………...*w*[n]}，其中*w*[i]表示语句*S*中的一个词。这里的工作是预测数据集中语句*S*的极性，即在文本中找到语句*S*的极性*P*。情感词典为*S*中的每个词*w*[i]赋予了相应的权重*SW*[i]。为此使用了各种开源情感词典。
- en: First  we have removed the words of sentiment which are giving some neutral
    meaning and maintain the words of sentiment that are negative and derogatory,
    i.e., words that have polarities of 1 and 2 have to be retained. According to
    their strength, sentiment terms are classified into five groups, comprising 1,
    3, 5, 7, and 9, with the severity of the sentiment as the weight of the sentiment,
    and words of sentiment with sentiment polarity of negative multiply with the sentiment
    of weight by −1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们去掉了一些中性含义的情感词，保留了具有负面和贬义含义的情感词，即极性为1和2的词语必须保留。根据它们的强度，情感词被分为五组，分别为1、3、5、7和9，情感的权重为情感的严重程度，情感极性为负的词语用-1乘以情感权重。
- en: 'Sentiment weight of word expression after construction is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 构造后的词表达的情感权重如下：
- en: '![$$ \mathrm{Senti}\left({w}_{\mathrm{i}}\right)=\Big\{{\displaystyle \begin{array}{cc}{\mathrm{SW}}_{\mathrm{i}},&amp;
    {w}_{\mathrm{i}}\in \mathrm{SD}\\ {}1,&amp; {w}_{\mathrm{i}}\notin \mathrm{SD}\end{array}}
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ1.png)(12.1)where
    *w*[i] indicates a word and SW[i] indicates the weight of the word w[i] in the
    sentiment lexicon and sentiment lexicon represented by SD.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![$$ \mathrm{Senti}\left({w}_{\mathrm{i}}\right)=\Big\{{\displaystyle \begin{array}{cc}{\mathrm{SW}}_{\mathrm{i}},&amp;
    {w}_{\mathrm{i}}\in \mathrm{SD}\\ {}1,&amp; {w}_{\mathrm{i}}\notin \mathrm{SD}\end{array}}
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ1.png)(12.1)其中，*w*[i]表示一个单词，SW[i]表示情感词典中单词*w*[i]的权重，情感词典由SD表示。'
- en: The embedding layer in this model represents the statement S in the dataset
    in the vector matrix as a weighted word. In this method BERT embedding model is
    used to train the word vectors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型中的嵌入层将数据集中的语句S表示为加权单词的向量矩阵。在这种方法中，使用BERT嵌入模型训练单词向量。
- en: '*V*[i] is a 768-dimensional vector, and with this BERT model, every word *w*[i]
    in statement *S* is converted to the word vector *V*[i]. Later, with sentiment
    weights, the weight of the word vector is checked:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*V*[i]是一个768维向量，并且使用BERT模型，语句*S*中的每个单词*w*[i]都转换为单词向量*V*[i]。随后，使用情感权重，检查单词向量的权重：'
- en: '![$$ {V}_{\mathrm{i}}^{\prime }={V}_{\mathrm{i}}\ast \mathrm{senti}\left({w}_{\mathrm{i}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ2.png)(12.2)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![$$ {V}_{\mathrm{i}}^{\prime }={V}_{\mathrm{i}}\ast \mathrm{senti}\left({w}_{\mathrm{i}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ2.png)(12.2)'
- en: The output of the embedded layer will be a vector matrix consisting of weighted
    words. In the convolution layer, from the input matrix, important local features
    are extracted. A complete representation of the word vector is done in the area
    of natural language processing. So, the width of the kernel in the convolution
    layer consumes the entire region of the word vector. Next in the pooling layer,
    the text features acquired from the convolution layer are compressed, and the
    main features are extracted. The operations of pooling are split up as average
    pooling and max pooling. In the sentiment analysis of the text, some phrases or
    words will be influential that are present in the sentences; therefore k-max pooling
    is used.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的输出将是一个由加权单词组成的向量矩阵。在卷积层中，从输入矩阵中提取重要的局部特征。在自然语言处理领域完成了单词向量的完整表示。因此，卷积层中核的宽度占用了整个单词向量的区域。接下来在池化层中，从卷积层获取的文本特征被压缩，提取了主要特征。池化操作分为平均池化和最大池化。在文本的情感分析中，一些短语或词语会对句子产生影响；因此，使用了k-max池化。
- en: 12.3.3 Bi-GRU Layer
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.3 双向门控循环单元（Bi-GRU）层
- en: Coming to Bi-GRU layer, extraction of feature from the input matrix is the main
    task here. Another variation of the recurrent neural network is GRU model, and
    sequence of information is processed by this. Historical data of the earlier point
    of time which influences the current output and extraction of features of context
    data in sequence are combined in the method. In the sentences of text data, the
    current word is affected by both the word that is proceeding and also succeeding;
    therefore the extraction of the features of the context data of the input is done
    by Bi-GRU model. The Bi-GRU model is made up of forward GRU which process forward
    data and a reverse GRU that process reverse data. For the input text *x*[t] at
    time *t* and hidden states that are obtained by forward GRU is *H*[t]′ and hidden
    states that are obtained by reverse GRU is *H*[t]″, and these are given by
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Bi-GRU层，从输入矩阵中提取特征是主要任务。循环神经网络的另一种变体是GRU模型，该模型通过这种方式处理信息序列。此方法将早期时间点的历史数据影响当前输出以及顺序上下文数据的特征提取结合在一起。在文本数据的句子中，当前单词受到前后单词的影响；因此，Bi-GRU模型通过处理输入的上下文数据来提取特征。Bi-GRU模型由处理前向数据的前向GRU和处理反向数据的反向GRU组成。对于时间*t*的输入文本*x*[t]和由前向GRU获得的隐藏状态为*H*[t]′，以及由反向GRU获得的隐藏状态为*H*[t]″，它们分别为
- en: '![$$ {H}_{\mathrm{t}}^{\prime }=\left({x}_{\mathrm{t}},{H_{\mathrm{t}-1}}^{\prime}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ3.png)(12.3)![$$
    {H_{\mathrm{t}}}^{\prime \prime }=\left({x}_{\mathrm{t}},{H_{\mathrm{t}-1}}^{\prime
    \prime}\right) $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ4.png)(12.4)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![$$ {H}_{\mathrm{t}}^{\prime }=\left({x}_{\mathrm{t}},{H_{\mathrm{t}-1}}^{\prime}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ3.png)(12.3)![$$
    {H_{\mathrm{t}}}^{\prime \prime }=\left({x}_{\mathrm{t}},{H_{\mathrm{t}-1}}^{\prime
    \prime}\right) $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ4.png)(12.4)'
- en: The combination of forward GRU and reverse GRU is considered to be the output
    at time *t*. In the attention layer, every word in the statements of the text
    has a significant impact on the sentimental polarity of the entire sentence. The
    sentence will contain some of the words which give a partial effect on the sentiment
    of the entire text; also, it may contain some of the words which do not contribute
    anything to the sentiment of that sentence. Therefore, making use of the attention
    layer, various weights can be assigned to various words in the given sentence.
    At last in the fully connected layer, the classification of the input feature
    matrix is done.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 前向GRU和反向GRU的组合被认为是时间*t*的输出。在注意力层中，文本中的每个单词对整个句子的情感极性都有重要影响。句子将包含一些对整个文本情感产生部分影响的词，也可能包含一些对该句子情感没有任何贡献的词。因此，利用注意力层，可以为给定句子中的各个词分配各种权重。最后在全连接层中，对输入特征矩阵进行分类。
- en: 'The output of this layer can be described as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层的输出可以描述如下：
- en: '![$$ P=f\left(M\ast X\right)+D $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ5.png)(12.5)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![$$ P=f\left(M\ast X\right)+D $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ5.png)(12.5)'
- en: where *f* is indicating the function of activation sigmoid, *M* indicates the
    matrix of the weights, and *D* indicates the offset. In this layer input feature
    is depicted with a value in the interval [0, 1]. If that value is closer to 0,
    it indicates the sentiment polarity of the text closer to the negative direction,
    and on the other hand, if the value is closer to 1, it indicates that polarity
    of the sentence is toward positive polarity.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*f*表示激活sigmoid函数，*M*表示权重矩阵，*D*表示偏移量。在此层中，输入特征用区间[0, 1]中的值表示。如果该值接近0，则表示文本情感极性接近负向，另一方面，如果该值接近1，则表示句子的极性朝向正极性。
- en: 12.3.4 Long Short-Term Memory
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3.4 长短期记忆
- en: Another method that is widely used for sentiment analysis is long short-term
    memory (LSTM) which is a distinct model in recurrent neural network (RNN) architecture
    which is structured more specifically to model temporal sequences and their long-range
    dependencies than traditional RNNs. To solve problems posed by the RNN model,
    LSTM can be used. Therefore, it can be used for solving issue with long-term dependence
    in RNN and gradient disappearance and gradient bursting. The soul of LSTM network
    is its cell which provides the LSTM network a small amount of memory that it can
    remember the past data. The structure of LSTM memory cell is shown in Fig. [12.2](#Fig2).
    LSTM network is made up of gates which have input gates, forget gates, and output
    gates. The gates in LSTM are nothing but a function of sigmoid activation function
    which means they give output a value in between 0 and 1\. In most of the cases,
    it will be either 0 or 1\. The equations for the gates in LSTM are:![../images/504166_1_En_12_Chapter/504166_1_En_12_Fig2_HTML.png](../images/504166_1_En_12_Chapter/504166_1_En_12_Fig2_HTML.png)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个广泛用于情感分析的方法是长短期记忆（LSTM），它是循环神经网络（RNN）架构中的一个独特模型，更专门地用于建模时间序列及其长期依赖性，而不是传统的RNN。为了解决RNN模型提出的问题，可以使用LSTM。因此，它可用于解决RNN中的长期依赖问题以及梯度消失和梯度爆炸的问题。LSTM网络的灵魂是其单元，它为LSTM网络提供了一小部分可以记住过去数据的存储。LSTM存储单元的结构如图
    [12.2](#Fig2) 所示。LSTM网络由门组成，其中包括输入门、遗忘门和输出门。LSTM中的门仅仅是一个sigmoid激活函数的函数，这意味着它们输出一个介于0和1之间的值。在大多数情况下，它将是0或1。LSTM中门的方程为：![../images/504166_1_En_12_Chapter/504166_1_En_12_Fig2_HTML.png](../images/504166_1_En_12_Chapter/504166_1_En_12_Fig2_HTML.png)
- en: Fig. 12.2
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2
- en: Structure  of LSTM memory cell
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM（长短期记忆）存储单元的结构
- en: '![$$ {I}_{\mathrm{t}}=\sigma \left({M}_{\mathrm{i}}\left[{P}_{\mathrm{t}-1},{Q}_{\mathrm{t}}\right]+{Z}_{\mathrm{i}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ6.png)(12.6)![$$
    {G}_{\mathrm{t}}=\sigma \left({M}_{\mathrm{f}}\left[{P}_{\mathrm{t}-1},{Q}_{\mathrm{t}}\right]+{Z}_{\mathrm{f}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ7.png)(12.7)![$$
    {K}_{\mathrm{t}}=\sigma \left({M}_{\mathrm{o}}\left[{P}_{\mathrm{t}-1},{Q}_{\mathrm{t}}\right]+{Z}_{\mathrm{o}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ8.png)(12.8)where
    *I*[t] indicates the input gate, *G*[t] indicates the forget gate, *K*[t] indicates
    the output gate, *σ* indicates the sigmoid function, *M*[x] indicates the weight
    for the respective gates(*x*), *H*[t-1] indicates the output of the previous block
    (at time stamp t-1), *Q*[t] indicates the input at current time stamp, and *Z*[x]
    indicates the biases for respective gates(*x*).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![$$ {I}_{\mathrm{t}}=\sigma \left({M}_{\mathrm{i}}\left[{P}_{\mathrm{t}-1},{Q}_{\mathrm{t}}\right]+{Z}_{\mathrm{i}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ6.png)(12.6)![$$
    {G}_{\mathrm{t}}=\sigma \left({M}_{\mathrm{f}}\left[{P}_{\mathrm{t}-1},{Q}_{\mathrm{t}}\right]+{Z}_{\mathrm{f}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ7.png)(12.7)![$$
    {K}_{\mathrm{t}}=\sigma \left({M}_{\mathrm{o}}\left[{P}_{\mathrm{t}-1},{Q}_{\mathrm{t}}\right]+{Z}_{\mathrm{o}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ8.png)(12.8)其中
    *I*[t] 表示输入门，*G*[t] 表示遗忘门，*K*[t] 表示输出门，*σ* 表示 sigmoid 函数，*M*[x] 表示相应门的权重(*x*)，*H*[t-1]
    表示前一块的输出（在时间戳 t-1），*Q*[t] 表示当前时间戳的输入，*Z*[x] 表示相应门(*x*)的偏置。'
- en: The equation is used for input gate that gives the information of the new data
    that is going to be stored in the state of the cell. The second equation is written
    for the forget gate that gives the information to be removed away from the state
    of the cell. The third equation is used for the output gate that is used to furnish
    the final output of the block of LSTM at time “*t*.”
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方程用于输入门，给出了将要存储在细胞状态中的新数据的信息。第二个方程用于遗忘门，给出了将要从细胞状态中移除的信息。第三个方程用于输出门，用于提供
    LSTM 块在时间“*t*”的最终输出。
- en: 'We shall give the equations of the state of the cell, state of the candidate
    cell, and the final output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将给出细胞状态、候选细胞状态和最终输出的方程：
- en: '![$$ {C_{\mathrm{t}}}^{\prime }=\tanh \left({M}_{\mathrm{c}}\left[{P}_{\mathrm{t}-1},{Q}_{\mathrm{t}}\right]+{Z}_{\mathrm{c}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ9.png)(12.9)![$$
    {C}_{\mathrm{t}}={G}_{\mathrm{t}}\ast {C}_{\mathrm{t}-1}+{I}_{\mathrm{t}}\ast
    {C_{\mathrm{t}}}^{\prime } $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ10.png)(12.10)![$$
    {P}_{\mathrm{t}}={K}_{\mathrm{t}}\ast \tanh \left({C}^{\mathrm{t}}\right) $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ11.png)(12.11)where
    *C*[t] indicates the cell state in the memory at time stamp *t*, *C*[t]′ indicates
    the candidate for cell state at time stamp *t*, *I*[t] indicates the input gate,
    *G*[t] indicates the forget gate, *K*[t] indicates  the output gate, *M*[c] indicates
    the weight for the respective gates(*x*), *P*[t-1] indicates the output of the
    previous block (at time stamp t-1), *Q*[t] indicates the input at current time
    stamp, and *Z*[c] indicates the biases for respective gates(*x*).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: At time “*t*” with the above equations, it can be seen that the state of the
    cell will have the information of what the former states ought to forget and the
    information that needs to be considered from the current time “*t*.” At last,
    the state of the cell needs to be filtered, and then it has to pass through the
    function of activation that makes the prediction of what part must appear as the
    output of the current unit of LSTM at time “*t*.” Then the output of the current
    block of LSTM is passed to the Softmax layer to get the output that is predicted
    from the current block of LSTM.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 Case Study
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we have discussed the case study on Amazon product reviews.
    First, we have described the dataset, then in the next part, the novel model developed
    for sentiment analysis of Amazon product reviews is explained.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.1 Dataset
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dataset  we have used is taken from Kaggle website. We have considered rating
    for the products given by the user from 1 to 5\. Reviews that has rating for the
    product 3–5 are considered to be positive reviews, and reviews that have rating
    for the product 1–2 are considered to be negative reviews. Then we perform manual
    screening on the dataset to ensure the reviews under positive dataset are all
    positive and the reviews under negative dataset are all negative. Our dataset
    consisted of total records 3000\. Out of these 1500 are positive reviews and 1500
    are negative reviews. We have divided this dataset into two parts. Seventy percent
    of the records in the dataset that constitute 2100 records are used for training
    the model, and 30% of the records in the dataset that constitute  of 900 records
    are used for testing the model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的数据集来自 Kaggle 网站。我们考虑了用户对产品的评分，评分范围从 1 到 5。对产品评分为 3–5 的评论被视为积极的评论，而对产品评分为
    1–2 的评论被视为消极的评论。然后我们对数据集进行手动筛选，以确保正面数据集下的评论都是积极的，负面数据集下的评论都是消极的。我们的数据集总共有 3000
    条记录。其中 1500 条是积极的评论，另外 1500 条是消极的评论。我们将这个数据集分成了两部分。数据集中 70% 的记录，即 2100 条记录，用于训练模型，数据集中
    30% 的记录，即 900 条记录，用于测试模型。
- en: The architecture of the novel model developed  is as shown in Fig. [12.3](#Fig3).![../images/504166_1_En_12_Chapter/504166_1_En_12_Fig3_HTML.png](../images/504166_1_En_12_Chapter/504166_1_En_12_Fig3_HTML.png)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所开发的新模型的架构如图所示。[12.3](#Fig3)![../images/504166_1_En_12_Chapter/504166_1_En_12_Fig3_HTML.png](../images/504166_1_En_12_Chapter/504166_1_En_12_Fig3_HTML.png)
- en: Fig. 12.3
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3
- en: Architecture of the model using stacked embedding
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用堆叠嵌入的模型架构
- en: The architecture consists  of preprocessing unit, embedding layer, and Bi-GRU
    model with attention layer.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构由预处理单元、嵌入层和带有注意力层的双向 GRU 模型组成。
- en: 12.4.2 Data Preprocessing
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.2 数据预处理
- en: Our corpus has unclean data  , so before we offer it as an input for our model,
    we have cleaned it. During the cleaning process, we deleted all redundant data
    and null data, and all HTML characters are removed. Our dataset included some
    of the complicated symbols that we decoded and made clear understandable things,
    removed all the punctuation marks, and removed all the stop words. Then the cleaned
    data is fed into the tokenizer for tokenization; for this we used Natural Language
    Toolkit (NLTK) tokenizer from SK library. Further, the tokenized data is given
    for embedding  .
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语料库存在不干净的数据，因此在将其作为模型的输入之前，我们对其进行了清理。在清理过程中，我们删除了所有冗余数据和空数据，并删除了所有 HTML 字符。我们的数据集包含一些复杂的符号，我们对其进行解码并清楚地解释了这些内容，删除了所有标点符号，并删除了所有停用词。然后将清理后的数据输入到分词器进行分词；我们使用了来自
    SK 库的 Natural Language Toolkit (NLTK) 分词器进行分词。此外，分词后的数据会被送入嵌入器进行嵌入。
- en: 12.4.3 Stacked Embedding
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4.3 堆叠嵌入
- en: We have stacked  two context-based embedding approaches such as BERT and GloVe
    in our experiment with the help of Flair Library. “BERT-base-uncased” with an
    embedding dimension of 768 and a GloVe with a vector dimension of 100 are the
    underlying option templates for BERT and GloVe, respectively. The resultant embeddings
    are given as input to Bi-GRU model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实验中使用了两种基于上下文的嵌入方法，如 BERT 和 GloVe，借助 Flair Library 的帮助。"BERT-base-uncased"
    的嵌入维度为 768，GloVe 的向量维度为 100，分别是 BERT 和 GloVe 的基本选项模板。生成的嵌入被作为输入提供给 Bi-GRU 模型。
- en: The next step  , the output from embedding layer is provided to the bidirectional
    GRU model. This model uses knowledge from both the previous steps and the next
    steps to predict the current state. In our experiment, we used Bi-GRU with three
    hidden layers of size 120, 60, and 30, respectively. The output of the third layer
    is given to the attention layer. The attention layer assigns different weights
    to different words depending on the influence of the sentiment on each word. There
    might be some words which may not contribute to the sentiment of any words, or
    they may influence less. Based on this weight must be assigned. The output of
    the attention layer is again transferred through the dense layer to reduce dimensionality.
    At last, the output from a dense layer is given to a sigmoid layer to classify
    our corpus in either a positive or a negative class. The findings of our model
    are seen in the following section  .
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，嵌入层的输出被提供给双向 GRU 模型。该模型利用前一步和下一步的知识来预测当前状态。在我们的实验中，我们使用了三个隐藏层大小分别为 120、60
    和 30 的 Bi-GRU。第三层的输出被传递给注意力层。注意力层根据情感对每个词的影响分配不同的权重。可能有些词对任何词的情感都没有贡献，或者它们的影响较小。基于此，必须分配权重。注意力层的输出再次通过密集层传递以降低维度。最后，密集层的输出被传递给一个
    Sigmoid 层，以将我们的语料库分类为正类或负类。我们模型的发现见下一节。
- en: 12.5 Results
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.5 结果
- en: Findings from our proposed model reveal that the accuracy of our model is 95.32%,
    with an F1 score of 95%. In addition, we have tried to boost our model performance
    by using k-fold cross-validation  . Results indicate that the model performance
    with k-fold cross-validation increases slightly. Here, tenfold cross-validation
    has been used. Table [12.1](#Tab1) displays the outcomes.Table 12.1
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的模型的研究结果表明，我们的模型准确率为 95.32%，F1 分数为 95%。此外，我们尝试通过使用 k 折交叉验证来提升模型性能。结果表明，使用
    k 折交叉验证的模型性能略有提高。这里使用了十折交叉验证。表[12.1](#Tab1)展示了结果。表 12.1
- en: F1 score and accuracy of the model
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的 F1 分数和准确率
- en: '| Accuracy | F1 score | Tenfold cross-validation |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | F1 分数 | 十折交叉验证 |'
- en: '| --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Accuracy | F1 score |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | F1 分数 |'
- en: '| --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 95.32% | 95.10% | 95.92% | 95.89% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 95.32% | 95.10% | 95.92% | 95.89% |'
- en: We have tried to explore the impact of the number of iterations and dropout
    values on the output of our model. We find that our model would reach maximum
    efficiency at eight iterations with a dropout value of 0.4\. Later output will
    continue to decline. Tables [12.2](#Tab2) and [12.3](#Tab3) demonstrate the impact
    of iteration number and dropout value on the output of our model.Table 12.2
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图探索迭代次数和辍值对我们模型输出的影响。我们发现，在辍值为0.4的情况下，我们的模型在八次迭代时达到最大效率。后续输出将继续下降。表[12.2](#Tab2)和[12.3](#Tab3)展示了迭代次数和辍值对我们模型输出的影响。表12.2
- en: The effect of the number of iterations in the model
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中迭代次数的影响
- en: '| Iteration | Accuracy | F1 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 迭代 | 准确率 | F1 |'
- en: '| --- | --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 3 | 95.3% | 95.1% |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 95.3% | 95.1% |'
- en: '| 5 | 95.5% | 95.2% |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 95.5% | 95.2% |'
- en: '| 8 | 95.9% | 95.8% |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 95.9% | 95.8% |'
- en: '| 10 | 95.3% | 95.2% |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 95.3% | 95.2% |'
- en: '| 12 | 95.0% | 95.1% |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 95.0% | 95.1% |'
- en: '| 15 | 94.6% | 94.5% |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 94.6% | 94.5% |'
- en: Table 12.3
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.3
- en: The effect of dropout value in the model
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中辍值的影响
- en: '| Dropout | Accuracy | F1 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 辍值 | 准确率 | F1 |'
- en: '| --- | --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0.2 | 95.1% | 95.1% |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 0.2 | 95.1% | 95.1% |'
- en: '| 0.4 | 95.8% | 95.6% |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 0.4 | 95.8% | 95.6% |'
- en: '| 0.5 | 95.5% | 95.2% |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 95.5% | 95.2% |'
- en: '| 0.6 | 95.5% | 95.5% |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 0.6 | 95.5% | 95.5% |'
- en: '| 0.8 | 94.7% | 94.8% |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 0.8 | 94.7% | 94.8% |'
- en: The accuracy of the model was calculated to be 95.32%. The model was more accurate
    than the models produced using other algorithms. The novelty of stacked embedding
    developed by the Flair Library has contributed to the accuracy of the findings.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的准确率被计算为95.32%。该模型比使用其他算法生成的模型更准确。Flair库开发的堆叠嵌入的新颖性为研究结果的准确性做出了贡献。
- en: 12.6 Conclusion
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.6 结论
- en: As there is a tremendous growth in the online business platforms, sentiment
    analysis has got a great importance for the reviews of the products given by the
    users. In this paper, we have developed a model using stacked embedding for sentiment
    analysis of Amazon product reviews. Initially, various methods like polarity-based
    sentiment classification, long short-term memory (LSTM), and bidirectional GRU
    models have been explained. Taking the case study into consideration, we have
    developed a model in which we have considered specifically Amazon product reviews
    as the case study since it is a popular e-commerce platform. We have developed
    a stacked embedding model in which Flair Library is used to implement GloVe and
    BERT embedding models. Then the bidirectional GRU model is used to extract the
    features of the sentiment in the text sentences of reviews. Then at attention
    layer, different weights are assigned to different words depending on the contribution
    of their sentiment on each word. Then these are passed to sigmoid layer through
    the dense layer. Making use of sigmoid function, the model classifies the statements
    of reviews to be either positive or negative by assigning the values either 0
    or 1\. A dataset consisting of 3000 samples was considered, for which our model
    has achieved a highest accuracy compared to other models. With the increase in
    the dataset given to the model, it will achieve consistently improved accuracy.
    The model we have developed can be used for various applications of sentiment
    analysis.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 随着在线商务平台的巨大增长，情感分析对用户评价的产品具有重要意义。在本文中，我们采用堆叠嵌入的模型对亚马逊产品评论进行情感分析。首先，解释了各种方法，如基于极性的情感分类、长短期记忆（LSTM）和双向
    GRU 模型。考虑到案例研究，我们开发了一个模型，其中将亚马逊产品评论专门作为案例研究，因为它是一个流行的电子商务平台。我们开发了一个堆叠嵌入模型，其中使用
    Flair 库来实现 GloVe 和 BERT 嵌入模型。然后，双向 GRU 模型用于提取评论文本句子中的情感特征。然后在注意层，根据其在每个词上的情感贡献分配不同的权重。然后将这些通过稠密层传递给
    sigmoid 层。利用 sigmoid 函数，该模型通过将值分配为 0 或 1 将评论的语句分类为积极或消极。考虑了一个包含 3000 个样本的数据集，对于该数据集，我们的模型相对于其他模型获得了最高的准确性。随着提供给模型的数据集增加，它将实现持续改进的准确性。我们开发的模型可用于情感分析的各种应用。
- en: A further study on fineness of the sentiment must be done to meet the requirements
    of the areas where a higher level of refinement of sentiment analysis is expected.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足对情感分析更高级别细化要求的领域，必须进一步研究情感的细腻程度。
