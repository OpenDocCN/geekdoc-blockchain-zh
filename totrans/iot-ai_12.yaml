- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2021R.
    Kumar et al. (eds.)Internet of Things, Artificial Intelligence and Blockchain
    Technology[https://doi.org/10.1007/978-3-030-74150-1_12](https://doi.org/10.1007/978-3-030-74150-1_12)
  prefs: []
  type: TYPE_NORMAL
- en: '12. Bi-GRU Model with Stacked Embedding for Sentiment Analysis: A Case Study'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sanjana Kavatagi^([1](#Aff5)) and Vinayak Adimule^([2](#Aff6)  )(1)Department
    of Computer Science and Engineering, Angadi Institute of Technology and Management
    (AITM), Belagavi, Karnataka, India(2)Dean R&D, Angadi Institute of Technology
    and Management (AITM), Belagavi, Karnataka, IndiaKeywordsMachine learningSentiment
    analysisLSTMGRUCNNPolarity-based techniqueEmbeddingProf. Sanjana M. Kavatagi
  prefs: []
  type: TYPE_NORMAL
- en: received her bachelor’s degree and master’s degree from Visvesvaraya Technological
    University, Belagavi, Karnataka. She is currently working as an Assistant Professor
    in the Department of Computer Science and Engineering, Angadi Institute of Technology
    and Management, Belagavi. She has published two papers in national and international
    journals. Her research interests are artificial intelligence, machine learning,
    and big data. ![../images/504166_1_En_12_Chapter/504166_1_En_12_Figa_HTML.jpg](../images/504166_1_En_12_Chapter/504166_1_En_12_Figa_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Prof. Dr. Vinayak Adimule
  prefs: []
  type: TYPE_NORMAL
- en: has 13 years of research experience as a Senior Scientist and an Associate Research
    Scientist in R&D organizations of TATA (Advinus), Astra Zeneca India, Trans. Chem.
    Ltd. He is expert in the area of medicinal chemistry (anticancer drugs), nanoscience
    and technology, material chemistry, and biochemistry. He published more than 30
    research articles and book chapters in Scopus and Google Scholar (SJR)-indexed
    journals; attended and presented papers in national and international conferences;
    received best oral/poster awards; published few books in Notion Press International
    Inc.; chaired few international and national conferences symposia related to material
    electronics. He is an Editorial Board Member, Life Member, and Associate Member
    of many international societies¸ research institutions. He is recognized as Research
    Guide VTU, Belagavi. Presently, he is guiding one research scholar for his PhD
    degree. His research interests include nanoelectronics, material chemistry, sensors
    and actuators, bionanomaterials, and medicinal chemistry.![../images/504166_1_En_12_Chapter/504166_1_En_12_Figb_HTML.jpg](../images/504166_1_En_12_Chapter/504166_1_En_12_Figb_HTML.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the growth of Internet in today’s world, the data is getting generated
    exponentially. e-Commerce is one such area where people make most use of it. The
    e-commerce applications like Flipkart, Amazon, Snapdeal, Grofers, big basket,
    and others are the highly used platforms for online shopping. Due to the exponential
    development and popularization of e-commerce technologies, everyone loves shopping
    on numerous e-commerce pages (Liang & Wang, [2019](#CR26)).
  prefs: []
  type: TYPE_NORMAL
- en: Consumers have been increasingly keen to share their opinions and feelings on
    the Internet with the rise of online shopping. Extraction of emotions from the
    users in online reviews is of great importance, which not only attracts potential
    users by helping them make purchase decisions but also allows organizations to
    gain feedback on the product. On goods or services, customers may share their
    views, opinions, and user experiences. They may also provide other people with
    clear suggestions (Chen et al., [2018a](#CR5), [b](#CR6)). Nowadays, reviews given
    online by the customers are considered to be the best and successful sources for
    supporting the online buying decisions of prospective customers on goods or services
    (Gao et al., [2017](#CR11); Gavilan et al., [2018](#CR12)). In Natural Language
    Processing (NLP), sentiment analysis plays a critical role in collecting and processing
    relevant information from user feedback shared through online communities and
    collaborative media. From an applicative point of view, the viewed data is becoming
    increasingly relevant, solely in intelligent science and business systems to overcome
    and enhance the quality of their services and products (La’ercio Dias, [2018](#CR25);
    Xing et al., [2018](#CR38)).
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the data of online reviews is complex and comprehensive, and it also
    contains multiple characteristics of a product or service being reviewed (also
    referred to as aspects of literature) (Zhang et al., [2010](#CR43)). The words
    in the review can be interpreted as either positive or negative. The words describing
    the quality of products as “good” can be considered as a positive perception on
    the product, whereas the word “high” used in expressing the cost can be considered
    to give some negative perception on the products.
  prefs: []
  type: TYPE_NORMAL
- en: Every statement in the review will assume some knowledge of positive perception,
    negative perception, or some neutral perception altogether because of the nuances
    and inherent fuzziness that is inherent in preferences and decisions of human
    beings. So, the assessment of such a broad range of online ratings is challenging.
    If we just take into account the general product sentiment values and disregard
    the fuzziness and randomness implicit in online feedback of different features,
    valuable details can be overlooked or distorted.
  prefs: []
  type: TYPE_NORMAL
- en: Till date, a huge research has been conducted on sentiment analysis. A lot of
    tools and algorithms on sentiment analysis have been implemented by the researchers.
    Sentiment analysis research can not only be carried out on consumer review results,
    but it can also be conducted on ads, data from social media, etc. The reviews
    that are given by the users can have sarcasm, and some reviews may be irrelevant
    to the product, which means customer relationships, product distribution, etc.
    may be the reviews that need to be considered as important ones.
  prefs: []
  type: TYPE_NORMAL
- en: We have focused on the study of sentiment at the aspect level (Kajal & Vandana,
    [2017](#CR21)) initially, whether the argument is true or speculative should be
    decided. Sentence-level SA has to decide if the sentence is subjective, and whether
    the sentence represents positive or negative views (Liu, [2012](#CR27)) claimed
    that representations of feeling do not always have an arbitrary essence. However,
    there is no fundamental difference between the classification of the text and
    the classification of the sentence level, as the sentences are only brief documents
    (Yu Liang-Chih et al., [2013](#CR41)).
  prefs: []
  type: TYPE_NORMAL
- en: At the aspect level, it seeks to classify the feelings expressed in terms of
    words in the review by people. For different features of the same product, each
    individual may have a different opinion. For example, the statement “the display
    quality of the phone is good but the battery is not long-lasting.” The first two
    forms of SA are covered by this survey.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to product ratings, SA applies to capital markets (Hagenau et al.,
    [2013](#CR14); Xu et al., [2012](#CR39)), news stories (Kim et al., [2018](#CR24)),
    and parliamentary discussions (Graham, [2009](#CR9)). The desire to learn, mine,
    and analyze this knowledge has increased dramatically due to the massive data
    evolution and the amount of data being exchanged and generated every second. And
    since standard machine learning techniques and neural networks were not enough
    for this big data to be collected, deep learning was the key to the era of big
    data (Chitkara et al., [2019](#CR7)).
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is a discipline of machine learning and an alternation of neural
    networks. In other words, in addition to hidden layers in between, a standard
    neural network is a single network with an input and output layer, where computation
    is performed, whereas deep neural networks consist essentially of multiple neural
    networks, where one network’s output is an input to the next network and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning networks learn the characteristics on their own, i.e., it has
    become apparent as a robust technique of machine learning that learns multiple
    layers of data characteristics and induces prediction performance. In various
    applications in the field of signal and information processing, deep learning
    has recently been used, especially with the evolution of big data (Zeng et al.,
    [2018](#CR42)). Furthermore, in sentiment analysis and opinion mining, deep learning
    networks have been used. In recent years, in many fields, it has created great
    achievements. Deep learning doesn’t require human intervention features compared
    to conventional machine learning approaches; big data is needed as support for
    deep learning, however. Deep learning approaches automatically derive characteristics
    from various neural network techniques and improve with their own mistakes (Yang
    et al., [2019](#CR40)).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we have discussed polarity-based model, gated recurrent unit
    (GRU) combined with convolution neural network (CNN), and long and short-term
    memory (LSTM) model. We have developed a novel model using stacked embedding to
    perform sentiment analysis on Amazon product reviews which improves the accuracy
    compared to the previous models.
  prefs: []
  type: TYPE_NORMAL
- en: In Sect. [12.2](#Sec2) we have discussed literature review. Then we have explained
    various methods used for sentiment analysis in Sect. [12.3](#Sec6). Then in Sect.
    [12.4](#Sec11), we have introduced our novel model developed for Amazon product
    reviews. Results are discussed in Sect. [12.5](#Sec15) followed by summarization
    of the chapter and future scope in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Literature Review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The study of sentiment analysis is a field that deals with the analysis and
    classification of the opinions, thoughts, and sentiment of the customer. At present,
    the research on techniques for sentiment analysis is grouped into three categories,
    namely, machine learning-based sentiment analysis technique, sentiment analysis
    based on lexicon techniques (Liu et al., [2017](#CR28); Zhou et al., [2017](#CR45)),
    and it is also based on some hybrid models.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning strategies succeed in the optimization of parameters of a system
    which has a vast volume of training data. Positive and negative word sets are
    used in lexicon-based approaches in order to differentiate feelings, while mixed
    versions make use of both models in combination. In recent studies, approaches
    to machine learning involving supervised learning models (Agarwal et al., [2015](#CR2);
    Xia et al., [2014](#CR37)), semi-supervised learning models  , and unsupervised
    learning models (Guo et al., [2017](#CR13)) have been studied. The focus of this
    is the development of sentiment dictionaries (Loughran & Mcdonald, [2011](#CR30))
    by manual or automatic processes. Extant literature on lexicon-based sentiment
    analysis approaches mostly contains dictionary-based and corpus-based categories.
    While this one is used to identify patterns of the syntax and develop a list of
    words of opinion. Performance analysis of sentiments in Twitter dataset using
    SVM models was analyzed by (Ramasamy et al. [2021a](#CR33)).
  prefs: []
  type: TYPE_NORMAL
- en: In this review, we use the latter to distinguish sentiment of the product as
    either positive, neutral, or negative relevant to each of the features, provided
    that machine learning techniques face many limitations and portability and robustness
    benefits are shown by dictionary-based sentiment analysis techniques across different
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Sentiment Lexicon-Based Sentiment Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jurek et al. ([2015](#CR20)) showed that the words  in the dictionary with the
    indication of polarity and intensity of sentiment are used in this method to derive
    sentiment from the document. A lexicon-based sentiment analysis algorithm was
    proposed by (Ashhar et al. [2017](#CR3)), using evidence-based mixture features
    and sentiment normalization techniques. Ashhar et al. To analyze emotion processing
    of online user feedback, in addition to using terms for emotion. Some modifiers
    that are domain specific terms are added. A DSEL-based unigram mixture model (UMM)
    was suggested by (Bandhakavi et al. [2017](#CR4)). Emotion classification is done
    to extract effective features with the use of labeled and coarsely labeled emotion
    text. Dhaoui et al. ([2017](#CR8)) used the machine learning package of LIWC2015
    lexicon, and RTextTools are used to evaluate the process of the study of sentiments
    based on lexicon and machine learning (Ramasamy et al., [2021b](#CR34)). Selection
    of optimal hyper-parameter values of support vector machine for sentiment analysis
    tasks uses nature-inspired optimization methods. Khoo and Johnkhan ([2018](#CR23))
    introduced a common sentiment lexicon which is named the WKWSCI sentiment lexicon
    and is evaluated by comparing to the present sentiment lexicons (Zhang et al.,
    [2018](#CR44)). To test Chinese microblog text sentiment, used different sentiment
    lexicons. Keshavarz and Abadeh ([2017](#CR22)) produced an adaptive sentiment
    vocabulary to increase Weibo’s accuracy in the classification of sentiment by
    using a mixture of corpus and lexicons. A two-layer graph model was developed
    by (Feng et al., [2015](#CR10)), built using different sentiment terms of emoji
    and terms of candidate sentiment, and picked the popular terms present in the
    model as phrases of sentiment. Therefore, machine learning-based solution that
    impulsively generates sentiment features has become a safer choice for researchers
    with a little manual involvement  .
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 Machine Learning-Based Sentiment Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By using the gini index and the classification  of the support vector machine
    (SVM), a function extraction method was proposed by (Manek et al. [2017](#CR32))).
    A novel probabilistic supervised joint emotion model (SJSM) was suggested by (Hai
    et al., [2017](#CR15)) which accepts semantic emotions from the data of comments
    given by users but also hypothesize the general sentiment of the data in comments.
    A theme model developed in joint with a multi-model for sentiment analysis was
    suggested by (Singh et al., [2017](#CR35)). With the focus on the personality
    traits of the user and words with effect of sentiment, this model used latent
    dirichlet allocation (LDA) to determine the sentiment of the user written in words.
    Huq et al. [2017](#CR18) studied various algorithums to evaluate twitter data
    sentiment. Long et al. ([2018](#CR29)) used SVM to use additional samples containing
    previous information to identify stock forum messages. While features can be extracted
    automatically by the machine learning-based process, it also relies on selection
    of features manually. The strategy based on deep learning doesn’t need manual
    intervention. Through the neural network framework, it can pick and extract features
    automatically, and it can master from its own mistakes  .
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.3 Deep Learning-Based Sentiment Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A qualitative analysis  of the characteristics of semantics and some co-occurrence
    characteristics of statistics in the words of tweet and method reference for the
    n-gram convolution neural network to test the emotion polarity was used by (Jianqiang
    et al., [2018](#CR19)). Ma et al. ([2018](#CR31)) developed a model using the
    target-dependent convolution neural networks in which the words surrounds the
    target word that influence it and the distance between the intended word and the
    words surrounding it. If we focus on the process, every word in the sentence will
    give some emotional impact on deciding the polarity of the sentence. A model which
    combines some of the dominant features and some of the features which are not
    dominant was developed by Ma et al. using the LSTM model. This requires a memory  at
    the token level to be injected at the output gate separately. A model based on
    the theory of mathematics for the regression network was proposed by (Chen et
    al. [2018a](#CR5), [b](#CR6)). A combined framework to incorporate CNN and RNN
    was developed by Abid et al., ([2014](#CR1)). Opinion of the customer on the products
    they buy can be identified with the reviews they give for it, and this can be
    achieved by using sentiment analysis. With the help of the above opinion analysis,
    we can take advantages of this and can build some policies and directives that
    can be incorporated for taking better decisions at the business.
  prefs: []
  type: TYPE_NORMAL
- en: Liu et al. defined sentiment strength  by degree and punctuation adverbs and
    created the sentiment feature vectors in 2014 to examine the emotional polarity
    of product comments by dependency parsing. Wang et al. used statistical and point
    mutual data to mine and identify the new meanings and emotions of SinaWeibo, defined
    principles of emotional computing at different semantic levels, and conducted
    Chinese sentiment analysis by constructing an emotional dictionary and collection
    of rules in 2015 to incorporate emotional computing. A relationship between the
    product and its various attributes was developed by Liu et al. who also prepared
    a model to match the parts of speech pattern to extract the words of function
    and the words that describe emotions using the domain emotion ontology. To classify
    the text based on properties, with labeled inputs, the model is trained, and this
    learned model will predict the patters of sentiment. Various machine learning
    algorithms like maximum entropy (ME), Naïve Bayes, and SVM were used to train
    the data, and the result was achieved. The probability of sentiment in the words
    and the polarity of sentiment is calculated by the techniques that focus on deep
    learning derive valuable artificial text information.
  prefs: []
  type: TYPE_NORMAL
- en: A model developed  in the year 2017, for product review emotion analysis, the
    researcher attempted to use the convolution neural network (CNN) and achieved
    improved performance (Hu et al., [2017](#CR17)). A special model developed using
    RNN and LSTM can solve the problem of explosion of gradient values in RNN or the
    disappearance of gradient with one extra cell as compared to the traditional long
    short-term memory model and the RNN model.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, for e-commerce review sentiment analysis based on LSTM with bidirectional
    encoding, a model was developed in this paper. Coming to aspect-based sentiment
    analysis, mainly the tasks involved here are extraction of the aspect term, polarity
    recognition of terms in the aspect, recognition of aspect group, and finding the
    polarity of aspect categories (Hochreiter & Schmidhuber, [2017](#CR16)). A methodology
    based on frequency of occurrence of words was adopted by the authors  , which
    identifies and nominates the most frequent nouns as elements.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we have discussed polarity-based mechanism, GRU model combined
    with CNN, Bi-GRU layer, and LSTM model for sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.1 Polarity-Based Mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This method is widely used to identify the polarity of the text that is present
    in the reviews given by the user. The classification is done normally as positive,
    negative, and neutral. Positive statements are those which contain all the positive
    words in describing the products that they have purchased. Negative statements
    are those that consist of some words which give negative meaning. Neutral statements
    are those that are neither positive nor negative in meaning.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used for this method is reviews of products given by the users.
    The reviews of the products are collected from the Python Crawler. In the data
    collection step, the dataset to be used for research is identified. In this case
    the reviews of products that are purchased online by the user are considered.
    Secondly, in the step of pre-processing, boundary of the sentence is identified
    in the first phase, and after having verified, the boundary of the sentence in
    the text is tokenized into individual words. Deletion of stop words, removal of
    white spaces, removal of html tags, removal of emotions, and removal of special
    symbols are also included in the process of data pre-processing.
  prefs: []
  type: TYPE_NORMAL
- en: In the tokenization step, each word should be assigned with a token, and the
    score of that word is taken from the library called SentiWordNet, based on the
    token assigned. In the process of stemming, identical words that are present in
    the dataset considered are removed to make sure that there is no repetition of
    identical words in the reviews. At last, parts of speech tagging is done. The
    reviews given by the user may contain different parts of speech that were tagged
    using Natural Language Tool Kit (NLTK). All adverbs are extracted from the reviews.
    After this, score of the sentence and score of the reviews must be calculated,
    and score of the sentence in the review is computed by the score of the individual
    words that are there in the given particular sentence. The score of the review
    can be computed by computing the scores of the sentences that are there in a review.
  prefs: []
  type: TYPE_NORMAL
- en: To classify a review, it must be tagged using various adverbs and its variations.
    After the process of tagging, reviews and various forms of adverbs can be fetched.
  prefs: []
  type: TYPE_NORMAL
- en: Once these are extracted, those can be merged to get scores, using SentiWordNet.
    Initially, in the level of sentence, scores are assigned also at the level of
    review scores. Finally, at the end, the final scores of the reviews are acquired,
    and they are categorized with a rating of 5 star which consists of strongly negative,
    negative, neutral, positive, and strongly positive using classification algorithms.
    Here, we have used Naïve Bayes classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.2 GRU Combined with CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to provide high accuracy of sentiment analysis on the reviews of products
    procured online by the user, a combined model of CNN and GRU is used. Initially
    a sentiment lexicon is used for the reviews to boost the features given by users.
    Then the main features of sentiment are extracted by the CNN and GRU networks,
    and for weights, attention mechanism is used. At last the features of weighted
    sentiment can be categorized. The structure of the model is shown in Fig. [12.1](#Fig1).
    The model is made up of six layers: an embedded layer, a convolution layer, a
    pooled layer, a layer of Bi-GRU, an attention layer, and a layer which is fully
    connected.![../images/504166_1_En_12_Chapter/504166_1_En_12_Fig1_HTML.png](../images/504166_1_En_12_Chapter/504166_1_En_12_Fig1_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 12.1
  prefs: []
  type: TYPE_NORMAL
- en: Structure of GRU model combined with CNN
  prefs: []
  type: TYPE_NORMAL
- en: We have considered the input text which will consists of statements S, and each
    statement S consists of words; therefore *S* = {*w*[1], *w*[2], *w*[3], *w*[4]………...*w*[n]}
    where *w*[i] indicates a word in the statement *S*. The work here is to predict
    the polarity of the sentiment in the dataset which is nothing but finding polarity
    *P* for the statement *S* in the text. The sentiment lexicon gives each word *w*[i]
    in *S* a corresponding weight SW[i]. Various open-source sentiment dictionaries
    are used for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: First  we have removed the words of sentiment which are giving some neutral
    meaning and maintain the words of sentiment that are negative and derogatory,
    i.e., words that have polarities of 1 and 2 have to be retained. According to
    their strength, sentiment terms are classified into five groups, comprising 1,
    3, 5, 7, and 9, with the severity of the sentiment as the weight of the sentiment,
    and words of sentiment with sentiment polarity of negative multiply with the sentiment
    of weight by −1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment weight of word expression after construction is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![$$ \mathrm{Senti}\left({w}_{\mathrm{i}}\right)=\Big\{{\displaystyle \begin{array}{cc}{\mathrm{SW}}_{\mathrm{i}},&amp;
    {w}_{\mathrm{i}}\in \mathrm{SD}\\ {}1,&amp; {w}_{\mathrm{i}}\notin \mathrm{SD}\end{array}}
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ1.png)(12.1)where
    *w*[i] indicates a word and SW[i] indicates the weight of the word w[i] in the
    sentiment lexicon and sentiment lexicon represented by SD.'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding layer in this model represents the statement S in the dataset
    in the vector matrix as a weighted word. In this method BERT embedding model is
    used to train the word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '*V*[i] is a 768-dimensional vector, and with this BERT model, every word *w*[i]
    in statement *S* is converted to the word vector *V*[i]. Later, with sentiment
    weights, the weight of the word vector is checked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![$$ {V}_{\mathrm{i}}^{\prime }={V}_{\mathrm{i}}\ast \mathrm{senti}\left({w}_{\mathrm{i}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ2.png)(12.2)'
  prefs: []
  type: TYPE_IMG
- en: The output of the embedded layer will be a vector matrix consisting of weighted
    words. In the convolution layer, from the input matrix, important local features
    are extracted. A complete representation of the word vector is done in the area
    of natural language processing. So, the width of the kernel in the convolution
    layer consumes the entire region of the word vector. Next in the pooling layer,
    the text features acquired from the convolution layer are compressed, and the
    main features are extracted. The operations of pooling are split up as average
    pooling and max pooling. In the sentiment analysis of the text, some phrases or
    words will be influential that are present in the sentences; therefore k-max pooling
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.3 Bi-GRU Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coming to Bi-GRU layer, extraction of feature from the input matrix is the main
    task here. Another variation of the recurrent neural network is GRU model, and
    sequence of information is processed by this. Historical data of the earlier point
    of time which influences the current output and extraction of features of context
    data in sequence are combined in the method. In the sentences of text data, the
    current word is affected by both the word that is proceeding and also succeeding;
    therefore the extraction of the features of the context data of the input is done
    by Bi-GRU model. The Bi-GRU model is made up of forward GRU which process forward
    data and a reverse GRU that process reverse data. For the input text *x*[t] at
    time *t* and hidden states that are obtained by forward GRU is *H*[t]′ and hidden
    states that are obtained by reverse GRU is *H*[t]″, and these are given by
  prefs: []
  type: TYPE_NORMAL
- en: '![$$ {H}_{\mathrm{t}}^{\prime }=\left({x}_{\mathrm{t}},{H_{\mathrm{t}-1}}^{\prime}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ3.png)(12.3)![$$
    {H_{\mathrm{t}}}^{\prime \prime }=\left({x}_{\mathrm{t}},{H_{\mathrm{t}-1}}^{\prime
    \prime}\right) $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ4.png)(12.4)'
  prefs: []
  type: TYPE_IMG
- en: The combination of forward GRU and reverse GRU is considered to be the output
    at time *t*. In the attention layer, every word in the statements of the text
    has a significant impact on the sentimental polarity of the entire sentence. The
    sentence will contain some of the words which give a partial effect on the sentiment
    of the entire text; also, it may contain some of the words which do not contribute
    anything to the sentiment of that sentence. Therefore, making use of the attention
    layer, various weights can be assigned to various words in the given sentence.
    At last in the fully connected layer, the classification of the input feature
    matrix is done.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of this layer can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![$$ P=f\left(M\ast X\right)+D $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ5.png)(12.5)'
  prefs: []
  type: TYPE_IMG
- en: where *f* is indicating the function of activation sigmoid, *M* indicates the
    matrix of the weights, and *D* indicates the offset. In this layer input feature
    is depicted with a value in the interval [0, 1]. If that value is closer to 0,
    it indicates the sentiment polarity of the text closer to the negative direction,
    and on the other hand, if the value is closer to 1, it indicates that polarity
    of the sentence is toward positive polarity.
  prefs: []
  type: TYPE_NORMAL
- en: 12.3.4 Long Short-Term Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another method that is widely used for sentiment analysis is long short-term
    memory (LSTM) which is a distinct model in recurrent neural network (RNN) architecture
    which is structured more specifically to model temporal sequences and their long-range
    dependencies than traditional RNNs. To solve problems posed by the RNN model,
    LSTM can be used. Therefore, it can be used for solving issue with long-term dependence
    in RNN and gradient disappearance and gradient bursting. The soul of LSTM network
    is its cell which provides the LSTM network a small amount of memory that it can
    remember the past data. The structure of LSTM memory cell is shown in Fig. [12.2](#Fig2).
    LSTM network is made up of gates which have input gates, forget gates, and output
    gates. The gates in LSTM are nothing but a function of sigmoid activation function
    which means they give output a value in between 0 and 1\. In most of the cases,
    it will be either 0 or 1\. The equations for the gates in LSTM are:![../images/504166_1_En_12_Chapter/504166_1_En_12_Fig2_HTML.png](../images/504166_1_En_12_Chapter/504166_1_En_12_Fig2_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 12.2
  prefs: []
  type: TYPE_NORMAL
- en: Structure  of LSTM memory cell
  prefs: []
  type: TYPE_NORMAL
- en: '![$$ {I}_{\mathrm{t}}=\sigma \left({M}_{\mathrm{i}}\left[{P}_{\mathrm{t}-1},{Q}_{\mathrm{t}}\right]+{Z}_{\mathrm{i}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ6.png)(12.6)![$$
    {G}_{\mathrm{t}}=\sigma \left({M}_{\mathrm{f}}\left[{P}_{\mathrm{t}-1},{Q}_{\mathrm{t}}\right]+{Z}_{\mathrm{f}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ7.png)(12.7)![$$
    {K}_{\mathrm{t}}=\sigma \left({M}_{\mathrm{o}}\left[{P}_{\mathrm{t}-1},{Q}_{\mathrm{t}}\right]+{Z}_{\mathrm{o}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ8.png)(12.8)where
    *I*[t] indicates the input gate, *G*[t] indicates the forget gate, *K*[t] indicates
    the output gate, *σ* indicates the sigmoid function, *M*[x] indicates the weight
    for the respective gates(*x*), *H*[t-1] indicates the output of the previous block
    (at time stamp t-1), *Q*[t] indicates the input at current time stamp, and *Z*[x]
    indicates the biases for respective gates(*x*).'
  prefs: []
  type: TYPE_NORMAL
- en: The equation is used for input gate that gives the information of the new data
    that is going to be stored in the state of the cell. The second equation is written
    for the forget gate that gives the information to be removed away from the state
    of the cell. The third equation is used for the output gate that is used to furnish
    the final output of the block of LSTM at time “*t*.”
  prefs: []
  type: TYPE_NORMAL
- en: 'We shall give the equations of the state of the cell, state of the candidate
    cell, and the final output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![$$ {C_{\mathrm{t}}}^{\prime }=\tanh \left({M}_{\mathrm{c}}\left[{P}_{\mathrm{t}-1},{Q}_{\mathrm{t}}\right]+{Z}_{\mathrm{c}}\right)
    $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ9.png)(12.9)![$$
    {C}_{\mathrm{t}}={G}_{\mathrm{t}}\ast {C}_{\mathrm{t}-1}+{I}_{\mathrm{t}}\ast
    {C_{\mathrm{t}}}^{\prime } $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ10.png)(12.10)![$$
    {P}_{\mathrm{t}}={K}_{\mathrm{t}}\ast \tanh \left({C}^{\mathrm{t}}\right) $$](../images/504166_1_En_12_Chapter/504166_1_En_12_Chapter_TeX_Equ11.png)(12.11)where
    *C*[t] indicates the cell state in the memory at time stamp *t*, *C*[t]′ indicates
    the candidate for cell state at time stamp *t*, *I*[t] indicates the input gate,
    *G*[t] indicates the forget gate, *K*[t] indicates  the output gate, *M*[c] indicates
    the weight for the respective gates(*x*), *P*[t-1] indicates the output of the
    previous block (at time stamp t-1), *Q*[t] indicates the input at current time
    stamp, and *Z*[c] indicates the biases for respective gates(*x*).'
  prefs: []
  type: TYPE_NORMAL
- en: At time “*t*” with the above equations, it can be seen that the state of the
    cell will have the information of what the former states ought to forget and the
    information that needs to be considered from the current time “*t*.” At last,
    the state of the cell needs to be filtered, and then it has to pass through the
    function of activation that makes the prediction of what part must appear as the
    output of the current unit of LSTM at time “*t*.” Then the output of the current
    block of LSTM is passed to the Softmax layer to get the output that is predicted
    from the current block of LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we have discussed the case study on Amazon product reviews.
    First, we have described the dataset, then in the next part, the novel model developed
    for sentiment analysis of Amazon product reviews is explained.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.1 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dataset  we have used is taken from Kaggle website. We have considered rating
    for the products given by the user from 1 to 5\. Reviews that has rating for the
    product 3–5 are considered to be positive reviews, and reviews that have rating
    for the product 1–2 are considered to be negative reviews. Then we perform manual
    screening on the dataset to ensure the reviews under positive dataset are all
    positive and the reviews under negative dataset are all negative. Our dataset
    consisted of total records 3000\. Out of these 1500 are positive reviews and 1500
    are negative reviews. We have divided this dataset into two parts. Seventy percent
    of the records in the dataset that constitute 2100 records are used for training
    the model, and 30% of the records in the dataset that constitute  of 900 records
    are used for testing the model.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of the novel model developed  is as shown in Fig. [12.3](#Fig3).![../images/504166_1_En_12_Chapter/504166_1_En_12_Fig3_HTML.png](../images/504166_1_En_12_Chapter/504166_1_En_12_Fig3_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 12.3
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of the model using stacked embedding
  prefs: []
  type: TYPE_NORMAL
- en: The architecture consists  of preprocessing unit, embedding layer, and Bi-GRU
    model with attention layer.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.2 Data Preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our corpus has unclean data  , so before we offer it as an input for our model,
    we have cleaned it. During the cleaning process, we deleted all redundant data
    and null data, and all HTML characters are removed. Our dataset included some
    of the complicated symbols that we decoded and made clear understandable things,
    removed all the punctuation marks, and removed all the stop words. Then the cleaned
    data is fed into the tokenizer for tokenization; for this we used Natural Language
    Toolkit (NLTK) tokenizer from SK library. Further, the tokenized data is given
    for embedding  .
  prefs: []
  type: TYPE_NORMAL
- en: 12.4.3 Stacked Embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have stacked  two context-based embedding approaches such as BERT and GloVe
    in our experiment with the help of Flair Library. “BERT-base-uncased” with an
    embedding dimension of 768 and a GloVe with a vector dimension of 100 are the
    underlying option templates for BERT and GloVe, respectively. The resultant embeddings
    are given as input to Bi-GRU model.
  prefs: []
  type: TYPE_NORMAL
- en: The next step  , the output from embedding layer is provided to the bidirectional
    GRU model. This model uses knowledge from both the previous steps and the next
    steps to predict the current state. In our experiment, we used Bi-GRU with three
    hidden layers of size 120, 60, and 30, respectively. The output of the third layer
    is given to the attention layer. The attention layer assigns different weights
    to different words depending on the influence of the sentiment on each word. There
    might be some words which may not contribute to the sentiment of any words, or
    they may influence less. Based on this weight must be assigned. The output of
    the attention layer is again transferred through the dense layer to reduce dimensionality.
    At last, the output from a dense layer is given to a sigmoid layer to classify
    our corpus in either a positive or a negative class. The findings of our model
    are seen in the following section  .
  prefs: []
  type: TYPE_NORMAL
- en: 12.5 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Findings from our proposed model reveal that the accuracy of our model is 95.32%,
    with an F1 score of 95%. In addition, we have tried to boost our model performance
    by using k-fold cross-validation  . Results indicate that the model performance
    with k-fold cross-validation increases slightly. Here, tenfold cross-validation
    has been used. Table [12.1](#Tab1) displays the outcomes.Table 12.1
  prefs: []
  type: TYPE_NORMAL
- en: F1 score and accuracy of the model
  prefs: []
  type: TYPE_NORMAL
- en: '| Accuracy | F1 score | Tenfold cross-validation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | F1 score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 95.32% | 95.10% | 95.92% | 95.89% |'
  prefs: []
  type: TYPE_TB
- en: We have tried to explore the impact of the number of iterations and dropout
    values on the output of our model. We find that our model would reach maximum
    efficiency at eight iterations with a dropout value of 0.4\. Later output will
    continue to decline. Tables [12.2](#Tab2) and [12.3](#Tab3) demonstrate the impact
    of iteration number and dropout value on the output of our model.Table 12.2
  prefs: []
  type: TYPE_NORMAL
- en: The effect of the number of iterations in the model
  prefs: []
  type: TYPE_NORMAL
- en: '| Iteration | Accuracy | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 95.3% | 95.1% |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 95.5% | 95.2% |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 95.9% | 95.8% |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 95.3% | 95.2% |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 95.0% | 95.1% |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 94.6% | 94.5% |'
  prefs: []
  type: TYPE_TB
- en: Table 12.3
  prefs: []
  type: TYPE_NORMAL
- en: The effect of dropout value in the model
  prefs: []
  type: TYPE_NORMAL
- en: '| Dropout | Accuracy | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.2 | 95.1% | 95.1% |'
  prefs: []
  type: TYPE_TB
- en: '| 0.4 | 95.8% | 95.6% |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 95.5% | 95.2% |'
  prefs: []
  type: TYPE_TB
- en: '| 0.6 | 95.5% | 95.5% |'
  prefs: []
  type: TYPE_TB
- en: '| 0.8 | 94.7% | 94.8% |'
  prefs: []
  type: TYPE_TB
- en: The accuracy of the model was calculated to be 95.32%. The model was more accurate
    than the models produced using other algorithms. The novelty of stacked embedding
    developed by the Flair Library has contributed to the accuracy of the findings.
  prefs: []
  type: TYPE_NORMAL
- en: 12.6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As there is a tremendous growth in the online business platforms, sentiment
    analysis has got a great importance for the reviews of the products given by the
    users. In this paper, we have developed a model using stacked embedding for sentiment
    analysis of Amazon product reviews. Initially, various methods like polarity-based
    sentiment classification, long short-term memory (LSTM), and bidirectional GRU
    models have been explained. Taking the case study into consideration, we have
    developed a model in which we have considered specifically Amazon product reviews
    as the case study since it is a popular e-commerce platform. We have developed
    a stacked embedding model in which Flair Library is used to implement GloVe and
    BERT embedding models. Then the bidirectional GRU model is used to extract the
    features of the sentiment in the text sentences of reviews. Then at attention
    layer, different weights are assigned to different words depending on the contribution
    of their sentiment on each word. Then these are passed to sigmoid layer through
    the dense layer. Making use of sigmoid function, the model classifies the statements
    of reviews to be either positive or negative by assigning the values either 0
    or 1\. A dataset consisting of 3000 samples was considered, for which our model
    has achieved a highest accuracy compared to other models. With the increase in
    the dataset given to the model, it will achieve consistently improved accuracy.
    The model we have developed can be used for various applications of sentiment
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: A further study on fineness of the sentiment must be done to meet the requirements
    of the areas where a higher level of refinement of sentiment analysis is expected.
  prefs: []
  type: TYPE_NORMAL
