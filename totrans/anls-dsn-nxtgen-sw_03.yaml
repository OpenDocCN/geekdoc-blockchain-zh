- en: © Springer Nature Switzerland AG 2020A. M. LangerAnalysis and Design of Next-Generation
    Software Architectures[https://doi.org/10.1007/978-3-030-36899-9_3](https://doi.org/10.1007/978-3-030-36899-9_3)
  prefs: []
  type: TYPE_NORMAL
- en: 3. Reviewing the Object Paradigm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Arthur M. Langer^([1](#Aff2) [ ](#ContactOfAuthor2))(1)Center for Technology
    Management, Columbia University, New York, NY, USAArthur M. LangerEmail: [al261@columbia.edu](mailto:al261@columbia.edu)'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will provide the historical structured analysis and design methodology
    that led to the object paradigm. At the core of an evolutionary approach are a
    set of traditional tools that need to be extended to meet the needs of an agile
    architecture in a mobile IoT market.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The Concept of the Logical Equivalent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary mission of an analyst or systems designer is to extract the physical
    requirements of the users and convert them to software. All software can trace
    its roots to a physical act or a physical requirement. A physical act can be defined
    as something that occurs in the interaction of people, that is, people create
    the root requirements of most systems, especially those in business. For example,
    when Mary tells us that she receives invoices from vendors and pays them thirty
    days later, she is explaining her physical activities during the process of receiving
    and paying invoices. When the analyst creates a technical specification, which
    represent Mary’s physical requirements, the specification is designed to allow
    for the translation of her physical needs into an automated environment. We know
    that software must operate within the confines of a computer, and such systems
    must function on the basis of logic. The logical solution does not always treat
    the process using the same procedures employed in the physical world. In other
    words, the software system implemented to provide the functions which Mary does
    physically will probably work differently and more efficiently than Mary herself.
    Software, therefore, can be thought of as a logical equivalent of the physical
    world. This abstraction, which I call the concept of the Logical Equivalent(LE),
    is a process that analysts must use to create effective requirements of the needs
    of a system. The LE can be compared to a schematic of a plan or a diagram of how
    a technical device works.
  prefs: []
  type: TYPE_NORMAL
- en: Any success in creating a concise and accurate schematic of software that needs
    to be developed by a programmer will be directly proportional to how well the
    analyst masters Langer’s ([1997](#CR2)) Concept of the Logical Equivalent. Very
    often requirements are developed by analysts using various methods that do not
    always contain a basis for consistency, reconciliation and maintenance. There
    is usually far too much prose used as opposed to specific diagramming standards
    that are employed by engineers. After all, we are engineering a system through
    the development of software applications. The most critical step in obtaining
    the LE is the understanding of the process of Functional Decomposition. Functional
    Decomposition is the process for finding the most basic parts of a system, like
    defining all the parts of a car so that it can be built. It would be possible
    not from looking at a picture of the car, but rather at a schematic of all the
    functionally decomposed parts. Developing and engineering software is no different
    and essential to create reusable component applications that operate in the IoT
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Below is an example of an analogous process using functional decomposition,
    with its application to the LE.
  prefs: []
  type: TYPE_NORMAL
- en: In obtaining the physical information from the user, there are a number of modeling
    tools that can be used. Each tool provides a specific function to derive the LE.
    The word “derive” has special meaning here. It relates to the process of Long
    Division, or the process or formula we apply when dividing one number by another.
    Consider the following example:![../images/480347_1_En_3_Chapter/480347_1_En_3_Figa_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Figa_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: The above example shows the formula that is applied to a division problem. We
    call this formula long division. It provides the answer, and if we change any
    portion of the problem, we simply re-apply the formula and generate a new result.
    Most important, once we have obtained the answer, the value of the formula steps
    is only one of documentation. That is, if someone questioned the validity of the
    result, we could show them the formula to prove that the answer was correct (based
    on the input).
  prefs: []
  type: TYPE_NORMAL
- en: Now let us apply long division to obtaining the LE via functional decomposition.
    The following is a result of an interview with Joe, a bookkeeper, about his physical
    procedure for handling bounced checks.
  prefs: []
  type: TYPE_NORMAL
- en: Joe the bookkeeper receives bounced checks from the bank. He fills out a Balance
    Correction Form and forwards it to the Correction Department so that the outstanding
    balance can be corrected. Joe sends a bounced check letter to the customer requesting
    a replacement check plus a $15.00 penalty (this is now included as part of the
    outstanding balance). Bounced checks are never re-deposited.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The appropriate modeling tool to use in this situation is a Data Flow Diagram
    (DFD). A DFD is a tool that shows how data enters and leaves a particular process.
    The process we are looking at with Joe is the handling of the bounced check. A
    DFD has four possible components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/480347_1_En_3_Chapter/480347_1_En_3_Figb_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Figb_HTML.png)
    Process:'
  prefs: []
  type: TYPE_NORMAL
- en: This denotes the name of the actual function being performed. A valid process
    is one in which data is transformed from one form to another.
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/480347_1_En_3_Chapter/480347_1_En_3_Figc_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Figc_HTML.png)
    Data Flow:'
  prefs: []
  type: TYPE_NORMAL
- en: This represents data entering or leaving a Process, External or Data Store.
    The arrow denotes direction of the flow. A data flow is sometimes called “data-in-motion.”
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/480347_1_En_3_Chapter/480347_1_En_3_Figd_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Figd_HTML.png)
    Data Store:'
  prefs: []
  type: TYPE_NORMAL
- en: Stored data usually kept in a file. It represents data that can be accessed
    from a specific area. A data store is sometimes called “data-at-rest.”
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/480347_1_En_3_Chapter/480347_1_En_3_Fige_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fige_HTML.png)
    External:'
  prefs: []
  type: TYPE_NORMAL
- en: A provider or user of the data that is not part of the system. It therefore
    represents a boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us draw the LE of Joe’s procedure using DFD tools as shown in Fig. [3.1](#Fig1).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig1_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig1_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.1
  prefs: []
  type: TYPE_NORMAL
- en: Data flow diagram for handling bounced checks
  prefs: []
  type: TYPE_NORMAL
- en: The above DFD shows that bounced checks arrive from the bank, the Account Master
    file is updated, the Correction Department is informed and Customers receive a
    letter. The Bank, Correction Department and Customers are considered “outside”
    the system and are therefore represented logically as Externals. This diagram
    is considered to be at the first level or “Level 1” of functional decomposition.
    You will find that all modeling tools employ a method to functionally decompose.
    DFDs use a method called “Leveling.”
  prefs: []
  type: TYPE_NORMAL
- en: The question is whether we have reached the most basic parts of this process
    or should we level further. Many analysts suggest that a fully decomposed DFD
    should have only one data flow input and one data flow output. Our diagram currently
    has many inputs and outputs and therefore it can be leveled further. The result
    of functionally decomposing to the second level (Level 2) is as shown in Fig. [3.2](#Fig2).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig2_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig2_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.2
  prefs: []
  type: TYPE_NORMAL
- en: Level 2 data flow diagram for handling bounced checks
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the functional decomposition shows us that Process 1: Handling
    Bounced Checks is really made up of two sub-processes called 1.1 Update Balance
    and 1.2 Send Letter. The box surrounding the two processes within the Externals
    reflects them as components of the previous or parent level. The double-sided
    arrow in Level 1 is now broken down to two separate arrows going in different
    directions because it is used to connect Processes 1.1 and 1.2\. The new level
    is more functionally decomposed and a better representation of the LE.'
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we must ask ourselves whether Level 2 can be further decomposed.
    The answer is yes. Process 1.1 has two outputs to one input. On the other hand,
    Process 1.2 has one input and one output and is therefore complete. 1.2 is said
    to be at the Functional Primitive, a DFD that cannot be decomposed further. Therefore,
    only 1.1 will be decomposed.
  prefs: []
  type: TYPE_NORMAL
- en: Let us decompose 1.1 as depicted in Fig. [3.3](#Fig3).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig3_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig3_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.3
  prefs: []
  type: TYPE_NORMAL
- en: Level 3 data flow diagram for handling bounced checks
  prefs: []
  type: TYPE_NORMAL
- en: 'Process 1.1 is now broken down into two sub processes: 1.1.1 Update Account
    Master and 1.1.2 Inform Correction Department. Process 1.1.2 is a Functional Primitive
    since it has one input and one output. Process 1.1.1 is also considered a Functional
    Primitive because the “Bounced Check Packet” flow is between the two processes
    and is used to show connectivity only. functional decomposition is at Level-3
    and is now complete.'
  prefs: []
  type: TYPE_NORMAL
- en: The result of functional decomposition is the following DFD (Fig. [3.4](#Fig4)).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig4_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig4_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.4
  prefs: []
  type: TYPE_NORMAL
- en: Functionally decomposed level 3 data flow diagram for handling bounced checks
  prefs: []
  type: TYPE_NORMAL
- en: As in long division, only the complete result, represented above, is used as
    the answer. The preceding steps are formulas that we use to get to the lowest,
    simplest representation of the logical equivalent. Levels 1, 2 and 3 are used
    only for documentation of how the final DFD was determined.
  prefs: []
  type: TYPE_NORMAL
- en: The logical equivalent is an excellent method that allows analysts and systems
    designers to organize information obtained from users and to systematically derive
    the most fundamental representation of their process. It also alleviates unnecessary
    pressure to immediately understand the detailed flows and provides documentation
    of how the final schematic was developed.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Tools of Structured Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have established the importance and goals of the logical equivalent,
    we can turn to a discussion of the methods available to assist the analyst. These
    methods serve as the tools to create the best models in any given situation, and
    thus the most exact logical equivalent. The tools of the analyst are something
    like those of a surgeon, who uses only the most appropriate instruments during
    an operation. It is important to understand that the surgeon is sometimes faced
    with choices about which surgical instruments to use; particularly with new procedures,
    there is sometimes disagreement among surgeons about which instruments are the
    most effective. The choice of tools for analysis and data processing is no different;
    indeed, it can vary more and be more confusing. The medical profession, like many
    others, is governed by its own ruling bodies. The American Medical Association
    and the American College of Physicians and Surgeons, as well as state and federal
    regulators, represent a source of standards for surgeons. Such a controlling body
    does not exist in the data processing industry, nor does it appear likely that
    one will arise in the near future. Thus, the industry has tried to standardize
    among its own leaders. The result of such efforts has usually been that the most
    dominant companies and organizations create standards to which others are forced
    to comply. For example, Microsoft has established itself as an industry leader
    by virtue of its software domination. Here, Might is Right!
  prefs: []
  type: TYPE_NORMAL
- en: Since there are no real formal standards in the industry, the analysis tools
    discussed here will be presented on the basis of both their advantages and their
    shortcomings. It is important then to recognize that no analysis tool (or methodology
    for that matter) can do the entire job, nor is any perfect at what it does. To
    determine the appropriate tool, analysts must fully understand the environment,
    the technical expertise of users and the time constraints imposed on the project.
    By “environment” we mean the existing system and technology, computer operations,
    and the logistics–both technically and geographically–of the new system. The treatment
    of the user interface should remain consistent with the guidelines discussed in
    Chap. [2](480347_1_En_2_Chapter.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: The problem of time constraints is perhaps the most critical of all. The tools
    you would ideally like to apply to a project may not fit the time frame allotted.
    What happens, then, if there is not enough time? The analyst is now faced with
    selecting a second-choice tool that undoubtedly will not be as effective as the
    first one would have been. There is also the question of how tools are implemented,
    that is, can a hybrid of a tool be used when time constraints prevent full implementation
    of the desired tool?
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Making Changes and Modifications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Within the subject of analysis tools is the component of maintenance modeling,
    or how to apply modeling tools when making changes or enhancements to an existing
    product. Maintenance modeling falls into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pre-modeled: where the existing system already has models that can be used
    to effect the new changes to the software.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Legacy System: where the existing system has never been modeled; any new modeling
    will therefore be incorporating analysis tools for the first time.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Pre*-*modeled*:'
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, a Pre-Modeled product is already in a structured format. A structured
    format is one that employs a specific format and methodology such as the data
    flow diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most challenging aspects of changing Pre-Modeled tools are:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: keeping them consistent with their prior versions, and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: implementing a version control system that provides an audit-trail of the analysis
    changes and how they differ from the previous versions. Many professionals in
    the industry call this Version Control; however, care should be taken in specifying
    whether the version control is used for the maintenance of analysis tools. Unfortunately,
    Version Control can be used in other contexts, most notably in the tracking of
    program versions and software documentation. For these cases, special products
    exist in the market which provide special automated “version control” features.
    We are not concerned here with these products but rather with the procedures and
    processes that allow us to incorporate changes without losing the prior analysis
    documentation. This kind of procedure can be considered consistent with the long
    division example in which each time the values change, we simply re-apply the
    formula (methodology) to calculate the new answer. Analysis version control must
    therefore have the ability to take the modifications made to the software and
    integrate them with all the existing models as necessary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Being Consistent*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is difficult to change modeling methods and/or CASE tools in the middle
    of the life cycle of a software product. One of our main objectives then is to
    try avoid doing so. How? Of course, the simple answer is to select the right tools
    and CASE software the first time. However, we all make mistakes, and more importantly,
    there are new developments in systems architecture that may make a new CASE product
    attractive. You would be wise to foresee this possibility and prepare for inconsistent
    tools implementation. The best offense here is to:'
  prefs: []
  type: TYPE_NORMAL
- en: ensure that your CASE product has the ability to transport models through an
    ASCII file or cut/paste method. Many have interfaces via an “export” function.
    Here, at least, the analyst can possibly convert the diagrams and data elements
    to another product.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: keep a set of diagrams and elements that can be used to establish a link going
    forward, that is, a set of manual information that can be re-input to another
    tool. This may be accomplished by simply having printed documentation of the diagrams;
    however, experience has shown that it is difficult to keep such information up
    to date. Therefore, the analyst should ensure that there is a procedure for printing
    the most current diagrams and data elements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should the organization decide to use different tools, e.g., process-dependency
    diagrams instead of data flow diagrams, or a different methodology such as crows-foot
    method in Entity Relational Diagramming, then the analyst must implement a certain
    amount of re-engineering. This means mapping the new modeling tools to the existing
    ones to ensure consistency and accuracy. This is no easy task, and it is strongly
    suggested that you document the diagrams so you can reconcile them.
  prefs: []
  type: TYPE_NORMAL
- en: '*Version Control*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This book is not intended to focus on the generic aspects of version control;
    however, structured methods must have audit trail. When a new process is changed,
    a directory should be created for the previous version. The directory name typically
    consists of the version and date such as: xyz1.21295, where xyz is the name of
    the product or program, 1.2 the version and 1295 the version date. In this way
    previous versions can be easily re-created or viewed. Of course, saving a complete
    set of each version may not be feasible or may be too expensive (in terms of disk
    space, etc.). In these situations, it is advisable to back up the previous version
    in such a manner as to allow for easy restoration. In any case, a process must
    exist, and it is crucial that there be a procedure to do backups periodically.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 What is Object-Oriented Analysis?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object-Oriented Analysis is the key analysis tool in the design of successful
    mobile applications. It is without question the most important element of creating
    what may be called the “complete” requirement agile system. There are a number
    of approaches used by the industry and perhaps controversy about the best approach
    and tools that should be used to create mobile-object systems. This chapter will
    focus on developing the requirements for object systems and the challenges of
    converting legacy systems. Therefore, many of the terms will be defined based
    on their fundamental capabilities and how they can be used by a practicing analyst
    (as opposed to a theorist!).
  prefs: []
  type: TYPE_NORMAL
- en: 'Object Orientation (OO) is based on the concept that every requirement ultimately
    must belong to an object. It is therefore critical that we first define what is
    meant by an object. In the context of OO analysis, an object is any cohesive whole
    made up of two essential components: data and processes (Fig. [3.5](#Fig5)).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig5_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig5_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.5
  prefs: []
  type: TYPE_NORMAL
- en: A car is an example of a physical object
  prefs: []
  type: TYPE_NORMAL
- en: Traditional analysis approaches were traditionally based on the examination
    of a series of events. We translated these events from the physical world by first
    interviewing users and then developing what was introduced as the concept of the
    logical equivalent. Although we are by no means abandoning this necessity, the
    OO paradigm requires that these events belong to an identifiable object. Let us
    expand on this difference using the object shown below, an object we commonly
    call a “car.”
  prefs: []
  type: TYPE_NORMAL
- en: The above car may represent a certain make and model, but it also contains common
    components that are contained in all cars (e.g., an engine). If we were to look
    upon the car as a business entity of an organization, we might find that the following
    three systems were developed over the years.
  prefs: []
  type: TYPE_NORMAL
- en: 'The above diagram shows us that the three systems were built over a period
    of 21 years. Each system was designed to provide service to a group of users responsible
    for particular tasks. The diagram shows that the requirements for System 1 were
    based on the engine and front-end of the car. The users for this project had no
    interest in or need for any other portions of the car. System 2, on the other
    hand, focused on the lower center and rear of the car. Notice, however, that System
    2 and System 1 have an overlap. This means that there are parts and procedures
    common to both systems. Finally, System 3 reflects the upper center and rear of
    the car and has an overlap with System 2\. It is also important to note that there
    are components of the car that have not yet been defined, probably because no
    user has had a need for them. We can look at the car as an object and Systems
    1–3 as the software which has so far been defined about that object. Our observations
    should also tell us that the entire object is not defined and more important,
    that there is probable overlap of data and functionality among the systems that
    have been developed. This case exemplifies the history of most development systems.
    It should be clear that the users who stated their requirements never had any
    understanding that their own situation belonged to a larger composite object.
    Internal users tend to establish requirements based on their own job functions
    and their own experiences in those functions. Therefore, the analyst who interviews
    users about their events is exposed to a number of risks:'
  prefs: []
  type: TYPE_NORMAL
- en: Users tend to identify only what they have experienced, rather than speculating
    about other events that could occur. This is a significant limitation in the mobile
    world and in attempting to understand what consumer *may* want in the future.
    We know that such events can take place, although they have not yet occurred (you
    should recall the discussion of using STDs as a modeling tool to identify unforeseen
    possibilities). Consider, for example, an analysis situation in which $50,000
    must be approved by the firm’s Controller. This event might show only the approval,
    not the rejection. The user’s response is that the Controller, while examining
    the invoices, has never rejected one and therefore no rejection procedure exists.
    You might ask why. Well, in this case the Controller was not reviewing the invoices
    for rejection but rather holding them until he/she was confident that the company’s
    cash flow could support the issuance of these invoices. Obviously, the Controller
    could decide to reject an invoice. In such a case, the software would require
    a change to accommodate this new procedure. From a software perspective we call
    this a system enhancement, and it would result in a modification to the existing
    system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other parts of the company may be affected by the Controller’s review of the
    invoices. Furthermore, are we sure that no one else has automated this process
    before? One might think such prior automation could never be overlooked, especially
    in a small company, but when users have different names for the same thing (remember
    Customer and Client!) it is very likely that such things will occur. In this example
    there were two situations where different systems overlapped in functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There will be conflicts between the systems with respect to differences in data
    and process definitions. Worst of all, these discrepancies may not be discovered
    until years after the system is delivered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The above example shows us that requirements obtained based on an individual’s
    events require another level of reconciliation to ensure they are accurate. Requirements
    are said to be “complete” when they define the whole object. The more incomplete
    they are, the more modifications likely will be required later. The more modifications
    in a system, the higher the likelihood that data and processes across applications
    may conflict with each other. Ultimately this results in a less dependable, lower
    quality system. Most of all, event analysis alone is prone to missing events that
    users have never experienced. This situation is represented in the car example
    by the portions of the car not included in any of the three systems. System functions
    and components may also be missed because users are absent or unavailable at the
    time of the interviews, or because no one felt the need to automate a certain
    aspect of the object. In either case, the situation should be clear. We need to
    establish objects prior to doing event analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Before we discuss the procedures for identifying an object, it is worth looking
    at the significant differences between the object approach and earlier approaches.
    The first major systems were developed in the 1960s and were called Batch, meaning
    that they typically operated on a transaction basis. Transactions were collected
    and then used to update a master file. Batch systems were very useful in the financial
    industries, including banks. We might remember having to wait until the morning
    after a banking transaction to see our account balance because a batch process
    updated the master account files overnight. These systems were built based on
    event interviewing, where programmer/analysts met with users and designed the
    system. Most of these business systems were developed and maintained using COBOL.
  prefs: []
  type: TYPE_NORMAL
- en: In the early seventies, the new buzz word was “on-line, real-time” meaning that
    many processes could now update data immediately or on a “real-time” basis. Although
    systems were modified to provide these services, it is important to understand
    that they were not re-engineered. That is, the existing systems, which were based
    on event interviews, were modified, but not redesigned.
  prefs: []
  type: TYPE_NORMAL
- en: In the late 80s and early 90s the hot term became “Client/Server.” These systems,
    which will be discussed later, are based on sophisticated distributed systems
    concepts. Information and processes are distributed among many Local and Wide
    Area Networks. Many of these client/server systems are re-constructions of the
    on-line real-time systems which in turn were developed from the 1960s batch systems.
    The point here is that we have been applying new technology to systems that were
    designed over 30 years ago without considering the obsolescence of the design.
  prefs: []
  type: TYPE_NORMAL
- en: Through these three generations of systems, the analyst has essentially been
    on the outside looking in (see Fig. [3.6](#Fig6)). The completeness of the analysis
    was dependent upon–and effectively dictated by–the way the inside users defined
    their business needs.![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig6_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig6_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.6
  prefs: []
  type: TYPE_NORMAL
- en: Requirements are often developed by analysts from an outside view. The specifications
    are therefore dependent on the completeness of the user’s view
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig7_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig7_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3.7
  prefs: []
  type: TYPE_NORMAL
- en: This diagram reflects the three systems developed to support the car object
  prefs: []
  type: TYPE_NORMAL
- en: OO, on the other hand, requires that the analyst have a view from the inside
    looking out. What we mean here is that the analyst first needs to define the generic
    aspects of the object and then map the user views to the particular components
    that exist within the object itself (Fig. [3.7](#Fig7)). The diagram below shows
    a conceptual view of the generic components that could be part of a bank.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [3.8](#Fig8) shows the essential functions of the bank. The analyst is
    on the inside of the organization when interviewing users and therefore will have
    the ability to map a particular requirement to one or more of its essential functions.
    In this approach, any user requirement must fit into at least one of the essential
    components. If a user has a requirement that is not part of an essential component,
    then it must be either qualified as missing (and thus added as an essential component)
    or rejected as inappropriate.![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig8_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig8_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.8
  prefs: []
  type: TYPE_NORMAL
- en: Using the object approach, the analyst interviews users from the inside looking
    out
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of taking user requirements and placing each of their functions
    into the appropriate essential component can be called mapping. The importance
    of mapping is that functions of requirements are logically placed where they generically
    belong, rather than according to how they are physically implemented. For example,
    suppose Joseph, who works for a bank, needed to provide information to a customer
    about the bank’s investment offerings. Joseph would need to access investment
    information from the system. If OO methods were used to design the system, all
    information about banking investments would be grouped together generically. Doing
    it this way allows authorized personnel to access investment information regardless
    of what they do in the bank. If event analysis alone was used, Joseph would probably
    have his own subsystem that defines his particular requirements for accessing
    investment information. The problem here is twofold: first, the subsystem does
    not contain all of the functions relating to investments. Should Joseph need additional
    information, he may need an enhancement or need to use someone else’s system at
    the bank. Second, Joseph’s subsystem may define functions that have already been
    defined elsewhere in another subsystem. The advantage of OO is that it centralizes
    all of the functions of an essential component and allows these functions to be
    “reused” by all processes that require its information. The computer industry
    calls this capability Reusable Objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Identifying Objects and Classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most important challenge of successfully implementing OO is the ability
    to understand and select Objects. We have already used an example which identified
    a car as an object. This example is what can be called the tangible object, or
    as the industry calls them “physical objects.” Unfortunately, there is another
    type of object called an “abstract” or intangible object. An intangible object
    is one that you cannot touch or as Grady Booch originally described: “something
    that may be apprehended intellectually…Something towards which thought or action
    is directed.”^([1](#Fn1)) An example of an intangible object is the security component
    of the essentials of the bank. In many instances OO analysis will begin with identifying
    tangible objects which will in turn make it easier to discover the intangible
    ones.'
  prefs: []
  type: TYPE_NORMAL
- en: OO is somewhat consistent with the architecture of process and data in that
    all objects contain their own data and processes, called attributes and services,
    respectively. Attributes are effectively a list of data elements which are permanent
    components of the object. For example, a steering wheel is a data element that
    is a permanent attribute of the object “Car.” The services (or operations), on
    the other hand, define all of the processes that are permanently part or “owned”
    by the object. “Starting the Car” is a service that is defined within the object
    car. This service contains the algorithms necessary to start a car. Services are
    defined and invoked through a method. A method is a process specification for
    an operation (service).^([2](#Fn2)) For example, “Driving the Car” could be a
    method for the car object. The Driving the Car” method would invoke a service
    called “Starting the Car” as well as other services until the entire method requirement
    is satisfied. Although a service and method can have a one-to-one relationship,
    it is more likely that a service will be a subset or be one of the operations
    that make up a method.
  prefs: []
  type: TYPE_NORMAL
- en: Objects have the ability to inherit attributes and methods from other objects
    when they are placed within the same class. A class is a group of objects that
    have similar attributes and methods and typically have been put together to perform
    a specific task. To further understand these concepts, we will establish the object
    for “Car” and place it in a class of objects that focuses on the use of transmissions
    in cars.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3.9](#Fig9) represents an object class called Car Transmissions. It
    has three component objects: cars, automatic trans, and standard trans. The car
    object is said to be the parent object. Automatic trans and standard trans are
    object types. Both automatic trans and standard trans will inherit all attributes
    and services from their parent object cars. Inheritance in object technology means
    that the children effectively contain all of the capabilities of their parents.
    Inheritance is implemented as a tree structure^([3](#Fn3)); however, instead of
    information flowing upward (as is the case in tree structures), the data flows
    downward to the lowest level children. Therefore, an object inheritance diagram
    is said to be an inverted tree. Because the lowest level of the tree inherits
    from everyone’s of its parents, only the lowest level object need be executed,
    that is, executing the lowest level will automatically allow the application to
    inherit all of the parent information and applications as needed. We call the
    lowest level objects concrete, while all others in the class are called abstract.
    Objects within classes can change simply by the addition of a new object. Let
    us assume that there is another level added to our example. The new level contains
    objects for the specific types of automatic and standard transmissions.![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig9_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig9_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.9
  prefs: []
  type: TYPE_NORMAL
- en: Class car transmissions
  prefs: []
  type: TYPE_NORMAL
- en: 'The class in Fig. [3.10](#Fig10) has been modified to include a new concrete
    layer. Therefore, the automatic trans object and standard trans object are now
    abstract. The new four concrete objects not only inherit from their respective
    parent objects, but also from their common grandparent, cars. It is also important
    to recognize that classes can inherit from other classes. Therefore, the same
    example could show each object as a class: that is, cars would represent a class
    of car objects and automatic trans another class of objects. Therefore, the class
    automatic trans would inherit from the cars class in the same manner described
    above. We call this “class inheritance.”![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig10_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig10_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.10
  prefs: []
  type: TYPE_NORMAL
- en: Class car transmission types
  prefs: []
  type: TYPE_NORMAL
- en: I mentioned before the capability of OO objects to be reusable (Re-usable Objects).
    This is very significant in that it allows a defined object to become part of
    another class, while still keeping its own original identity and independence.
    The example below demonstrates how Cars can be reused in another class (Fig. [3.11](#Fig11)).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig11_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig11_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.11
  prefs: []
  type: TYPE_NORMAL
- en: Class transportation vehicles
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the object Car is now part of another class called Transportation
    Vehicles. However, Car, instead of being an abstract object within its class,
    has become concrete and thus inherits from its parent, Transportation Vehicles.
    The object Cars has methods that may execute differently depending on the class
    it is in. Therefore, Cars in the Transportation Vehicle class might interpret
    a request for “driving the car” as it relates to general transportation vehicles.
    Specifically, it might invoke a service that shows how to maneuver a car while
    it is moving. On the other hand, Cars in the Transmission class might interpret
    the same message coming from one of its children objects as meaning how the transmission
    shifts when a person is driving. This phenomenon is called polymorphism. Polymorphism
    allows an object to change its behavior within the same methods under different
    circumstances. What is more important is that polymorphism is dynamic in behavior
    so its changes in operation are determined when the object is executed or during
    run-time.
  prefs: []
  type: TYPE_NORMAL
- en: Because objects can be reused, keeping the same version current in every copy
    of the same object in different classes is important. Fortunately, objects are
    typically stored in Dynamic Link Libraries (DLL). The significance of a DLL is
    that it always stores the current version of an object. Because objects are linked
    dynamically before each execution, you are ensured that the current version is
    always the one used. The DLL facility therefore avoids the maintenance nightmares
    of remembering which applications contain the same sub-programs. Legacy systems
    often need to re-link every copy of the subprogram in each module where a change
    occurs. This problem continues to haunt the COBOL application community.
  prefs: []
  type: TYPE_NORMAL
- en: Another important feature in object systems is Instantiation and Persistence.
    Instantiation allows multiple executions of the same class to occur independent
    of another execution. This means that the there are multiple copies of the same
    class executing concurrently. The significance of these executions is that they
    are mutually exclusive and can be executing different concrete objects within
    that class. Because of this capability, we say that objects can have multiple
    *instances* within each executing copy of a class it belongs to. Sometimes, although
    class executions are finished, a component object continues to operate or *persist*.
    Persistence is therefore an object that continues to operate after the class or
    operation that invoked it has finished. The system must keep track of each of
    these object instances.
  prefs: []
  type: TYPE_NORMAL
- en: The abilities of objects and classes to have inheritance, polymorphic behavior,
    instantiation and persistence are just some of the new mechanisms that developers
    can take advantage of when building OO systems.^([4](#Fn4)) Because of this, the
    analyst must not only understand the OO methodology, but must also apply new approaches
    and tools that will allow an appropriate schematic to be produced for system developers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Object Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another analysis modeling tool is called a State Transition Diagram (STD) and
    useful for modeling event driven and time dependent systems. A state very closely
    resembles an object/class and therefore can be used with little modification to
    depict the flow and relationships of objects. The major difference between an
    object and a state is that an object is responsible for its own data (which we
    call an attribute in OO). An object’s attributes are said to be *encapsulated*
    behind its methods, that is, a user cannot ask for data directly. The concept
    of encapsulation is that access to an object is allowed only for a purpose rather
    than for obtaining specific data elements. It is the responsibility of the method
    and its component services to determine the appropriate attributes that are required
    to service the request of the object. An object diagram, regardless of whose methodology
    is used, is essentially a hybrid of an STD and an Entity Relational Diagram (ERD).
    The STD represents the object’s methods and the criteria for moving from one object
    to another. The ERD , on the other hand, defines the relationship of the attributes
    between the stored data models. The result is best shown using the order processing
    example below.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [3.12](#Fig12) reflects that a customer object submits a purchase order
    for items to the order object. The relationship between customer and order reflects
    both STD and ERD characteristics. The “submits purchase order” specifies the condition
    to change the state of or move to the order object. The direction arrow also tells
    us that the order object cannot send a purchase order to the customer object.
    The crow’s foot cardinality shows us that a customer object must have at least
    one order to create a relationship with the order object. After an order is processed,
    it is prepared for shipment. Notice that each order has one related shipment object;
    however multiple warehouse items can be part of a shipment. The objects depicted
    above can also represent classes suggesting that they are comprised of many component
    objects. These component objects might in turn be further decomposed into other
    primitive objects. This is consistent with the concept of the logical equivalent
    and with functional decomposition (Fig. [3.13](#Fig13)).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig12_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig12_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.12
  prefs: []
  type: TYPE_NORMAL
- en: An object/class diagram
  prefs: []
  type: TYPE_NORMAL
- en: '![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig13_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig13_HTML.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3.13
  prefs: []
  type: TYPE_NORMAL
- en: The component objects of the warehouse class
  prefs: []
  type: TYPE_NORMAL
- en: It is important that the analyst specify whether classes or objects are depicted
    in the modeling diagrams. It is not advisable to mix classes and objects at the
    same level. Obviously, the class levels can be effective for user verification,
    but objects will be inevitably required for final analysis and engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Relationship to Structured Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many analysts make the assumption that the traditional structured tools are
    not required in OO analysis. This simply is not true, as we have shown in the
    previous examples. To further emphasize the need to continue using structured
    techniques, we need to understand the underlying benefit of the OO paradigm and
    how structured tools are necessary to map to the creation of objects and classes.
    It is easy to say: “find all the objects in the essential components”; actually,
    to have a process to do so is another story. Before providing an approach to determine
    objects, let us first understand the problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.1 Application Coupling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coupling can be defined as the measurement of an application’s dependency on
    another. Simply put, does a change in an application program necessitate a change
    to another application program? Many known system malfunctions have resulted from
    highly coupled systems. The problem, as you might have anticipated, relates back
    to the analysis function, where decisions could be made as to what services should
    be joined to form one single application program. Coupling is never something
    that we want to do, but no system can be made up of just one program. Therefore,
    coupling is a reality and one that analysts must focus on. Let us elaborate on
    the coupling problem through the following example (Fig. [3.14](#Fig14)).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig14_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig14_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.14
  prefs: []
  type: TYPE_NORMAL
- en: Application coupling
  prefs: []
  type: TYPE_NORMAL
- en: The two programs A and B are coupled via the passing of the variable Y. Y is
    subsequently used in B to calculate R. Should the variable Y change in A, it will
    not necessitate a change in B. This is considered good coupling. However, let
    us now examine X. We see that X is defined in both A and B. Although the value
    of X does not cause a problem in the current versions of A and B, a subsequent
    change of X will cause a programmer to remember to change the value in B. This
    is a maintenance nightmare. In large enterprise level systems, analysts and programmers
    cannot “remember” where all of these couples have occurred, especially when the
    original developers are no longer with the organization. The solution to this
    problem is also to pass X from program A as shown in Fig. [3.15](#Fig15).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig15_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig15_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.15
  prefs: []
  type: TYPE_NORMAL
- en: Application coupling using variables X and Y
  prefs: []
  type: TYPE_NORMAL
- en: We now see that both X and Y are passed and programs A and B are said to have
    low coupling. In addition, program A is said to be more *cohesive.*
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.2 Application Cohesion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cohesion is the measurement of how independent a program is on its own processing.
    That is, a cohesive program contains all of the necessary data and logic to complete
    its applications without being directly affected by another program; a change
    in another program should not require a change to a cohesive one. Furthermore,
    a cohesive program should not cause a change to be made in another program. Therefore,
    cohesive programs are independent programs that react to messages to determine
    what they need to do; however, they remain self-contained. When program A also
    passed X it became more cohesive because a change in X no longer required a change
    to be made to another program. In addition, B is more cohesive because it gets
    the change of X automatically from A. Systems that are designed more cohesively
    are said to be more maintainable. Their codes can also be reused or retrofitted
    into other applications as components because they are wholly independent. A cohesive
    program can be compared to an interchangeable standard part of a car. For example,
    if a car requires a standard 14-in. tire, typically any tire that meets the specification
    can be used. The tire, therefore, is not married to the particular car, but rather
    is a cohesive component for many cars.
  prefs: []
  type: TYPE_NORMAL
- en: Cohesion is in many ways is the opposite of coupling. The higher the cohesion,
    the lower the coupling. Analysts must understand that an extreme of either cohesion
    or coupling cannot exist. This is shown in the graph below (Fig. [3.16](#Fig16)).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig16_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig16_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.16
  prefs: []
  type: TYPE_NORMAL
- en: Coupling and cohesion relationships
  prefs: []
  type: TYPE_NORMAL
- en: The graph shows that we can never reach 100% cohesion; that would mean there
    is only one program in the entire system, a situation that is unlikely. However,
    it is possible to have a system where a 75% cohesion ratio is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: We now need to relate this discussion to OO. Obviously OO is based very much
    on the concept of cohesion. Objects are independent reusable modules that control
    their own attributes and services. Object coupling is based entirely on message
    processing via inheritance or collaboration.^([5](#Fn5)) Therefore, once an object
    is identified, the analyst must define all of its processes in a cohesive manner.
    Once the cohesive processes are defined, the required attributes of the object
    are then added to the object. Below is a table which shows how processes can be
    combined to create the best cohesion (Fig. [3.17](#Fig17)).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig17_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig17_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.17
  prefs: []
  type: TYPE_NORMAL
- en: Methods of selecting cohesive objects
  prefs: []
  type: TYPE_NORMAL
- en: The tiers above are based on best to worst, where By Function is the most desirable
    and By Lines of Code the least desirable. Tiers 1 and 2 will render the best object
    cohesiveness. This can be seen with the following example.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [3.18](#Fig18) depicts a four-screen system that includes four objects,
    that is, each screen is a separate object. The Transaction Processing object has
    been designed using Tier 2, By Same Data since it deals only with the Transaction
    File. The object is cohesive because it does not depend on or affect another module
    in its processing. It provides all of the methods required for transaction data.![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig18_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig18_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.18
  prefs: []
  type: TYPE_NORMAL
- en: Applications with varying types of object cohesion
  prefs: []
  type: TYPE_NORMAL
- en: The Financials object is an example of Tier 1, By Function since a Balance Sheet
    is dependent on the Income Statement and the Income Statement is dependent on
    the Trial Balance. The object therefore is self-contained within all the functions
    necessary to produce financial information (in this example).
  prefs: []
  type: TYPE_NORMAL
- en: The System Editor, on the other hand, being an example of Tier 3, shows that
    it handles all of the editing (verification of the quality of data) for the system.
    Although there appears to be some benefit to having similar code in one object,
    we can see that it affects many different components. It is therefore considered
    a highly coupled object and not necessarily the easiest to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: We can conclude that Tiers 1 and 2 provide analysts with the most attractive
    way for determining an object’s attributes and services. Tiers 3 and 4, although
    practiced, do not provide any real benefits in OO and should be avoided as much
    as possible. The question now is what technique do we follow to start providing
    the services and attributes necessary when developing logical objects?
  prefs: []
  type: TYPE_NORMAL
- en: The structured tools discussed in Chap. [3](480347_1_En_3_Chapter.xhtml) provide
    us with the essential capabilities to work with OO analysis and design. The STD
    can be used to determine the initial objects and the conditions of how one object
    couples or relates to another. Once the STD is prepared it can be matured into
    the object model discussed earlier in this chapter. The object model can be decomposed
    to its lowest level; the attributes and services of each object must then be defined.
    All of the DFD functional primitives can now be mapped to their respective objects
    as services within their methods. It is also a way of determining whether an object
    is missing (should there be a DFD that does not have a related object). The analyst
    should try to combine each DFD using the Tier 1 By Function approach. This can
    sometimes be very difficult depending on the size of the system. If the Tier 1
    approach is too difficult, the analyst should try Tier 2 by combining DFDs based
    on their similar data stores. This is a very effective approach; since Tier 1
    implies Tier 2,^([6](#Fn6)) it is a very productive way to determine how processes
    should be mapped to their appropriate objects. This does not suggest that the
    analyst should not try Tier 1 first.
  prefs: []
  type: TYPE_NORMAL
- en: The next activity is to determine the object’s attributes or data elements.
    The ERD serves as the link between an attribute in an object and its actual storage
    in a database. It is important to note that the attribute setting in an object
    may have no resemblance to its setting in the logical and physical data entity.
    The data entity is focused on the efficient storage of the elements and its integrity,
    whereas the attribute data in an object is based on its cohesiveness with the
    object’s services.
  prefs: []
  type: TYPE_NORMAL
- en: The mapping of the object to the DFD and ERD can be best shown graphically below
    (Fig. [3.19](#Fig19)).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig19_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig19_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.19
  prefs: []
  type: TYPE_NORMAL
- en: The relationships between an object and the ERD and DFD
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the functional primitive DFDs and the ERD resulting from the normalization
    process provide the vehicles for providing an object’s attributes and services.
  prefs: []
  type: TYPE_NORMAL
- en: 3.8 Object-Oriented Databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There was a movement in the industry to replace the traditional Relational Database
    Management Systems (RDBMS) with the Object-Oriented Database Management System
    (OODBMS). Object databases differ greatly from the relational model in that the
    object’s attributes and services are stored together. Therefore, the concept of
    columns and rows of normalized data becomes extinct. The proponents of OODBMS
    saw a major advantage in that object databases could also keep graphical and multimedia
    information about the object, something that relational databases cannot do. The
    result has been the ultimate creation of different data storages, many of which
    do not require row and column architecture, but it is expected that the relational
    model will continue to be used for some time. However, most RDBMS products will
    become more OO. This means they will use the relational engine but employ more
    OO capabilities, that is, build a relational hybrid model. In either case, analysts
    should continue to focus on the logical aspects of capturing the requirements.
    Changes in the OO methodologies are expected to continue with the evolution of
    block chain architectures as well as the use of unformatted data using Natural
    Language methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: 3.9 Designing Distributed Objects Using Use-Case Analysis and Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use-Cases were first proposed in 1986 as a result of the popularity of the object-oriented
    paradigm. Use-Case today is widely used in the development of web-based systems
    and is the appropriate methodology to use for mobile IoT application development.
    Use-Case was designed to be very effective when defining current and potential
    actions of a product. That is, Use-Case can be used to model activities that may
    never have occurred in a system, but are technically possible. Indeed, many system
    deficiencies occur because a user tried to perform something for the first time.
    These types of situations are sometimes referred to as supplementary specifications.
    In many ways, Use-Case methodology represented the next generation of the State
    Transition Diagrams (STD) discussed earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 3.9.1 Use-Case Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Use-Case model contains three essential components: use-cases, actors, and
    relationships (Bittner and Spence [2003](#CR1)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.9.2 Actors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An Actor represents a user of the system. When interfacing with the system,
    an “user” can be internal (traditional), consumer, or another system. They are
    notated using the symbol in Fig. [3.20](#Fig20).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig20_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig20_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.20
  prefs: []
  type: TYPE_NORMAL
- en: Use-case actor symbol
  prefs: []
  type: TYPE_NORMAL
- en: 3.10 Use Case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Use Case identifies a particular interface or “use” that an Actor does with
    the system to achieve a need. In many ways, the sum of all use cases represents
    an inventory of all possible transactions and events that can be accomplished
    with the system. It, in effect, replicates all possible permutations that can
    occur. In its most decomposed form, each use case defines one transaction. A use
    case must result in some form of output. Obviously, use cases may have restrictions;
    certain possible actor requests may require certain authorizations. A use case
    is denoted by a sphere symbol as shown in Fig. [3.21](#Fig21) (note its similarity
    with a DFD process).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig21_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig21_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.21
  prefs: []
  type: TYPE_NORMAL
- en: Use case symbol
  prefs: []
  type: TYPE_NORMAL
- en: Figure [3.22](#Fig22) shows a basic Actor/Use Case diagram.![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig22_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig22_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.22
  prefs: []
  type: TYPE_NORMAL
- en: Actor/use case flow
  prefs: []
  type: TYPE_NORMAL
- en: Note that the Use Case model in Fig. [3.22](#Fig22) actually contains two transactions.
    It could be decomposed to two separate Use Case models as shown in Fig. [3.23](#Fig23).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig23_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig23_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.23
  prefs: []
  type: TYPE_NORMAL
- en: Use case as functional primitives
  prefs: []
  type: TYPE_NORMAL
- en: The third component of the Use-Case modeling is the relationship specified by
    a data flow line, which often has an arrow to depict directionality. Similar to
    a DFD, a data flow carries data that will be transformed by the use case process
    sphere. Directionality depicts whether the data is being supplied by the Actor
    or received by the Actor from the use case process, or both! These relationship
    data flows are shown in Figs. [3.22](#Fig21) and [3.23](#Fig22).
  prefs: []
  type: TYPE_NORMAL
- en: 'While a use-case model has three essential symbols, there is another component
    that is critical. Some analysts call this the description, however, the concept
    again originated from the DFD in which the actual algorithm inside the process
    is called a process specification. Process specifications typically contained
    two forms of description: (1) the actual algorithm in a form of pseudocode, or
    (2) Pre–Post Conditions. Both can be used together depending on the complexity
    of the process. Many analysts define a process specification as everything else
    about the process not already included in the other modeling tools. Indeed, it
    must contain the remaining information that normally consists of business rules
    and application logic. DeMarco suggested that every functional primitive DFD point
    to a “Minispec” which would contain that process’ application logic.^([7](#Fn7))
    We will follow this rule and expand on the importance of writing good application
    logic even in a Use Case. There are, of course, different styles, and few textbooks
    that explain the importance to the analyst of understanding how these need to
    be developed and presented. Like other modeling tools, each process specification
    style has its good, bad and ugly.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.11 Pseudocode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most detailed and regimented process specification is pseudocode or “Structured
    English”. Its format is designed to require the analysts to have a solid understanding
    of how to write algorithms. The format is very “COBOL-like” and was initially
    designed as a way of writing functional COBOL programming specifications. The
    rules governing pseudocode are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the Do While with an Enddo to show iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use If-Then-Else to show conditions and ensure each If has an End-If
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be specific about initializing variables and other detail processing requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pseudocode is designed to give the analyst tremendous control over the design
    of the code. Take the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a requirement to calculate a 5% bonus for employees who work on the
    1st shift and a 10% bonus for workers on the 2nd or 3rd shift. Management is interested
    in a report listing the number of employees who receive a 10% bonus. The process
    also produces the bonus checks.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The pseudocode would be:![../images/480347_1_En_3_Chapter/480347_1_En_3_Figg_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Figg_HTML.png)The
    above algorithm gives the analyst great control over how the program should be
    designed. For example, note that the pseudocode requires that the programmer have
    an error condition should a situation occur where a record does not contain a
    1st, 2nd or 3rd shift employee. This might occur should there be a new shift that
    was not communicated to the information systems department. Many programmers might
    have omitted the last “If” check as follows:![../images/480347_1_En_3_Chapter/480347_1_En_3_Figh_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Figh_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: The above algorithm simply assumes that if the employee is not on the 1st shift
    then they must be either a 2nd or 3rd shift employee. Without this being specified
    by the analyst, the programmer may have omitted this critical logic which could
    have resulted in a 4th shift worker receiving a 10% bonus! As mentioned earlier,
    each style of process specification has its advantages and disadvantages, in other
    words, the good, the bad, and the ugly.
  prefs: []
  type: TYPE_NORMAL
- en: '*The Good*:'
  prefs: []
  type: TYPE_NORMAL
- en: The analyst who uses this approach has practically written the program, and
    thus the programmer will have very little to do with regards to figuring out the
    logic design.
  prefs: []
  type: TYPE_NORMAL
- en: '*The Bad*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm is very detailed and could take a long time for the analyst to
    develop. Many professionals raise an interesting point: Do we need analysts to
    be writing process specifications to this level of detail? In addition, many programmers
    may be insulted and feel that an analyst does not possess the skill-set to design
    such logic.'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Ugly*:'
  prefs: []
  type: TYPE_NORMAL
- en: The analyst spends the time, the programmers are not supportive and the logic
    is incorrect. The result here will be the “*I told you so*” remarks from programmers,
    and hostilities may grow over time.
  prefs: []
  type: TYPE_NORMAL
- en: 3.11.1 Case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Case^([8](#Fn8)) is another method of communicating application logic. Although
    the technique does not require as much technical format as pseudocode, it still
    requires the analyst to provide a detailed structure to the algorithm. Using the
    same example as in the pseudocode discussion, we can see the differences in format:![../images/480347_1_En_3_Chapter/480347_1_En_3_Figi_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Figi_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'The above format provides control as it still allows the analyst to specify
    the need for error checking; however, the exact format and order of the logic
    is more in the hands of the programmer. Let’s now see the good, bad and ugly of
    this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Good*:'
  prefs: []
  type: TYPE_NORMAL
- en: The analyst has provided a detailed description of the algorithm without having
    to know the format of logic in programming. Because of this advantage, CASE takes
    less time than pseudocode.
  prefs: []
  type: TYPE_NORMAL
- en: '*The Bad*:'
  prefs: []
  type: TYPE_NORMAL
- en: Although this may be difficult to imagine, the analyst may miss some of the
    possible conditions in the algorithm, such as forgetting a shift! This happens
    because the analyst is just listing conditions as opposed to writing a specification.
    Without formulating the logic as we did in pseudocode, the likelihood of forgetting
    or overlooking a condition check is increased.
  prefs: []
  type: TYPE_NORMAL
- en: '*The Ugly*:'
  prefs: []
  type: TYPE_NORMAL
- en: Case logic can be designed without concern for the sequence of the logic, that
    is, the actual progression of the logic as opposed to just the possibilities.
    Thus the logic can become more confusing because it lacks actual progressive structure.
    As stated previously, the possibility of missing a condition is greater because
    the analyst is not actually following the progression of the testing of each condition.
    There is thus a higher risk of the specification being incomplete.
  prefs: []
  type: TYPE_NORMAL
- en: 3.12 Pre–Post Conditions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pre-Post is based on the belief that analysts should not be responsible for
    the details of the logic, but rather for the overall highlights of what is needed.
    Therefore, the pre-post method lacks detail and expects that the programmers will
    provide the necessary details when developing the application software. The method
    has two components: Pre-Conditions and Post-Conditions. Pre-conditions represent
    things that are assumed true or that must exist for the algorithm to work. For
    example, a pre-condition might specify that the user must input the value of the
    variable X. On the other hand, the post-condition must define the required outputs
    as well as the relationships between calculated output values and their mathematical
    components. Suppose the algorithm calculated an output value called Total_Amount.
    The post-condition would state that Total_Amount is produced by multiplying Quantity
    times Price. Below is the pre-post equivalent of the Bonus algorithm:![../images/480347_1_En_3_Chapter/480347_1_En_3_Figj_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Figj_HTML.png)'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the above specification does not show how the actual algorithm
    should be designed or written. It requires the programmer or development team
    to find these details and implement the appropriate logic to handle it. Therefore,
    the analyst has no real input into the way the application will be designed or
    the way it functions.
  prefs: []
  type: TYPE_NORMAL
- en: '*The Good*:'
  prefs: []
  type: TYPE_NORMAL
- en: The analyst need not have technical knowledge to write an algorithm and does
    not need to spend an inordinate amount of time to develop what is deemed a programming
    responsibility. Therefore, less technically oriented analysts can be involved
    in specification development.
  prefs: []
  type: TYPE_NORMAL
- en: '*The Bad*:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no control over the design of the logic, and thus the opportunity for
    misunderstandings and errors is much greater. The analyst and the project are
    much more dependent on the talent of the development staff.
  prefs: []
  type: TYPE_NORMAL
- en: '*The Ugly*:'
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps we misunderstand the specification. Since the format of pre-post conditions
    is less specific, there is more room for ambiguity.
  prefs: []
  type: TYPE_NORMAL
- en: 3.13 Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A matrix or table approach is one that shows the application logic in tabular
    form. Each row reflects a result of a condition, with each column representing
    the components of the condition to be tested. The best way to explain a matrix
    specification is to show an example as shown in Fig. [3.24](#Fig24).![../images/480347_1_En_3_Chapter/480347_1_En_3_Fig24_HTML.png](../images/480347_1_En_3_Chapter/480347_1_En_3_Fig24_HTML.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fig. 3.24
  prefs: []
  type: TYPE_NORMAL
- en: Sample matrix specification
  prefs: []
  type: TYPE_NORMAL
- en: Although this is a simple example that uses the same algorithm as the other
    specification styles, it does show how a matrix can describe the requirements
    of an application without the use of sentences and pseudocode.
  prefs: []
  type: TYPE_NORMAL
- en: '*The Good*:'
  prefs: []
  type: TYPE_NORMAL
- en: The analyst can use a matrix to show complex conditions in a tabular format.
    The tabular format is preferred by many programmers because it is easy to read,
    organized and often easy to maintain. Very often the matrix resembles the array
    and table formats used by many programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: '*The Bad*:'
  prefs: []
  type: TYPE_NORMAL
- en: It is difficult, if not impossible, to show a complete specification in matrices.
    The above example supports this, in that the remaining logic of the bonus application
    is not shown. Therefore, the analyst must integrate one of the other specification
    styles to complete the specification.
  prefs: []
  type: TYPE_NORMAL
- en: '*The Ugly*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrices are used to describe complex condition levels, where there are many
    “If” conditions to be tested. These complex conditions often require much more
    detailed analysis than shown in a matrix. The problem occurs when the analyst,
    feeling the matrix may suffice, does not provide enough detail. The result: conditions
    may be misunderstood by the programmer during development.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Conclusion*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The question must be asked again: What is a good specification? We will continue
    to explore this question. In this chapter we have examined the logic alternatives.
    Which logic method is best? It depends! We have seen from the examples that each
    method has its advantages and shortcomings. The best approach is to be able to
    use them all, and to select the most appropriate one for the task at hand. To
    do this effectively means clearly recognizing where each style provides a benefit
    for the part of the system you are working with, and who will be doing the development
    work. The table below attempts to put the advantages and shortcomings into perspective.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.14 Problems and Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an Object?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the relationship between a Method and a Service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a Class?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the Object Paradigm change the approach of the analyst?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the two types of objects and provide examples of each type.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are Essential Functions?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an Object Type and how is it used to develop specific type of classes?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is meant by Object and Class Inheritance?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the association differences between an ERD and an Object diagram?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does functional decomposition operate with respect to classes and objects?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '11.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Coupling and Cohesion? What is their relationship with each other?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '12.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the concept of cohesion relate the structured approach to the object
    model?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '13.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What four methods can be used to design a cohesive object?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '14.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are Object Databases?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '15.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Client/Server?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '16.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do objects relate to Client/Server design?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '17.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is there a need for a hybrid object in Client/Server design?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '18.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Use Case analysis and design?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '19.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is meant by distributed objects?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.15 Mini-project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have been asked to automate the Accounts Payable process. During your interviews
    with users you identify four major events as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: I.*Purchase Order Flow*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The Marketing Department sends a Purchase Order (P.O.) form for books to the
    Accounts Payable System (APS).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'APS assigns a P.O. # and sends the P.O.-White copy to the Vendor and files
    the P.O.-Pink copy in a file cabinet in P.O.#.sequence.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: II.*Invoice Receipt*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A vendor sends an invoice for payment for books purchased by APS.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: APS sends invoice to Marketing Department for authorization.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Marketing either returns invoice to APS approved or back to the vendor if not
    authorized.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the invoice is returned to APS it is matched up against the original P.O.-Pink.
    The PO and vendor invoice are then combined into a packet and prepared for the
    voucher process.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: III.*Voucher Initiation*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: APS receives the packet for vouchering. It begins this process by assigning
    a voucher number.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The Chief Accountant must approve vouchers > $5,000.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: APS prepares another packet from the approved vouchers. This packet includes
    the P.O.-Pink, authorized invoice and approved voucher.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: IV.*Check Preparation*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Typist receives the approved voucher packet and retrieves a numbered blank check
    to pay the vendor.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Typist types a two-part check (blue, green) using data from the approved voucher
    and enters invoice number on the check stub.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: APS files the approved packet with the Check–green in the permanent paid file.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The check is either picked up or mailed directly to the vendor.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Assignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide the DFDs for the four events. Each event should be shown as a single
    DFD on a separate piece of paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Level each event to its functional primitives.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop the Process Specifications for each functional primitive DFD.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
