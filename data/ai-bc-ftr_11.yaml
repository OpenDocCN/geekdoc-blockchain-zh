- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2021Y.
    Maleh et al. (eds.)Artificial Intelligence and Blockchain for Future Cybersecurity
    ApplicationsStudies in Big Data90[https://doi.org/10.1007/978-3-030-74575-2_9](https://doi.org/10.1007/978-3-030-74575-2_9)
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者（S）独家许可给Springer Nature Switzerland AG 2021年Y. Maleh et al. (eds.)未来网络安全应用的人工智能和区块链Big
    Data研究[https://doi.org/10.1007/978-3-030-74575-2_9](https://doi.org/10.1007/978-3-030-74575-2_9)
- en: 'AntiPhishTuner: Multi-level Approaches Focusing on Optimization by Parameters
    Tuning in Phishing URLs Detection'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AntiPhishTuner：针对钓鱼网址检测的参数调优的多级方法
- en: 'Md. Fahim Muntasir^([1](#Aff7), [2](#Aff8) [ ](#ContactOfAuthor1)), Sheikh Shah Mohammad Motiur Rahman^([1](#Aff7),
    [2](#Aff8) [ ](#ContactOfAuthor2)), Nusrat Jahan^([1](#Aff7) [ ](#ContactOfAuthor3)),
    Abu Bakkar Siddikk^([1](#Aff7), [2](#Aff8) [ ](#ContactOfAuthor4)) and Takia Islam^([1](#Aff7),
    [2](#Aff8) [ ](#ContactOfAuthor5))(1)Department of Software Engineering, Daffodil
    International University, Dhaka, Bangladesh(2)nFuture Research Lab, Dhaka, BangladeshMd. Fahim Muntasir (Corresponding
    author)Email: [fahim35-1900@diu.edu.bd](mailto:fahim35-1900@diu.edu.bd)Sheikh Shah Mohammad Motiur Rahman (Corresponding
    author)Email: [motiur.swe@diu.edu.bd](mailto:motiur.swe@diu.edu.bd)Nusrat JahanEmail:
    [nusrat.swe@diu.edu.bd](mailto:nusrat.swe@diu.edu.bd)Abu Bakkar SiddikkEmail:
    [abu35-1994@diu.edu.bd](mailto:abu35-1994@diu.edu.bd)Takia IslamEmail: [takia35-1014@diu.edu.bd](mailto:takia35-1014@diu.edu.bd)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Md. Fahim Muntasir^([1](#Aff7), [2](#Aff8) [ ](#ContactOfAuthor1)), Sheikh Shah Mohammad Motiur Rahman^([1](#Aff7),
    [2](#Aff8) [ ](#ContactOfAuthor2)), Nusrat Jahan^([1](#Aff7) [ ](#ContactOfAuthor3)),
    Abu Bakkar Siddikk^([1](#Aff7), [2](#Aff8) [ ](#ContactOfAuthor4)) 和 Takia Islam^([1](#Aff7),
    [2](#Aff8) [ ](#ContactOfAuthor5))(1)软件工程系，孟加拉国达菲迪尔国际大学，达卡(2)nFuture研究实验室，达卡，孟加拉国Md. Fahim Muntasir（通讯作者）电子邮件：[fahim35-1900@diu.edu.bd](mailto:fahim35-1900@diu.edu.bd)Sheikh Shah Mohammad Motiur Rahman （通讯作者）电子邮件：[motiur.swe@diu.edu.bd](mailto:motiur.swe@diu.edu.bd)Nusrat Jahan电子邮件：[nusrat.swe@diu.edu.bd](mailto:nusrat.swe@diu.edu.bd)Abu Bakkar Siddikk电子邮件：[abu35-1994@diu.edu.bd](mailto:abu35-1994@diu.edu.bd)Takia Islam电子邮件：[takia35-1014@diu.edu.bd](mailto:takia35-1014@diu.edu.bd)
- en: Abstract
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Phishing is an alarming issue among the cybercriminals. In the last decade,
    online services have revolutionized the world. Due to the revolutionary transformations
    of web service, the reliance on the web has increased day by day. Security threats
    have emerged due to the increasing reliance on online orientation. There are many
    types of anti-phishing solutions available that have been proposed by many researchers.
    However, this chapter is to propose an intelligent framework to detect phishing
    URLs based on the optimized learning architecture scheme. Multi-layer based structures
    have been implemented to detect phishing URLs using Deep Neural Network (DNN),
    Neural Network (NN) and Stacking. These architectures are evaluated with various
    tuning hyper-parameters to obtain the optimized output named AntiPhishTuner. As
    a result, five-layer based DNN can provide accuracy of 0.95 with the minimum mean
    squared error (MSE) 0.30, and also a mean absolute error (MAE) 0.074 where the
    number of epochs was 50 and Adam optimizer as an optimizer. Using two-layer NN
    with AdaGard optimizer can provide accuracy of 0.95, with MSE 0.30 and MAE 0.074\.
    NN provides these results with 150 epochs. Stack generalization can reach maximum
    accuracy 0.97 in binary classification with MAE 2.1\. This chapter can provide
    a better lead to researchers and anti-phishing tools developers to make an initial
    decision about the approach that should be followed for further extension.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 钓鱼是网络犯罪分子之间的一个令人担忧的问题。在过去的十年里，在线服务已经彻底改变了世界。由于网络服务的革命性转变，人们对网络的依赖日益增加。由于对在线导向的依赖日益增加，安全威胁已经出现。有许多种类型的反钓鱼解决方案是由许多研究人员提出的。然而，本章旨在提出一种智能框架，以基于优化学习体系结构方案的方式检测钓鱼网址。采用了基于多层结构的深度神经网络（DNN）、神经网络（NN）和堆叠来检测钓鱼网址。这些体系结构是通过对各种调整超参数进行评估来实现的，以获得名为AntiPhishTuner的优化输出。因此，基于五层DNN的结果可以提供0.95的准确度，最小均方误差（MSE）0.30，以及平均绝对误差（MAE）0.074，其中周期数为50，优化器为Adam。使用两层NN和AdaGard优化器可以提供0.95的准确度，MSE为0.30，MAE为0.074\.
    NN通过150个周期提供这些结果。堆叠泛化在二进制分类中可以达到0.97的最大准确度，MAE为2.1\. 本章可以为研究人员和反钓鱼工具开发者提供更好的引导，使他们能够就应该遵循的方法做出初步决定，以进行进一步扩展。
- en: KeywordsUniform resource Locator (URL)PhishingDeep Neural Network (DNN)Neural
    Network (NN)Stacking
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词统一资源定位符（URL）钓鱼深度神经网络（DNN）神经网络（NN）堆叠
- en: 1 Introduction
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Phishing is a fraudulent technique used by both social and technological engineering
    for the purpose of stealing user identities and personal account information and
    credentials from financial accounts Huang [[22](#CR22)]. There are a broad variety
    of phishing forms, including algorithms, link handling, email phishing, domain
    spoofing, phishing using HTTPS, SMS, pop-ups. prefix, suffix, subdomain, IP address,
    URL-length, ‘@’ symbol, spear phishing, dual-slash attributes, port, https token,
    request URL, URL-anchor, tag-links, domain age are phishing attributes Rahman
    [[13](#CR13)].
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**钓鱼**是一种欺诈技术，既涉及社会工程学，又涉及技术工程，目的是窃取用户身份和个人账户信息以及金融账户的凭据 Huang [[22](#CR22)]。有各种各样的钓鱼形式，包括算法、链接处理、电子邮件钓鱼、域欺骗、使用
    HTTPS 的钓鱼、短信钓鱼、弹出窗口。前缀、后缀、子域、IP 地址、URL 长度、''@'' 符号、鱼叉式钓鱼、双斜线属性、端口、https 令牌、请求
    URL、URL 锚、标签链接、域名年龄是钓鱼的特征 Rahman [[13](#CR13)]。'
- en: The elements of a phishing platform are typically equivalent to a few legitimate
    websites literally and externally. Today’s security concerns are increasingly
    rising due to phishing. According to an eminent Washington-based cyber security
    company F5 Systems, Inc., which stamped its target choice, sociology and technological
    infiltration, a phishers strategy combines three special tasks Pompon [[23](#CR23)].
    According to the Anti-Phishing Working organization, there were 18,480 momentous
    phishing attacks and 9666 curiously phishing regions in March2006\. It impacts
    billions of site clients and enormous costing boundaries to businesses Viktorov
    [[24](#CR24)]. The prospective expenditure of computerized offense to the around
    the global network could be a phenomenal 500 billion USD and a clue break will
    fetch the ordinary organization around 3.8 million USD expenditure, considering
    that evidence by Microsoft, in 2018.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**钓鱼平台**的元素通常在字面上和外观上与几个合法网站相当。如今，由于钓鱼活动的增加，安全问题越来越受到关注。根据华盛顿一家知名的网络安全公司 F5
    Systems, Inc. 的说法，该公司在目标选择、社会学和技术渗透方面留下了它的印记，钓鱼者的策略结合了三个特殊任务 Pompon [[23](#CR23)]。根据反钓鱼工作组的说法，2006
    年 3 月有 18,480 次重大的钓鱼攻击和 9666 个奇怪的钓鱼域。它影响了数十亿个网站客户，对企业造成了巨大的成本障碍 Viktorov [[24](#CR24)]。全球计算犯罪的未来支出可能达到惊人的
    5000 亿美元，根据微软在 2018 年的证据，一个线索泄露将使普通组织遭受约 380 万美元的支出。'
- en: There are several proposed solutions that researchers have provided. For example,
    detect phishing websites through a hierarchical clustering approach which bunches
    the vectors produced from DOMs together concurring to their corresponding distance
    Cui [[25](#CR25)]. A few considers centered on detecting phishing URLs by using
    the potential characteristics of URLs. One to two hidden layers are usually used
    for neural networks. In some cases of deep learning, the number of layers varies.
    But it requires nearly more than 150 layers Le [[11](#CR11)]. There are a few
    rules to decide the number of layers that incorporate two or less layers for basic
    data sets and for computer vision, time series, or with intricate datasets extra
    layers can give way better results Rahman [[13](#CR13)]. Mostly classification
    the data patterns are accessible in a structured way. But the URL information
    isn’t accessible in a settled pattern. Applying the classification methods or
    machine learning techniques in URL data. In this way additional approaches ought
    to be utilized for overseeing the URLs Woogue [[26](#CR26)]. Phishing could be
    a pivotal issue in web security. Phishing detection technique Enables URLs recognition
    through Various URLs evaluations. Apropos assess the URLs, a number of procedures
    are accessible. Among the accessible techniques the machine learning techniques
    are more compelling and precise. Such techniques the malicious URLs patterns become
    acquainted by classification algorithm and when requisite. It distinguishes the
    URLs sorts that are phishing or legitimate Dong [[6](#CR6)]. Phish tank database
    is a norm assortment that keeps track of phishing reported URLs by various web
    security organizations. This database stores a variety of features Mohammad [[27](#CR27)].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员提出了几种解决方案。例如，通过分层聚类方法检测钓鱼网站，该方法将从DOM产生的向量聚集在一起，根据它们的相应距离进行聚类，Cui [[25](#CR25)]。一些考虑集中在利用URL的潜在特征来检测钓鱼URL上。神经网络通常使用一到两个隐藏层。在某些深度学习的情况下，层数会有所不同。但是它需要近150层以上的Le
    [[11](#CR11)]。有一些规则来决定层数，其中包括用于基本数据集的两层或更少的层，以及用于计算机视觉、时间序列或复杂数据集的额外层可以获得更好的结果Rahman
    [[13](#CR13)]。大多数情况下，分类数据模式是以结构化方式可访问的。但是URL信息并不以固定模式可访问。将分类方法或机器学习技术应用于URL数据。通过这种方式，应该使用额外的方法来管理URLs
    Woogue [[26](#CR26)]。钓鱼是网络安全中的一个关键问题。钓鱼检测技术通过各种URL评估来识别URLs。关于评估URLs，有许多方法可供选择。在可用方法中，机器学习技术更加有效和准确。通过这些技术，恶意URL的模式通过分类算法变得熟悉，当需要时，它可以区分是钓鱼还是合法的URLs
    Dong [[6](#CR6)]。Phish tank数据库是一个标准收藏，跟踪各种网络安全组织报告的钓鱼URLs。这个数据库存储了各种特征Mohammad
    [[27](#CR27)]。
- en: Phishing is the most known online security threat and it can be called fraudulent
    practice on criminal activities. Which is the main concern of phishing attackers.
    Usually, phishing attackers mimic legitimate websites for credential information
    such as online banking, e-commerce websites so that user’s expose their sensitive
    information such as name, password, login credential, credit card information,
    health-related information etc. refers to mimic sites. Attackers collect user’s
    information and carry out various fraudulent activities by phishing attacks Abutair
    [[1](#CR1)].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 钓鱼是最为人知的网络安全威胁，可以称之为犯罪活动的欺诈性实践。这是钓鱼攻击者的主要关注点。通常，钓鱼攻击者模仿合法网站以获取用户的凭证信息，例如在线银行、电子商务网站，以便用户透露其敏感信息，例如姓名、密码、登录凭证、信用卡信息、与健康相关的信息等，这些都是模仿网站。攻击者收集用户信息，并通过钓鱼攻击进行各种欺诈活动Abutair
    [[1](#CR1)]。
- en: URLs play a significant role in phishing attacks, where attackers send malicious
    URLs to users through various communication channels such as emails, social media,
    etc., and sending URLs look like a valid URLs Shirazi [[17](#CR17)].
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: URL在钓鱼攻击中起着重要作用，攻击者通过各种通信渠道（例如电子邮件、社交媒体等）向用户发送恶意URL，并发送的URL看起来像是一个有效的URLs Shirazi
    [[17](#CR17)]。
- en: Typically, three ways are used to take advantage of phishing attacks Hutchinson
    [[9](#CR9)]. First of all, mimic the legitimate web interface which looks exactly
    like a legitimate interface is called web-based phishing. Considering it valid,
    Phishers fool user provides credential information. Secondly, attackers use web-based
    techniques to send phishing content via email. The third one is which phishing
    attack also occurred by malware-based where attackers inject malicious code to
    user’s system Dong [[6](#CR6)]. In any case, why machine learning-based anti-phishing
    framework is used for phishing detection? Because to detect those phishing attacks
    some traditional approaches like Blacklisting, regular expression, and signature
    matching are used, however those approach fail to detect unknown URLs Rahman [[13](#CR13)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有三种方式用来利用钓鱼攻击 Hutchinson [[9](#CR9)]。首先，模仿合法的网络界面，看起来完全像合法界面，被称为基于网络的钓鱼。考虑到它是有效的，网络钓鱼者欺骗用户提供凭据信息。其次，攻击者使用基于网络的技术通过电子邮件发送钓鱼内容。第三种方式是钓鱼攻击也是通过恶意软件为基础的，攻击者向用户系统注入恶意代码
    Dong [[6](#CR6)]。无论哪种情况，为什么使用基于机器学习的反钓鱼框架进行钓鱼检测？因为为了检测那些钓鱼攻击，一些传统方法如黑名单、正则表达式和签名匹配被使用，然而这些方法无法检测未知的URL
    Rahman [[13](#CR13)]。
- en: Detecting the unknown pattern of malicious URLs database signatures have always
    remains updated. However, by the expansion of research in the number of machine
    learning-based research for malicious URLs detection, it’s observed that deep
    learning-based architecture provides better performance than existing machine
    learning algorithms Harikrishnan [[10](#CR10)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 检测恶意URL数据库签名的未知模式始终保持更新。然而，通过对基于机器学习的恶意URL检测研究数量的扩展，观察到深度学习架构比现有的机器学习算法提供更好的性能
    Harikrishnan [[10](#CR10)]。
- en: 'The principal objectives of this chapter can be stated as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要目标可以概括如下：
- en: Assessment of AntiPhishTuner with tuning optimizer for Neural Network as well
    as Deep Learning (Deep Neural Network).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用调优器对神经网络以及深度学习（深度神经网络）进行AntiPhishTuner的评估。
- en: Phishing URLs detection has been implemented to improve the accuracy by the
    stacking concept.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过堆叠概念实现了钓鱼网址检测，以提高准确性。
- en: Combining all types of classification can perform phish stack, like machine
    learning, ensemble learning and neural network based approach as base classification.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合所有类型的分类可以执行钓鱼堆叠，如机器学习、集成学习和基于神经网络的方法作为基本分类。
- en: Expressing intelligent Anti-phishing architectures with optimization tuning.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用优化调整来表达智能反钓鱼体系结构。
- en: Effect of learning rate in neural network-based technique.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络技术中学习率的影响。
- en: Appraise of training accuracy with regard to mutate in learning rate.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估训练准确率与学习率变化的关系。
- en: Detecting the optimized parameter that are suitable to develop the result for
    DNN and NN.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测适合开发DNN和NN结果的优化参数。
- en: Detecting the combination of adaptive learning optimization algorithm with DNN
    and NN.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测自适应学习优化算法与DNN和NN的组合。
- en: 'The remainder of the paper is organized as follows: in Sect. [2](#Sec2), represents,
    Literary Review. In Sect. [3](#Sec3), represents the methodology of phishing URLs
    detection using multilayer approaches and the dataset information which are used
    to experiment and evaluate has been described. Experiments, evaluation parameters
    along with obtained results have been identified and analyzed in Sect. [4](#Sec9).
    Finally, Sect. [5](#Sec17) concludes the paper.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的剩余部分安排如下：第[2](#Sec2)节表示文献综述。在第[3](#Sec3)节中，描述了使用多层方法检测钓鱼网址的方法以及用于实验和评估的数据集信息。在第[4](#Sec9)节中识别和分析了实验、评估参数以及获得的结果。最后，第[5](#Sec17)节总结了本文。
- en: 2 Literature Review
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 文献综述
- en: Adebowale [[2](#CR2)] proposed an ordinary technique that there are some users
    who steal confidential information from websites and call those users are phishing
    users. This activity commonly happens by fake websites or malicious URLs that
    are called fraudulent ventures. Cybercriminals use fraudulent activities to create
    a well-designed phishing attack. Gaining access to the victim’s systems the cybercriminals
    could install malware or inapt protected user systems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Adebowale [[2](#CR2)] 提出了一个普通的技术，即有些用户从网站上窃取机密信息，并称这些用户是钓鱼用户。这种活动通常发生在伪造网站或恶意URL上，被称为欺诈性行为。网络犯罪分子利用欺诈行为创建精心设计的钓鱼攻击。访问受害者的系统，网络犯罪分子可能安装恶意软件或不当地保护用户系统。
- en: Acquisti [[4](#CR4)] suggested that reduce the threat of phishing assaults,
    indicating at directing the hazard of phishing attacks, various strategies are
    recommended to get ready and instruct end-users to recognize phishing URLs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Acquisti [[4](#CR4)]建议减少网络钓鱼攻击的威胁，指出要针对网络钓鱼攻击的风险，建议采取各种策略来准备和教育最终用户识别网络钓鱼网址。
- en: Wang [[20](#CR20)] suggested ensemble classifiers for e-mail filtering that
    excluded five algorithms that are Support Vector Machines, K-Nearest Neighbor,
    Gaussian Naive Bayes, Bernoulli Naive Bayes, and Random Forest Classifier.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Wang [[20](#CR20)]建议对电子邮件过滤器使用集成分类器，排除了五种算法，分别是支持向量机、K近邻、高斯朴素贝叶斯、伯努利朴素贝叶斯和随机森林分类器。
- en: Ultimately random forest was improved accuracy 94.09% to 98.02%. Gupta, S. and
    Singhal, A. [[8](#CR8)] proposed that approximately for minimum execution time
    random forest tree is an admirable strategy to detect malicious URLs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，随机森林的准确率从94.09%提高到了98.02%。Gupta, S.和Singhal, A. [[8](#CR8)]建议，对于最短执行时间，随机森林树是检测恶意URL的一种令人钦佩的策略。
- en: Vrbančič [[19](#CR19)] recommended setting parameters of deep learning neural
    networks that are swarm intelligence-based techniques. After that the proposed
    technique applied to the classification of phishing website and capable of better
    detection by comparing to the existing algorithm.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Vrbančič [[19](#CR19)]建议设置基于群体智能技术的深度学习神经网络的参数。之后，所提出的技术应用于网络钓鱼网站的分类，并通过与现有算法的比较能够更好地检测出。
- en: El-Alfy, E. S. M. [[7](#CR7)] recommended for training the nodes framework that
    connected unsupervised and supervised algorithms. Phishing sites depend on feasibility
    neural networks and clustering K medoids. Feature selection and module is used
    to reduce space capacity is used by K-medoid technique. Thirty features are achieved
    96.79% accuracy by the desired technology.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: El-Alfy, E. S. M. [[7](#CR7)]建议为培训节点系统应用无监督和监督算法。网络钓鱼网站依赖于可行性神经网络和聚类K中心点。使用特征选择和模块来减少空间容量，使用K中心点技术。通过所需技术，30个特征达到了96.79%的准确率。
- en: Le [[11](#CR11)], recommended to DNN, are trained with implied deep stacking.
    The evaluated covers of the past outlines are upgraded as they were at the conclusion
    of each DNN preparing epoch, and after that, the upgraded evaluated veils give
    extra inputs to train the DNN within the other epoch. At the test period, the
    DNN makes expectations successively in a repetitive manner. In expansion, we propose
    to utilize the L1 loss for training. Implicit.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Le [[11](#CR11)]建议对DNN进行训练，采用暗示的深度堆叠。对过去框架的估计覆盖范围进行优化，因为它们在每个DNN训练周期的结尾处，并且，更新后的估计蒙版为其他周期内的DNN训练提供了额外的输入。在测试阶段，DNN以迭代的方式连续进行预测。此外，我们建议使用L1损失进行训练。内含。
- en: Winterrose [[21](#CR21)] claimed that exploring distinctive properties of veritable
    oversees methodologies for recognizing phishing web goals. Phishing URLs utilizing
    significant learning strategies, for the case, profound Boltzmann machine (DBM),
    stacked auto-encoder (SAE), and profound neural organization (DNN). DBM and SAE
    are utilized for pre-preparing the show with a predominant depiction of information
    for attribute assurance. DNN is utilized for twofold gathering in recognizing
    darken URL as either a phishing URL or a genuine URL. The proposed system fulfills
    a higher area rate of 94% with an under most false-positive rate than other machine
    learning procedures.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Winterrose [[21](#CR21)]声称探索真实外贸方法的不同属性，以识别网络钓鱼网址。使用深度玻尔兹曼机（DBM）、堆叠自动编码器（SAE）和深度神经网络（DNN）等深度学习方法来识别网络钓鱼网址。DBM和SAE用于对模型进行预训练，以更好地描述数据以进行属性确定。DNN用于在识别模糊网址时进行二元分类，即将其判定为网络钓鱼网址或真实网址。所提出的方法在高于其他机器学习技术的大多数误报率的情况下，达到了94%的更高区域率。
- en: Rahman [[30](#CR30)] suggested that to detect phishing attack in several anti-phishing
    systems for that reason used six machine learning classifiers (KNN, DT, SVM, RF,
    ERT, and GBT) and three publicly accessible datasets with multidimensional attributes
    could be used due to a lack of proper selection of machine learning classifiers.
    Using confusion matrix, precision, recall, F1-score, accuracy and misclassification
    rate to evaluate the performance of the classifiers. Find better performance that
    obtained from Random Forest and Exceptionally Randomized Tree of 97% and 98% accuracy
    rate for detection of phishing URLs respectively. Gradient Boosting Tree offers
    the best performance with 92% accuracy for multiclass feature set.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Rahman [[30](#CR30)] 建议，在几个反钓鱼系统中检测钓鱼攻击的原因是由于缺乏正确选择机器学习分类器，使用了六种机器学习分类器（KNN、DT、SVM、RF、ERT
    和 GBT）和三个公开可访问的具有多维属性的数据集。使用混淆矩阵、精确率、召回率、F1分数、准确率和误分类率来评估分类器的性能。发现随机森林和极端随机树的性能最佳，分别为97%和98%的准确率来检测钓鱼URL。梯度提升树对多类特征集提供了92%的准确率，性能最佳。
- en: Sahingoz [[31](#CR31)] proposed a real-time anti-phishing process that combines
    seven different classification algorithms also with different feature sets. Through
    using NLP-based Random Forest algorithm, 97.98% accuracy was observed.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Sahingoz [[31](#CR31)] 提出了一种实时反钓鱼过程，结合了七种不同的分类算法，还使用了不同的特征集。通过使用基于自然语言处理的随机森林算法，观察到了97.98%的准确率。
- en: '3 AntiPhishTuner: Proposed Approach'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 反钓鱼调谐器：提出的方法
- en: The proposed approach has been depicted in Fig. [1](#Fig1) and described in
    details step by step in this section.Table 1
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的方法已在图[1](#Fig1)中描述，并在本节中逐步详细描述。表1
- en: Dataset information
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集信息
- en: '| Total features of dataset | 30 features |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 数据集的总特征数 | 30个特征 |'
- en: '| --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Total URLs | 11,055 URLs |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 总URL数量 | 11,055 个URL |'
- en: '| Phishing URLs | 4898 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 钓鱼网址 | 4898 |'
- en: '| Legitimate URLs | 6157 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 合法的网址 | 6157 |'
- en: 3.1 Dataset
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集
- en: A publicly accessible dataset has been used for training or creating the architecture.
    The initial part of this model is to collect data and analyze the datasets. This
    dataset was collected from the UCI repository. It has a total of 11055 different
    types of URLs. It has a total 30 features used to train the model Rahman [[14](#CR14)].
    Table [1](#Tab1) represents the various aspects of the used dataset.Table 2
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练或创建体系结构的公开可访问数据集。此模型的初始部分是收集数据并分析数据集。该数据集是从UCI存储库中收集的。它总共有11055种不同类型的URL。共有30个特征用于训练模型
    Rahman [[14](#CR14)]。表[1](#Tab1)表示所使用数据集的各个方面。表2
- en: Address bar-based features
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于地址栏的特征
- en: '| Name of the features | Explanation |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 特征名称 | 解释 |'
- en: '| --- | --- |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Ip Address | In the event that IP address is utilized as an elective of a
    domain name within the URL that is a phishing website and client can almost be
    sure somebody is attempting to take his credential data . From this dataset, discover
    570 URLs having an IP address which add up to 22.8% of the dataset and proposed
    a rule IP address is in URL that called Phishing, otherwise its Legitimate |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| IP地址 | 如果IP地址被用作URL中域名的替代，则为钓鱼网站，用户几乎可以确定有人试图窃取他的凭据信息。从这个数据集中，发现有570个URL带有IP地址，占数据集的22.8%，提出了一个规则，即URL中有IP地址则称为钓鱼，否则为合法。'
- en: '| Length of URLs | Long URLs are mostly utilized to cover up the dubious portion
    within the address bar because it contains malicious content. Deductively, no
    well-founded length that recognizes phishing URLs from legitimate ones. For that
    legitimate URLs proposed length of the URLs is 75\. In this study to guarantee
    the accuracy measured the length of URLS is suspicious, legitimate or a phishing
    site in this dataset and proposes an average length. From this proposed condition
    the URL length is less than or equal 54 and it is classified as legitimate, if
    the URL is larger than 74 then it is phishing. According to the dataset found
    1220 URLs that’s length greater than or equal 54 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| URL的长度 | 长URL主要用于覆盖地址栏中的可疑部分，因为它包含恶意内容。从逻辑上讲，无法确定识别合法URL和钓鱼URL的长度是否有充分依据。因此，提出了URL的合法长度为75。在本研究中，为了保证测量准确性，测量了此数据集中URL的长度是否可疑、合法或钓鱼网站，并提出了一个平均长度。根据提出的条件，URL长度小于或等于54则被归类为合法，如果URL长度大于74，则属于钓鱼。根据数据集，发现了长度大于或等于54的URL有1220个
    |'
- en: '| TinyURLs | For shortening the URL length tinyURL is used. It diverts to the
    most page to click the shorter URL. This interface is like a phishing site since
    rather than an authentic site it diverts the end client to fake sites |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| TinyURLs | 为了缩短 URL 长度，使用 TinyURL。它将大多数页面重定向到单击较短 URL。此界面类似于钓鱼站点，因为它将最终用户重定向到假站点而不是真实站点
    |'
- en: '| Operate the @ Symbol | Web browsers mostly ignore the segment that is attached
    with @ symbol. Because it is kept away from real addresses. According to the dataset,
    finding 90 URLs that have the ‘@’ symbol will add up to only 3.6% |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 操作 @ 符号 | Web 浏览器主要忽略与 @ 符号附加的部分。因为它远离真实地址。根据数据集，发现具有 ''@'' 符号的 90 个 URL
    仅占总数的 3.6% |'
- en: '| Operate the “//” symbol | After HTTP or HTTPS the “//” symbol is used as
    legitimate URLs. On the off chance that after the initial protocol statement that’s
    considered phishing URLs. “//” symbol is utilized for diverting to other sites
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 操作“//”符号 | 在 HTTP 或 HTTPS 后，使用“//”符号作为合法的 URL。如果在初始协议声明后存在，则认为是钓鱼 URL。 “//”符号用于重定向到其他网站。
    |'
- en: '| Domain names prefix or suffix separated by “-” symbol | If any URL contains
    the “-” symbol in its domain name then consider it’s a phishing URLs. Generally
    validated URLs don’t contain the “-” symbol |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 由“-”符号分隔的域名前缀或后缀 | 如果任何 URL 在其域名中包含“-”符号，则认为是钓鱼 URL。通常，经过验证的 URL 不包含“-”符号
    |'
- en: '| Operate the “.” symbol in domain | Operate the “.” symbol in domain When
    a sub-domain with the domain name is added, it has to include dot. Considering
    suspicious in case drop out more than one subdomain and larger than that will
    point it like a phishing |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 在域中操作“.”符号 | 当添加域名的子域时，必须包含点。如果掉出多个子域并且大于那个大小，将指向它像是钓鱼 |'
- en: '| HTTPS with secure socket layer | Most of the legitimate site HTTPS protocol
    and the age of certificate is exceptionally vital for using HTTPS. For this that’s
    need a trusted certificate |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 具有安全套接字层的 HTTPS | 大多数合法网站使用 HTTPS 协议，证书的年龄对于使用 HTTPS 非常重要。因此需要受信任的证书 |'
- en: '| Expiry date of domain | Principally domain name have longer expiry date for
    legitimate sites |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 域的到期日期 | 主要域名具有较长的到期日期，适用于合法站点 |'
- en: '| Favicon | Favicon can divert clients to suspicious sites, when it is stacked
    from outside space. It’s by and large utilized in websites and it’s a graphic
    image |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Favicon | 当从外部加载时，Favicon 可以将用户重定向到可疑网站。它通常用于网站，是一种图形图像 |'
- en: '| Utilizing insignificant ports | Phishers continuously discover defenselessness
    and attempt to require an advantage on the off chance that any URLs has some open
    ports that’s superfluous |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 利用不必要的端口 | 钓鱼者不断发现漏洞，并试图利用任何具有不必要的开放端口的 URL |'
- en: '| HyperText Transfer Protocol in domain | The phishing websites are considered
    if any URLs of this website have HTTPS on domain name |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 域中的超文本传输协议 | 如果此网站的任何 URL 具有 HTTPS，则认为是钓鱼网站 |'
- en: 3.2 Feature Description
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 功能描述
- en: Before analyzing the features selection part, features and the ability to use
    these features need to be evaluated. Basically, there are four primary features
    and a total of 30 sub-features. Based on the details, each feature offers details
    as to whether the website may be phishing, legitimate or suspicious. This segment
    provides the planning to point up the features.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析特征选择部分之前，需要评估功能和使用这些功能的能力。基本上，有四个主要功能和总共 30 个子功能。根据详细信息，每个功能提供有关网站可能是钓鱼、合法或可疑的详细信息。此段提供了强调这些功能的计划。
- en: '**1\. Address bar-based features:** The address bar that means URL bar or location
    bar could be a GUI gadget that appears in an ongoing URL. According to the dataset
    it has 12 sub-features. That is appeared on the Table [2](#Tab2) below.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 基于地址栏的特征:** 地址栏，即 URL 栏或位置栏，是显示当前 URL 的 GUI 设备。根据数据集，它有 12 个子功能。请参见下表
    [2](#Tab2)。|'
- en: '**2\. Abnormal Based Features:** It for the most part centers on abnormal exercises
    on the site. According to the dataset it has 5 sub-features. That appears on the
    Table [3](#Tab3) below.Table 3'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 基于异常的特征:** 它主要关注网站上的异常活动。根据数据集，它有 5 个子功能。请参见下表 [3](#Tab3)。Table 3'
- en: Abnormal based features
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于异常的特征
- en: '| Name of the features | Explanation |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 功能名称 | 说明 |'
- en: '| --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Request URL | From another domain on the off chance that a page contains
    larger amount of outside URLS that’s considered it suspicious or phishing |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 请求 URL | 如果页面包含大量来自其他域的 URL，则认为是可疑或钓鱼的 |'
- en: '| Having URL of anchor | Comparable to the request URL features, the chance
    of phishing increases, more<a> tags utilized inside the site |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 锚文本的URL | 类似于请求URL特性，使用更多的<a>标签在网站内使用时，钓鱼的可能性会增加 |'
- en: '| Link among (Meta, script, Link) tag | It is calculated as either suspicious
    or phishing formed on their proportion if the tag contains large number of outer
    links |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| (Meta, script, Link)标签之间的链接 | 根据其包含的外部链接数量，标签中包含大量外部链接时被认为是可疑或钓鱼的 |'
- en: '| Server form handler | Phishing is considered in case the Server shape handler
    is blank or empty. Server frame handler diverts to a distinctive domain It’s checked
    as suspicious |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 服务器形式处理程序 | 如果服务器形式处理程序为空，则被认为是钓鱼。服务器形式处理程序重定向到不同的域，这被视为可疑 |'
- en: '| Having an email to submitting information | It is considered as phishing,
    rather than a server, web form coordinated to an individual email is submitted
    the information |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 提交信息的电子邮件 | 与服务器不同，将Web表单定向到个人电子邮件提交信息被视为钓鱼 |'
- en: '| Abnormal URLs | It considered as phishing, In case the character isn’t included
    within the URLs |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 异常的URL | 如果URL中不包含字符，则被视为钓鱼 |'
- en: '**3\. HTML and JavaScript based features:** According to the dataset it has
    5 sub-features. That appears on Table [4](#Tab4) below.Table 4'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 基于HTML和JavaScript的特性：** 根据数据集，它有5个子特性。见下表 [4](#Tab4) 。表4'
- en: HTML and JavaScript based features
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基于HTML和JavaScript的特性
- en: '| Name of the features | Explanation |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 特性名称 | 解释 |'
- en: '| --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Forwarding website | It can be frightening, on the off chance that diverting
    is happened different times |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 转发网站 | 如果发生多次重定向，则可能会让人害怕 |'
- en: '| Customization of status bar | To alter the status bar of the URLs can be
    utilized on “Mouseover” occasion. It continuously appears off genuine URLs and
    stows away the fake URLs. at a time When it’s connected on the site that’s obliging
    as phishing |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 状态栏的自定义 | 通过“Mouseover”事件可以修改URL的状态栏。它会持续显示真实的URL并隐藏伪造的URL。当应用于网站时，这会被视为钓鱼
    |'
- en: '| Right click disabled | Users can’t check the source code; right-click functions
    are impaired mainly by Phishers. When the framework is debilitated within the
    site that’s obliging as phishing |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 禁用右键点击 | 用户无法检查源代码；右键点击功能主要由钓鱼者禁用。当网站中禁用该功能时，这是作为钓鱼的 |'
- en: '| Having Pop-up Window | Pop-up window with a text field is consisted by a
    web page that’s obliging as phishing |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 弹出窗口 | 具有文本字段的弹出窗口由网页组成，这是作为钓鱼的 |'
- en: '| Custom IFrame | Stowing them away within the website phisher could be utilized
    IFrame. In for the most part Connect outside substance to appear in a domain utilized
    by IFrame |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 定制IFrame | 将它们隐藏在网站钓鱼者中可以利用IFrame。通常来说，将外部内容连接到IFrame使用的域中以显示 |'
- en: '**4\. Domain based features:** Using domain names prepares effortlessly identifiable
    and unforgettable names numerically. According to the dataset it has 7 sub-features.
    That appears on Table [5](#Tab5) below.Table 5'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 基于域的特性：** 使用域名准备易于识别和记忆的数字名称。根据数据集，它有7个子特性。见下表 [5](#Tab5) 。表5'
- en: Domain based features
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 基于域的特性
- en: '| Name of the features | Explanation |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 特性名称 | 解释 |'
- en: '| --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Age of domain | Obliging a authentic site as phishing site tend to live for
    shorter period of time in the event that the age of domain is longer than six
    month |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 域名年龄 | 如果域名的年龄超过六个月，则倾向于将合法网站视为钓鱼网站 |'
- en: '| Record of DNS | It is exceedingly recommended as phishing site within the
    event DNS record isn’t contained by website |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| DNS记录 | 如果网站中不包含DNS记录，则强烈建议作为钓鱼网站 |'
- en: '| Traffic of website | Colossal amount of individuals visit websites for the
    most part because it would have higher positioning. Positioning can distinguish
    on the off chance that a location is phishing or not. A phishing site is being
    tends to have a lower chance by the next ranked site |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 网站流量 | 大量人们访问网站主要是因为它可能具有较高的排名。排名可以识别一个地址是否是钓鱼的。钓鱼网站往往比排名较高的网站具有更低的几率 |'
- en: '| Ranking of page | In most time that phishing websites have no PageRank value
    since this value is allotted on its importance |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 页面排名 | 大多数情况下，钓鱼网站没有PageRank值，因为该值是根据其重要性分配的 |'
- en: '| Indexing of Google | A legitimate site can be accepted by a site that has
    a title on the google index |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 谷歌索引 | 在谷歌索引中具有标题的网站可以被接受为合法网站 |'
- en: '| Reports statistical | Guessing it as phishing webpage within the event the
    have of the webpage has a place in any beat phishing IP’s or domains |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 报告统计 | 在事件中猜测它是否为网络钓鱼网页，如果网页的任何部分在任何击败网络钓鱼IP或域中有位置 |'
- en: '| Joins indicating to page | Phishing site prohibiting have much links indicating
    apropos it since it has shorter lifetime |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 加入表明页面 | 网络钓鱼网站禁止有指向它的链接，因为它的生命周期较短 |'
- en: 3.3 Deep Learning Algorithm
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 深度学习算法
- en: This study has considered five adaptive optimizer such as Stochastic Gradient
    Descent (SGD), ADAM, ADADELTA, ADAGARD, and RMSPROP used for evaluation of NN
    and DNN.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究考虑了五种自适应优化器，例如随机梯度下降（Stochastic Gradient Descent，SGD）、ADAM、ADADELTA、ADAGARD
    和 RMSPROP，用于对NN和DNN进行评估。
- en: 3.4 Machine Leraning Algorithm
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 机器学习算法
- en: This study has considered some machine learning algorithms for stacking such
    as Support Vector Machine (SVM), Decision Tree (DT), Naive Bayes (GNB), Linear
    Discriminant Analysis (LDA), Random Forest (RF), Multilayer Perceptron (MLP),
    Stochastic Gradient Descent (SGD), Logistic Regression (LR), k nearest neighbors
    (KNN) and Gaussian are used for Stacked Generalization as a base classifiers in
    first step, and 10-fold cross validation Adebowale [[3](#CR3)] has been used.
    Here, XGBoost classifier is being used as meta estimator for final prediction
    in second step.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究考虑了一些用于堆叠的机器学习算法，例如支持向量机（Support Vector Machine，SVM）、决策树（Decision Tree，DT）、朴素贝叶斯（Naive
    Bayes，GNB）、线性判别分析（Linear Discriminant Analysis，LDA）、随机森林（Random Forest，RF）、多层感知器（Multilayer
    Perceptron，MLP）、随机梯度下降（Stochastic Gradient Descent，SGD）、逻辑回归（Logistic Regression，LR）、k近邻（k
    nearest neighbors，KNN）和高斯，用作堆叠泛化的基本分类器的第一步，并且使用了10折交叉验证 Adebowale [[3](#CR3)]。在这里，XGBoost分类器被用作第二步中的元估计器进行最终预测。
- en: 3.5 Model Generation Phase
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 模型生成阶段
- en: 'The above methodology indicates three types of multilayer approaches: NN, DNN
    and stack generalization respectively. The main purpose of this model is to determine
    the best output through evaluation by applying stacking technique and neural network
    and deep neural network on the processed data set and to propose an optimized
    model based on that output.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法表明了三种多层次方法：NN、DNN 和堆叠泛化。该模型的主要目的是通过应用堆叠技术和神经网络以及深度神经网络对处理后的数据集进行评估，确定最佳输出，并提出基于该输出的优化模型。
- en: Now an optimized output will be provided by applying neural network and deep
    neural network technique on this dataset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过在该数据集上应用神经网络和深度神经网络技术，将提供一个优化的输出。
- en: After loading the features from the dataset, the data set is split into two
    parts, test and train. The train segment is applied to a two-layer neural network
    architecture and Somesha [[16](#CR16)] a five-layer deep neural network architecture,
    respectively.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在从数据集加载特征后，数据集被分成两部分，测试集和训练集。训练集部分分别应用于两层神经网络架构和 Somesha [[16](#CR16)] 的五层深度神经网络架构。
- en: Since the data set is of binary type, for binary classification problem non-linear
    activation function ReLU have been used for hidden layers of neurons and sigmoid
    function have been used for output layers of neurons Vrbančič [[19](#CR19)]. According
    to this architecture, five types of adaptive optimizer have been used here.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集是二进制类型的，对于二分类问题，非线性激活函数ReLU已经被用于隐藏层的神经元，而Sigmoid函数已经被用于输出层的神经元 Vrbančič
    [[19](#CR19)]。根据这个架构，这里使用了五种类型的自适应优化器。
- en: The next step is to compile the model using these adaptive optimizers. It is
    then divided into two parts, train and validation, by splitting the train set.
    The model is fitted using a number of epochs and early stopping techniques, to
    prevent overfitting. Now two outputs are available by evaluating the two models
    using the test set. After applying the approach, stack generalization technique
    has been applied in the dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用这些自适应优化器编译模型。然后，通过分割训练集将其分为两部分，训练和验证。使用一些周期和早期停止技术来适应模型，以防止过拟合。现在通过使用测试集评估两个模型可以得到两个输出。应用这种方法后，堆叠泛化技术已经在数据集中应用。
- en: The evaluation technique of stack generalization has been described in figure.
    It’s a multilevel approach. Stacking is usually done in two steps. In the first
    level stacking provides transitory prediction using based on classifiers with
    k-fold cross validation and output probability prediction are revealed. During
    the system formation the output prediction and transitory prediction of step one
    are used in second steps. The estimate theory of phish stack are described below
    Rahman [[14](#CR14)]:![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig1_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig1_HTML.png)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠的评估技术已在图中描述。这是一种多层次方法。堆叠通常分两步进行。在第一层堆叠中，使用基于分类器的 k 折交叉验证提供临时预测，并且输出概率预测被揭示。在系统形成期间，第一步的输出预测和临时预测在第二步中使用。Phish
    堆叠的估计理论如下所述 Rahman [[14](#CR14)]:![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig1_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig1_HTML.png)
- en: Fig. 1
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: Methodology for phishing URLs detection
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 针对钓鱼 URL 检测的方法论
- en: In the first step of stacking by using base classifiers to predict train and
    test set according to the second step the desired predictions are being acquired
    then that are considered as features.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用基分类器进行堆叠的第一步中，根据第二步的预期预测来预测训练集和测试集，然后将其视为特征。
- en: Stacking is a multilevel approach so any kind of algorithm can be used to predict
    it in two steps.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠是一种多层方法，因此可以使用任何类型的算法在两个步骤中进行预测。
- en: This proposed system used k-fold cross-validation so that it eluded overfitting
    for this training set and each fold of the train portion it may predict using
    out-of-fold. According to this proposed model the value three to ten is used for
    k-fold cross validation after all provides output using a test set.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该提出的系统使用 k 折交叉验证，以避免过拟合该训练集，并且在训练部分的每个折叠中，它可能使用折叠外的数据进行预测。根据这个提出的模型，k 折交叉验证中使用值
    3 到 10，最后使用测试集提供输出。
- en: In the first step at the end of training the data the output is predicted using
    the test set. This time it’s complete with all folds technique that’s needed to
    mean for estimating all values from all folds that are used.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在培训数据结束时的第一步中，使用测试集预测输出。这一次，它使用了所有折叠技术，这需要对所有折叠中的所有值进行平均以进行估计。
- en: In the second step connected to another classifier that’s called a meta-estimator
    on the train set, from the test set it performs terminal prediction. This approach
    takes extra time because it again adds a classifier for its performances. When
    the k-fold cross validation done in the first step then prediction is not completed
    these are completed on the second step.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，连接到另一个称为元估计器的分类器，它在训练集上执行终端预测，从测试集中进行预测。这种方法需要额外的时间，因为它再次添加了一个分类器来进行其性能。当第一步中进行
    k 折交叉验证时，预测尚未完成，这些在第二步中完成。
- en: Three outputs are obtained from the above multilayer techniques then a model
    is selected based on the decision, according to the value of the output. An optimized
    architecture is proposed based on that model.Table 6
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述多层技术中获得三个输出，然后根据输出的值选择模型。基于该模型提出了优化的架构。表 6
- en: Evaluation parameters
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 评估参数
- en: '| Assessment parameter | Assessment parameters formula | Statement of the assessment
    parameter |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 评估参数 | 评估参数公式 | 评估参数说明 |'
- en: '| --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Mean Absolute Error (MAE) | ![$${\text {MAE = }}\frac{{\sum \nolimits _{{i
    = 1}}^{n} {\left&#124; {y_{i} - x_{i} } \right&#124; } }}{n}$$](../images/507793_1_En_9_Chapter/507793_1_En_9_Chapter_TeX_IEq1.png)
    | It is the average value of all absolute errors [[5](#CR5)] |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 平均绝对误差（MAE） | ![$${\text {MAE = }}\frac{{\sum \nolimits _{{i = 1}}^{n} {\left|#
    {y_{i} - x_{i} } \right| } }}{n}$$](../images/507793_1_En_9_Chapter/507793_1_En_9_Chapter_TeX_IEq1.png)
    | 这是所有绝对误差的平均值 [[5](#CR5)] |'
- en: '| Mean Square Error (MSE) | ![$${\text {MSE = }}\frac{1}{n}\sum \limits _{{i
    = 1}}^{n} {\left( {Y_{i} - \hat{{Y_{i} }}} \right) ^{2} }$$](../images/507793_1_En_9_Chapter/507793_1_En_9_Chapter_TeX_IEq2.png)
    | It is the average value of all squares errors |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 均方误差（MSE） | ![$${\text {MSE = }}\frac{1}{n}\sum \limits _{{i = 1}}^{n} {\left(
    {Y_{i} - \hat{{Y_{i} }}} \right) ^{2} }$$](../images/507793_1_En_9_Chapter/507793_1_En_9_Chapter_TeX_IEq2.png)
    | 这是所有平方误差的平均值 |'
- en: '| AUC-ROC curve | For Positive Recall TRP = TP/(TP + FN). For Negative Recall
    FPR = 1 − Specificity = 1 − TN/(TN+FP) = FP/TN+FP | AUC - ROC curve is intrigued
    with True Positive Rate that belongs on y-axis, in opposition to the False Positive
    Rate that belongs on x-axis [[1](#CR1)] |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| AUC-ROC曲线 | 对于正召回 TRP = TP/(TP + FN)。对于负召回 FPR = 1 − 特异性 = 1 − TN/(TN+FP)
    = FP/TN+FP | AUC - ROC曲线是与真正率相关的，它位于y轴上，与假正率相反，后者位于x轴上[[1](#CR1)] |'
- en: '| Precision - Recall Curve | For Positive Precision P = TP/(TP + FP) For Negative
    Precision N = TN/(TN+FN) For Positive Recall PR = TP/(TP + FN) For Negative Recall
    NR = TN/(TN+FP) | According to the precision-recall curve for a single classifier,
    estimating and intrigued the precision in opposition to the recall [[12](#CR12)]
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 - 召回率曲线 | 对于正精确度 P = TP/(TP + FP) 对于负精确度 N = TN/(TN+FN) 对于正召回 PR = TP/(TP
    + FN) 对于负召回 NR = TN/(TN+FP) | 根据单个分类器的精确度-召回率曲线，估计和研究了精确度与召回率的关系[[12](#CR12)]
    |'
- en: '| Accuracy | Accuracy = (TP + TN)/(TP + TN + FP + FN) | Accuracy means the
    rate of prediction that model executes [[15](#CR15)] |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 准确率 = (TP + TN)/(TP + TN + FP + FN) | 准确率指的是模型执行的预测率[[15](#CR15)] |'
- en: '| Misclassification rate | Error Rate = 1 − Accuracy | The failings of identify
    value that is not appropriate for classification |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 分类错误率 | 错误率 = 1 − 准确率 | 未能识别不适合分类的值 |'
- en: 4 Result and Discussion
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果与讨论
- en: 4.1 Environment Setup
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 环境设置
- en: The experiment that has been conducted is Intel(R) Core(TM) i3-7100 CPU @2.40 GHz
    processor, 64-bit PC with 4 GB RAM. The operating system is Windows 10 pro Education
    and python has been used to implement the architecture To Detect Phishing URLs
    with the packages of python such that TensorFlow, scikit-learn, Keras, Pandas,
    and NumPy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所进行的实验是在Intel(R) Core(TM) i3-7100 CPU @2.40 GHz处理器、64位PC上进行的，配备了4 GB RAM。操作系统为Windows
    10 pro Education，python被用来实现检测钓鱼网址的架构，所用到的python包括TensorFlow、scikit-learn、Keras、Pandas和NumPy。
- en: 4.2 Evaluation Parameters
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估参数
- en: 'The system was mainly focused on evaluation based on data phishing or legitimate
    that’s identified by binary classification. Confusion matrix, Accuracy, Precision-Recall
    Curve, Classification report, AUC-ROC Curve, Mean Absolute Error (MAE), Mean square
    Error (MSE) used to evaluate the performance of this system. The evaluation parameters
    [[14](#CR14)] for assessment are described in the Table [6](#Tab6):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统主要侧重于基于数据钓鱼或二元分类识别的评估。混淆矩阵、准确率、精确度-召回率曲线、分类报告、AUC-ROC曲线、平均绝对误差（MAE）、均方误差（MSE）用于评估该系统的性能。评估参数[[14](#CR14)]如表[6](#Tab6)所述：
- en: 4.3 Experiment Result
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 实验结果
- en: The representation of the various optimizers of DNN and NN was shown in Tables
    [7](#Tab7) and [8](#Tab8). Five separate adaptive optimizers have been used for
    this experiment, the number of hidden layers, the learning rate and the epoch
    size are considered HL, LR and EPS respectively. According to this condition,
    HL5 means the number of hidden layers is 5 and the number of hidden layer 2 is
    HL2\. For this evaluation, 15 types of learning rate and 10 types of epoch size
    were used for 20 times iteration for these five optimizer’s. After 20-fold iteration,
    have chosen a better combination of epoch size and learning rate to achieve optimized
    performance so that this model is more accurate.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: DNN和NN的各种优化器的表示在表[7](#Tab7)和表[8](#Tab8)中展示出来。对于此实验使用了五种单独的自适应优化器，隐藏层的数量、学习率和迭代大小被考虑为HL、LR和EPS。根据这个条件，HL5表示隐藏层数为5，而隐藏层2的数量是HL2。对于这个评估，使用了15种学习率和10种迭代大小，对这五个优化器进行了20次迭代。经过20次迭代后，选择了更好的迭代大小和学习率组合，以达到优化的性能，使得此模型更加准确。
- en: '4.3.1 Case Study #1'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '4.3.1 案例研究 #1'
- en: Evaluation rate of five Adaptive Optimizer with accuracy and loss for DNN.![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig2_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig2_HTML.png)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: DNN的五个自适应优化器的准确率和损失的评估率。![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig2_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig2_HTML.png)
- en: Fig. 2
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: Accuracy and loss DNN five optimizer
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率和损失DNN五个优化器
- en: As illustrate in Fig. [2](#Fig2) have shown that used different deep learning
    adaptive optimizer to take the decision which optimizer would be the best for
    anti-phishing proposed model. In this case Adam optimizer given the highest accuracy
    among all the optimizer where SGD optimizer given slightly low performance. On
    the contrary, model optimizer loss their performance while tuning the model for
    the prediction of proposed model with the selected optimizer. Where every optimizer
    loss their performance based on their adaptive quality. In this case being understand
    to take the optimizer based on their performance and loss accuracy for the shake
    which optimizer will be the best fit for anti-phishing proposed model.![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig3_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig3_HTML.png)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#Fig2) 所示，使用不同的深度学习自适应优化器来决定哪个优化器最适合反钓鱼提出的模型。在这种情况下，Adam优化器在所有优化器中获得了最高的准确度，而SGD优化器的性能稍微低一些。相反，当使用所选的优化器调整模型以预测所提出的模型时，模型优化器会失去性能。在这种情况下，理解根据性能和损失准确度选择优化器是最适合反钓鱼提出的模型的摇摆。
- en: Fig. 3
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3
- en: Different optimizer for ROC curve and Precision-Recall curve for DNN
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 用于DNN的ROC曲线和精度-召回曲线的不同优化器
- en: The ROC curve and precision-Recall curve the have been shown in Fig. [3](#Fig3).
    Maximum accuracy 0.955 attained from Adam individually. In case of precision-recall
    curve and the AUC-ROC curve SGD and AdaGard do better provides 0.96\. SGD and
    AdaGard perform better in ROC curve and precision-Recall curve than others.Table
    7
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线和精度-召回曲线在图 [3](#Fig3) 中已经显示。最大准确度0.955是从Adam单独获得的。在精度-召回曲线和AUC-ROC曲线方面，SGD和AdaGard提供了更好的性能。在ROC曲线和精度-召回曲线中，SGD和AdaGard的性能比其他优化器更好。表
    7
- en: Evaluation table for DNN
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: DNN的评估表
- en: '| Serial | Optimizer | Label | Learning rate | Epochs | Accuracy | MSE | MAE
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 序号 | 优化器 | 标签 | 学习率 | Epochs | 准确度 | MSE | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | Adam | HL5 | 0.01 | 50 | 0.955 | 0.030 | 0.074 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 1 | Adam | HL5 | 0.01 | 50 | 0.955 | 0.030 | 0.074 |'
- en: '| 2 | SGD | HL5 | 0.001 | 100 | 0.951 | 0.021 | 0.049 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 2 | SGD | HL5 | 0.001 | 100 | 0.951 | 0.021 | 0.049 |'
- en: '| 3 | RMSprop | HL5 | 0.0003 | 150 | 0.953 | 0.028 | 0.076 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 3 | RMSprop | HL5 | 0.0003 | 150 | 0.953 | 0.028 | 0.076 |'
- en: '| 4 | AdaDelta | HL5 | 0.0027570 | 250 | 0.954 | 0.023 | 0.078 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 4 | AdaDelta | HL5 | 0.0027570 | 250 | 0.954 | 0.023 | 0.078 |'
- en: '| 5 | AdaGard | HL5 | 0.0017470 | 150 | 0.953 | 0.018 | 0.049 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 5 | AdaGard | HL5 | 0.0017470 | 150 | 0.953 | 0.018 | 0.049 |'
- en: The analysis shows clearly in Table [7](#Tab7) that the learning rate has an
    essential contribution to the success of profound neural systems among all the
    measurement or appraisal parameters. It was found that the maximum accuracy of
    0.955, MSE 0.030 and MAE 0.074 of the hidden five layers using Adam optimizer,
    along with the 50 epochs and 0.01 learning rate (HL5 EPs50). Observing all the
    outcomes from Table [7](#Tab7) from above, it can be observe that all the optimizer
    provides 95% accuracy of which Adam pays a little more, Adam is the top scorer
    Vrbančič [[19](#CR19)].
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 分析清楚地显示在表 [7](#Tab7) 中，学习速率对所有测量或评估参数中深度神经系统的成功有着重要的贡献。发现使用Adam优化器的五层隐藏层的最大准确度为0.955，MSE为0.030，MAE为0.074，以及50个epochs和0.01的学习速率（HL5
    EPs50）。从上面观察表 [7](#Tab7) 的所有结果可以看出，所有的优化器都提供了95%的准确度，其中Adam稍微多一点，Adam是顶尖得分者Vrbančič
    [[19](#CR19)]。
- en: '4.3.2 Case Study #2'
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '4.3.2 案例研究 #2'
- en: Evaluation rate of five Adaptive Optimizer with accuracy and loss for NN![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig4_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig4_HTML.png)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 五个自适应优化器在NN的准确度和损失评估率![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig4_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig4_HTML.png)
- en: Fig. 4
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图4
- en: Accuracy and loss NN five optimizer
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的准确度和损失五个优化器
- en: As discussed before in Fig. [2](#Fig2) similarly in this phase according to
    the illustration in Fig. [4](#Fig4) used different deep learning adaptive optimizer
    where AdaGard optimizer given the highest accuracy among all the optimizer for
    two layer NN where both Adam and RMSprop optimizer given slightly low performance
    for the NN two layer. According to their performance, model have been loss their
    performance while evaluated the model to find the best optimizer if two NN layer
    used for all the adaptive optimizer.![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig5_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig5_HTML.png)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在图 [2](#Fig2) 中类似地，在这个阶段根据图 [4](#Fig4) 中的说明使用不同的深度学习自适应优化器，其中 AdaGard 优化器在所有优化器中为两层
    NN 提供了最高的准确性，而 Adam 和 RMSprop 优化器为 NN 两层提供了略低的性能。根据它们的性能，模型在评估模型时会失去性能，以找到最佳优化器，如果两个
    NN 层用于所有自适应优化器。![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig5_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig5_HTML.png)
- en: Fig. 5
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5
- en: Different optimizer ROC curve and Precision-Recall curve for NN
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 不同优化器的 ROC 曲线和 Precision-Recall 曲线用于 NN
- en: The ROC curve and precision-Recall curve the have been shown in Fig. [5](#Fig5).
    Maximum accuracy 0.955 attained from AdaGard individually. In case of precision-recall
    curve and the AUC-ROC curve Adam, SGD, AdaDelta and AdaGard do better provides
    0.95, expect RMSprop.Table 8
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线和 Precision-Recall 曲线已在图 [5](#Fig5) 中显示。从 AdaGard 单独获得最大准确率为 0.955。在 Precision-Recall
    曲线和 AUC-ROC 曲线方面，Adam、SGD、AdaDelta 和 AdaGard 表现更好，提供 0.95，期望 RMSprop。表 8
- en: Evaluation table for NN
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: NN 的评估表
- en: '| Serial | Optimizer | Label | Learning rate | Epochs | Accuracy | MSE | MAE
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Serial | Optimizer | Label | 学习率 | Epochs | 准确率 | MSE | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | Adam | HL2 | 0.0017470 | 150 | 0.948 | 0.014 | 0.058 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 1 | Adam | HL2 | 0.0017470 | 150 | 0.948 | 0.014 | 0.058 |'
- en: '| 2 | SGD | HL2 | 0.001 | 128 | 0.945 | 0.026 | 0.086 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 2 | SGD | HL2 | 0.001 | 128 | 0.945 | 0.026 | 0.086 |'
- en: '| 3 | RMSprop | HL2 | 0.0003 | 200 | 0.948 | 0.026 | 0.080 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 3 | RMSprop | HL2 | 0.0003 | 200 | 0.948 | 0.026 | 0.080 |'
- en: '| 4 | AdaDelta | HL2 | 0.0027570 | 250 | 0.949 | 0.016 | 0.067 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 4 | AdaDelta | HL2 | 0.0027570 | 250 | 0.949 | 0.016 | 0.067 |'
- en: '| 5 | AdaGard | HL2 | 0.0017470 | 150 | 0.955 | 0.030 | 0.074 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 5 | AdaGard | HL2 | 0.0017470 | 150 | 0.955 | 0.030 | 0.074 |'
- en: From the experiment it’s clearly shown in Table [8](#Tab8) that the learning
    rate has an essential contribution to the success of profound neural systems among
    all the measurement or appraisal parameters. It was found that the maximum accuracy
    of 0.955, MSE 0.030 and MAE 0.074 of the hidden five layers using AdaGard optimizer,
    along with the 150 epochs and 0.0017470 learning rate (HL2, EPs150) which is slightly
    near with the DNN.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从实验中清楚地显示，在表 [8](#Tab8) 中学习率对于所有测量或评估参数中深度神经系统的成功具有重要贡献。发现使用 AdaGard 优化器的隐藏五层的最大准确度为
    0.955，MSE 为 0.030，MAE 为 0.074，并且 150 个 epoch 和 0.0017470 的学习率（HL2，EPs150）与 DNN
    略有接近。
- en: '4.3.3 Case Study #3'
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '4.3.3 案例研究 #3'
- en: The main purpose of stacked generalization is used a higher grade model to combine
    low grade models to achieve higher predictive accuracy. Stacking combines multiple
    model and learns it up for classification task.Table 9
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠泛化的主要目的是使用更高级别的模型来组合低级别的模型，以实现更高的预测准确性。堆叠将多个模型结合起来，并为分类任务学习它。表 9
- en: Accuracy of machine learning classifier algorithm
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习分类算法的准确度
- en: '| LR | LDA | KNN | DT | GNB | SVM |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| LR | LDA | KNN | DT | GNB | SVM |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0.927 | 0.921 | 0.936 | 0.955 | 0.593 | 0.944 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 0.927 | 0.921 | 0.936 | 0.955 | 0.593 | 0.944 |'
- en: According to Table [9](#Tab9), first of all here 6 machine learning algorithms
    are used on the data of the desired dataset then some accuracy is found on the
    basis of that algorithm. These algorithm are used to build a stack model.Table
    10
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表 [9](#Tab9)，首先在所需数据集的数据上使用了 6 种机器学习算法，然后根据该算法找到了一些准确度。这些算法用于构建堆栈模型。表 10
- en: Build model stack and the increased accuracy of Machine learning Algorithm
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型堆叠及机器学习算法的增加准确度
- en: '| LR | LDA | KNN | DT | GNB | SVM |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| LR | LDA | KNN | DT | GNB | SVM |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0.966 | 0.965 | 0.965 | 0.966 | 0.965 | 0.966 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 0.966 | 0.965 | 0.965 | 0.966 | 0.965 | 0.966 |'
- en: In this step a stack model is generated by applying these algorithm. According
    to Table [10](#Tab10) this algorithm have changed in their accuracy after generating
    a stack model. The stack stipulates that it combines multiple models and learns
    for classification task. So purpose of this step is to stack learn stack.Table
    11
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，通过应用这些算法生成了一个栈模型。根据表[10](#Tab10)，在生成栈模型后，这些算法的准确性发生了变化。栈规定它结合了多个模型并学习用于分类任务。因此，这一步的目的是堆栈学习堆栈。表11
- en: Misclassification rate and accuracy of temporary prediction
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 临时预测的误分类率和准确率
- en: '| Algorithm | Accuracy | Misclassifation rate |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 准确率 | 误分类率 |'
- en: '| --- | --- | --- |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| RF | 0.96 | 0.0047 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| RF | 0.96 | 0.0047 |'
- en: '| DT | 0.95 | 0.0063 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| DT | 0.95 | 0.0063 |'
- en: '| MLP | 0.96 | 0.0022 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| MLP | 0.96 | 0.0022 |'
- en: '| SVM | 0.94 | 0.0072 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| SVM | 0.94 | 0.0072 |'
- en: '| SGD | 0.91 | 0.0073 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| SGD | 0.91 | 0.0073 |'
- en: '| GNB | 0.59 | 0.012 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| GNB | 0.59 | 0.012 |'
- en: The stack has already been learned, now it knows how to process a model. Table
    [11](#Tab11) represent the final accuracy and misclassification rate for first
    step. This work is done by two steps. So this table’s value indicates the first
    step’s prediction or temporary prediction because after second step prediction
    will find final prediction. The next step is to build a model, according to the
    study a model has been created XGBClassifier and through that model fitted the
    previous trained data and predict the final results.![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig6_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig6_HTML.png)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 栈已经学会了，现在它知道如何处理模型。表[11](#Tab11)代表第一步的最终准确率和误分类率。这项工作分为两步完成。因此，该表的数值表示第一步的预测或临时预测，因为在第二步预测之后将找到最终预测。接下来的步骤是构建模型，根据研究已经创建了一个XGBClassifier模型，通过该模型拟合了先前训练的数据并预测了最终结果。![../images/507793_1_En_9_Chapter/507793_1_En_9_Fig6_HTML.png](../images/507793_1_En_9_Chapter/507793_1_En_9_Fig6_HTML.png)
- en: Fig. 6
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图6
- en: Different algorithms ROC curve and Precision-Recall curve
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 不同算法的ROC曲线和Precision-Recall曲线
- en: The precision-Recall curve and the ROC curve have been shown in Fig. [6](#Fig6).
    The first step shows that the maximum accuracy 0.96 with minimum error rate. RF
    and MLP do better individually where precision-recall curve and the AUC-ROC curve,
    stacked generalization performs low. However in the time of final prediction stack
    generalization provides accuracy 0.97.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Precision-Recall曲线和ROC曲线已在图[6](#Fig6)中显示。第一步显示最大准确率为0.96，最小误差率。RF和MLP在个别情况下表现更好，其中Precision-Recall曲线和AUC-ROC曲线，堆叠泛化性能较低。然而，在最终预测时，堆栈泛化提供了0.97的准确率。
- en: 4.3.4 Discussion on the Difference Among the Three Multilayer Approaches
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 对三种多层方法的差异进行讨论
- en: '**Stack Generalization**Table 12'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**栈泛化**表12'
- en: Evaluation rate of stacking
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠评估率
- en: '| Algorithm | Accuracy | Accuracy of stack generalization |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 准确率 | 栈泛化的准确率 |'
- en: '| --- | --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| LR | 0.927 | 0.966 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| LR | 0.927 | 0.966 |'
- en: '| LDA | 0.921 | 0.965 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| LDA | 0.921 | 0.965 |'
- en: '| KNN | 0.936 | 0.965 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| KNN | 0.936 | 0.965 |'
- en: '| DT | 0.955 | 0.966 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| DT | 0.955 | 0.966 |'
- en: '| GNB | 0.953 | 0.965 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| GNB | 0.953 | 0.965 |'
- en: '| SVM | 0.944 | 0.966 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| SVM | 0.944 | 0.966 |'
- en: In this study binary classification type dataset has been selected for evaluation
    of three multilayer approaches. It is a well-known fact that machine learning
    algorithms provide good results for binary classification type datasets. The main
    features of stack generalization is that it integrates with low grade models using
    high grade models and also known as ensemble algorithm that basically works in
    two layers. According to the stacking concept, it learns multiple machine learning
    algorithms in the first layer, and then gives the predictions as an output which
    is used as the learning of another algorithm in the second layer. The machine
    learning algorithm used as the final predictor, this learning is more error-free.
    The main target of stack generalization is to develop the result of low grade
    models Li [[28](#CR28)] New model is trained by other models that are already
    trained from a dataset. Most commonly stacking uses simple linear function (mean,
    median, average etc.) to assemble the prediction for other models. According to
    the stacking techniqe for binary classification type data sets will provide more
    accuracy than NN and DNN. So optimized output can be obtained using this stacking
    concept (Table [12](#Tab12)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，选择了二元分类类型数据集来评估三种多层方法。众所周知，机器学习算法对二元分类类型的数据集提供了良好的结果。栈泛化的主要特点是，它将低等级模型与高等级模型集成在一起，也被称为集成算法，基本上是在两层中工作。根据堆叠的概念，它在第一层学习多种机器学习算法，然后将预测作为输出，该输出用作第二层中另一种算法的学习。作为最终预测器使用的机器学习算法，这种学习更加无误。栈泛化的主要目标是开发低等级模型的结果，Li
    [[28](#CR28)]新模型是由已经从数据集中训练过的其他模型训练的。通常使用简单线性函数（平均值、中位数、平均值等）来组装其他模型的预测。根据用于二元分类类型数据集的堆叠技术，将提供比
    NN 和 DNN 更高的准确性。因此，可以使用这种堆叠概念获得优化的输出（表 [12](#Tab12)）。
- en: '**Neural Network**Table 13'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络**表 13'
- en: Highest rate for NN
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: NN 的最高准确率
- en: '| Optimizer | Label | Learning rate | Epochs | Accuracy | MSE | MAE |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | 标签 | 学习率 | 迭代轮数 | 准确率 | MSE | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| AdaGard | HL2 | 0.0017470 | 150 | 0.955 | 0.030 | 0.074 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| AdaGard | HL2 | 0.0017470 | 150 | 0.955 | 0.030 | 0.074 |'
- en: According to the determination rule, the number of hidden layer two is used
    for arbitrary decisions with rational activation functions Rahman [[13](#CR13)].
    Therefore two hidden layer neural network is the best approach for a given data
    dataset. The first layer is called the input layer according to the structure
    of the neuron. Previous layer outcome obtained to be the weighted input to the
    following layer, there is no correlation among each layer but NN shows craved
    conduct is made finding the correct weight by knowing NN Fister [[29](#CR29)].
    The structure of the neural network is similar to tree structure. There are units
    in each layer of the neural network. These units indicate how deep these layers
    can go. The value of unit basically indicates how depth the data will go and how
    many combinations will be tree based. A complete accurate outcome is obtained
    from multiple averages of a value. Stacking provides more accuracy than a neural
    network for a given data set, therefore neural networks provide more optimized
    results than stacking. Stacking is done in two layers whereas neural networks
    provide optimized output from many tree base combinations. Stacking is done in
    two layers so in case of complex data it will performed low whereas each hidden
    layer of neural network have units which indicates how deep it will go in tree
    base structures, so that it will provide an optimized output from multiple combinations
    (Table [13](#Tab13)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 根据决定规则，使用了两个隐藏层的数量来进行具有合理激活函数的任意决策 Rahman [[13](#CR13)]。因此，对于给定的数据集，两个隐藏层神经网络是最佳方法。根据神经元的结构，第一层称为输入层。前一层的输出被作为下一层的加权输入获得，每层之间没有相关性，但是
    NN 显示出了渴望的行为，通过了解 NN Fister [[29](#CR29)] 来找到正确的权重。神经网络的结构类似于树结构。神经网络的每一层都有单元。这些单元表示这些层可以有多深。单元的值基本上表示数据将走多深，以及将会有多少基于树的组合。从多个值的平均值中获得完全准确的结果。对于给定的数据集，堆叠比神经网络提供更高的准确性，因此神经网络比堆叠提供更优化的结果。堆叠是在两层中完成的，而神经网络从许多基于树的组合中提供优化的输出。堆叠是在两层中完成的，因此在复杂数据的情况下它将执行较低，而神经网络的每个隐藏层都有单元，这些单元表示它将在基于树的结构中走多深，因此它将从多个组合中提供优化的输出（表
    [13](#Tab13)）。
- en: '**Deep Neural Network**Table 14'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度神经网络**表 14'
- en: Highest rate for DNN
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 的最高准确率
- en: '| Optimizer | Label | Learning rate | Epochs | Accuracy | MSE | MAE |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | 标签 | 学习率 | 迭代轮数 | 准确率 | 均方误差 | 平均绝对误差 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Adam | HL5 | 0.01 | 50 | 0.955 | 0.030 | 0.074 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Adam | HL5 | 0.01 | 50 | 0.955 | 0.030 | 0.074 |'
- en: This study ultimately dictates three multi-layer NN, DNN, and stacking strategies.
    Evaluating their output reveals that their outcomes among these stacks are virtually
    the same. Here two layers are used for NN, 5 for DNN, and two steps have been
    used for stack generalization. Stacking technique and NN have decent results for
    a basic dataset, such that the output of the dataset is lowered whether it has
    complicated or complex values. According to DNN, it works with a large number
    of layers and uses the value of the unit as needed. The DNN model-based architecture
    from this study offers good results for every form of dataset much of the time
    (Table [14](#Tab14)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此研究最终确定了三种多层 NN、DNN 和堆叠策略。评估它们的输出显示，它们在这些堆叠中的结果几乎相同。这里使用了 2 层 NN，5 层 DNN，堆叠则使用了两个步骤。对于基本数据集，堆叠技术和
    NN 具有不错的结果，即使数据集的输出降低了，数据集具有复杂或复杂的值。根据 DNN，它可以处理大量的层，并根据需要使用单位值。从这项研究中可以看出，基于
    DNN 模型的架构通常对任何形式的数据集都提供了良好的结果（表[14](#Tab14)）。
- en: 5 Conclusion
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: Though phishing is a sensational phenomenon in today’s cyber space, it is a
    matter of concern to investigate for securing the future. In this study, anti-phishing
    techniques have been developed based on NN, DNN and stacking concept. Parameter
    adjustment plays a vital role for these techniques. Among those parameters, learning
    rate is one of them. This is an unimaginable footstep of increasing the performance
    of NN and DNN based on systems. Here is an assessment of the effect of parameters
    that will be an evidence in the development of NN and DNN based system. The amount
    of data in the data set affect the system learning base. In the case of stacking,
    Random Forest and Multilayer perception provides better results for precision
    and recall. However stack generalization helps better to enhance the overall accuracy.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管网络钓鱼是当今网络空间中的一个轰动现象，但调查和保障未来的安全问题仍然是值得关注的。在这项研究中，基于 NN、DNN 和堆叠概念已经开发出了防网络钓鱼技术。参数调整对这些技术起着至关重要的作用。在这些参数中，学习率是其中之一。这是一个无法想象的步骤，可以增加基于
    NN 和 DNN 的系统的性能。这里对参数的影响进行了评估，这将成为发展基于 NN 和 DNN 的系统的证据。数据集中的数据量影响系统的学习基础。在堆叠的情况下，随机森林和多层感知提供了更好的精度和召回率结果。然而，堆叠泛化有助于提高整体准确性。
- en: Basically, this chapter indicates three multilayer techniques that are NN, DNN,
    stacking along with the parameter tuning for neural network based architectures.
    Evaluating their performance shows that the results they provide almost same outcome.
    Apart from those, stacking provides better accuracy with less complex dataset.
    Here 2 layers are used for NN, 5 layers for DNN and stack generalization has used
    two layer. DNN and NN layers have units which indicate how deep these layers can
    go. Fundamental difference between NN and DNN is that NN works with two layers
    on the behalf of DNN works with more than two layers.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，本章指出了三种多层技术，即 NN、DNN 和堆叠，以及基于神经网络架构的参数调整。评估它们的性能表明，它们提供的结果几乎相同。除此之外，堆叠在处理较简单的数据集时提供了更好的准确性。这里使用了
    2 层 NN，5 层 DNN 和 两层堆叠。NN 和 DNN 层的单位指示这些层可以有多深。NN 和 DNN 之间的基本区别在于 NN 代表着两层，而 DNN
    则代表着两层以上。
- en: In recapitulate, the final outcome is obtained from the averages of multiple
    outputs. Stacking technique and NN provide better results for the dataset with
    simplicity, if the dataset holds complex or more complicated values then performance
    is getting decreased. According to DNN, it works with a large number of layers
    and uses the value of the unit as needed. From this study, DNN model based architecture
    provides better results on an average for any type of dataset.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，最终结果是从多个输出的平均值得出的。如果数据集具有简单性，则堆叠技术和 NN 可以提供更好的结果；如果数据集具有复杂或更复杂的值，则性能会下降。根据
    DNN，它可以处理大量的层，并根据需要使用单位值。从这项研究中可以看出，基于 DNN 模型的架构通常对任何类型的数据集都提供更好的结果。
