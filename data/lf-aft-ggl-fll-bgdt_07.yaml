- en: CHAPTER 7
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 7 章
- en: Dally’s Parallel Paradigm
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 达利的并行范式
- en: Is this Life after Google or what?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这是谷歌之后的生活吗？
- en: Bill Dally is about to take me to the Palo Alto Caltrain station in his self-driving
    Tesla Model S.[¹](notes.html#ch07note-1)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 比尔·达利即将驾驶他的自动驾驶特斯拉 Model S 带我去帕洛阿尔托 Caltrain 车站。[¹](notes.html#ch07note-1)
- en: In the Nvidia garage in Santa Clara, I board the sleek gray boron steel and
    titanium missile, noting its futuristic payload of a 1,200-pound lithium-ion battery.
    Should be enough to get me to the station. Fully charged, it can almost replace
    sixty pounds of gasoline in the tank of an internal combustion engine. That might
    not seem like much, but in Google-era mathematics it can save the world.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Nvidia 圣克拉拉的车库里，我登上了时尚的灰色硼钢和钛制导弹，注意到它未来主义的 1200 磅锂离子电池。应该足够把我带到车站。充电满了，它几乎可以取代内燃机油箱中
    60 磅的汽油。这可能看起来不像什么，但在谷歌时代的数学中，它可以拯救世界。
- en: In calculating the energy budgets of its data centers, Google, like the rest
    of Silicon Valley, is as rigorous as a Kenyan marathoner. But you had better recheck
    its numbers when it begins rolling out cars gleaming in the sun of solar subsidies.
    They may cost “waymo” than they say.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算其数据中心的能源预算时，谷歌像硅谷的其他公司一样严谨，就像肯尼亚马拉松选手一样。但当它开始推出在太阳能补贴的阳光下闪闪发光的汽车时，最好重新检查一下它的数字。它们的成本可能比它们说的要“Waymo”多。
- en: This is a Tesla, though, and its self-driving aspiration comes from Nvidia’s
    industry-leading Drive PX system. To buckle myself into the bucket seat, I have
    to push aside a flier from the annual Hot Chips conference in nearby Cupertino.
    While I was analyzing semiconductors for Ben Rosen and Esther Dyson some three
    decades ago, when chips were still hot, I used to go to Hot Chips conferences
    to stay up to date. Silicon, then as now, was the foundation, the physical layer,
    underlying the entire edifice of information technology. I am reassured that Hot
    Chips lives on even though Google and others assert that “software eats everything.”
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一辆特斯拉，它的自动驾驶愿景来自英伟达业界领先的 Drive PX 系统。我要把自己系在驾驶座上，但必须把旁边的一张来自附近库比蒂诺的年度 Hot
    Chips 大会的传单搬开。三十年前，当我为本·罗森和埃斯特·戴森分析半导体时，芯片仍然很热，我常去 Hot Chips 大会保持时效。硅，从那时起直到现在，一直是信息技术整个建筑的基础，物理层的基础。我很放心，尽管谷歌等公司声称“软件吞噬一切”，Hot
    Chips 依然存在着。
- en: 'Nick Tredennick, the designer of a favorite “hot chip” of yore, the Motorola
    68,000 microprocessor behind Steve Jobs’s Macintosh computer, used to say that
    the industry seeks to exploit the “leading edge wedge.” Three overlapping design
    targets converged in this fertile crescent of chip design: zero delay (fast hot
    chips), zero power (cool low-energy devices), and zero cost (transistors going
    for billionths of a penny).[²](notes.html#ch07note-2) Between the 1980s and 2017,
    chips have been migrating from the hot fast end toward the cool cheap end, a trend
    Dally has led.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 尼克·特雷登尼克是一款过去热门芯片的设计师，这款芯片是史蒂夫·乔布斯的麦金塔电脑背后的摩托罗拉68,000微处理器，曾经说过，这个行业试图利用“领先边缘楔形”。三个重叠的设计目标汇聚在芯片设计的这片肥沃的新月之中：零延迟（快速热芯片）、零功耗（低能耗设备）、零成本（晶体管价格为亿分之一便士）。在1980年代到2017年期间，芯片已经从热快端向着冷廉端迁移，这一趋势是Dally带领的。
- en: In the Tesla’s front seat, I face a two-foot-high screen displaying pale green
    and striated Google maps. Dally points out that self-driving vehicles “don’t care
    where the road lanes are. They navigate on maps, register their place on a map.
    If they have an empty road, they just take a line down the middle, like they are
    riding a rail. It is only the presence of moving objects, such as pedestrians
    and other cars, that requires them to use all their motion-sensing capabilities.”
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在特斯拉的前座，我面对着一个两英尺高的屏幕，上面显示着苍白而有条纹的谷歌地图。Dally指出，自动驾驶车辆“不关心路线在哪里。它们在地图上导航，登记在地图上的位置。如果有空旷的道路，它们就沿着中间画一条线，就像它们在走铁轨一样。只有移动物体，比如行人和其他汽车的存在，才需要它们使用所有的运动感知能力。”
- en: While the maps come from Google, the processing comes from Nvidia GPUs. These
    chips compute the car’s response to lidar, radar, ultrasound, and camera signals
    that free the missile to descend from the outer space of Elon Musk’s domains and
    enter the ever-changing high-entropy world beyond Google Maps.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然地图来自谷歌，但处理来自Nvidia GPU。这些芯片计算汽车对激光雷达、雷达、超声波和摄像头信号的响应，使导弹从埃隆·马斯克的领域外降落，进入谷歌地图之外不断变化的高熵世界。
- en: 'Dally barks his command: “Navigate to California Avenue Caltrain station,”
    and the car crisply responds. Dally comments, “In the last couple years speech
    recognition has become dramatically better. Thirty-percent better. Two years ago
    it was not really capable of getting it right. But now with machine learning on
    our Tegra chips, it gets it right every time.” Benefiting are all the users of
    Amazon’s Alexa, Apple’s Siri, Microsoft’s Cortana, Google’s Go.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Dally吩咐道：“导航至加州大道的卡尔特雷恩站”，车辆迅速响应。Dally评论说：“在过去几年里，语音识别已经大幅提升。提高了百分之三十。两年前它还不能准确理解。但现在有了我们的Tegra芯片上的机器学习，每次都能准确理解。”
    受益的是亚马逊的Alexa、苹果的Siri、微软的Cortana和谷歌的Go的所有用户。
- en: Dally has his hands on the steering wheel now as he negotiates the back streets.
    “It’s only level-two autonomy,” he explains, using the Society of Auto Engineers’
    classifications, which range from level one, a mere driver assistant, to level
    five, full self-driving. Musk promises to get Tesla to level five in two years.
    That’s Elon for you. But for now Dally keeps his eye on the road as the Tesla
    makes its way, with several high-voltage bursts, up the ramp onto 101\. Now Tesla’s
    self-driving mode enables him to turn and show me his film of the recent solar
    eclipse—a series of vivid high-contrast images of the rare event.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，达利正在操纵方向盘穿越后街。“这只是二级自动驾驶，”他解释道，使用了汽车工程师协会的分类，该分类范围从一级，仅仅是司机助手，到五级，全自动驾驶。马斯克承诺在两年内将特斯拉提升至五级。这就是埃隆。但现在，达利专心盯着道路，因为特斯拉通过几次高电压突发事件，沿着匝道开上101号高速公路。现在特斯拉的自动驾驶模式使他能够转身向我展示他最近日食的电影——一系列生动的高对比度图像。
- en: Machine learning, Dally points out, is mostly accomplished by graphics processing
    chips from Nvidia. Some advances in artificial intelligence spring from improvements
    in algorithms, but the real source of these capabilities is the explosive improvement
    in computer speed achieved through a combination of Moore’s Law and parallel processing.
    Nvidia’s graphics processors are the climax of Dally’s long career as a prophet
    of parallel processing, which began thirty years ago at Virginia Tech, where he
    studied the virtues of multiple processors functioning together.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 达利指出，机器学习主要是由英伟达的图形处理芯片完成的。一些人工智能的进步源于算法的改进，但这些能力的真正来源是通过摩尔定律和并行处理的爆炸性提升来实现的计算机速度的显著提升。英伟达的图形处理器是达利作为并行处理先知长达三十年的职业生涯的高潮，他的并行处理研究始于三十年前在弗吉尼亚理工学院，他在那里研究了多处理器共同运行的优点。
- en: At a Hot Chips conference at Stanford in August 1991, Dally and Norm Jouppi
    first emerged as foils in fashioning the future philosophies of computation. Dally
    introduced his revolutionary massively parallel J-machine, and Jouppi, now at
    Google, then at Digital Equipment, touted the promise of revving up existing processor
    pipelines to “five instructions per-clock” cycle.[³](notes.html#ch07note-3)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在1991年8月斯坦福大学的一个热门芯片会议上，达利和Norm Jouppi首次以形成未来计算哲学的对比出现。达利介绍了他的革命性的大规模并行J机器，而现在在谷歌工作的Jouppi则在Digital
    Equipment公司时称赞将现有处理器管线提速到每个时钟周期“五条指令”的前景。
- en: 'Those two papers of 1991 polarize all computer science: do you make existing
    serial von Neumann processors go faster, seeking zero delay, stepping and fetching
    instructions and data from ever faster remote memories? Or do you diffuse the
    memory and processing all through the machine? In a massively parallel spread
    like Dally’s J-machine, the memory is always close to the processor.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 1991年的这两篇论文使整个计算机科学产生了两极分化：你是让现有的串行冯·诺伊曼处理器变得更快，寻求零延迟，从越来越快的远程内存中提取指令和数据？还是将内存和处理器分散到整个机器中？在像达利的J机器这样的大规模并行扩展中，内存总是靠近处理器。
- en: Twenty-six years later, Dally and Jouppi are still at it. At the August 2017
    Hot Chips in Cupertino, all the big guys were touting their own chips for what
    they call “deep learning,” the fashionable Silicon Valley term for the massive
    acceleration of multi-layered pattern recognition, correlation, and correction
    tied to feedback that results in a cumulative gain in performance. What they call
    “learning” originated in earlier ventures in AI. Guess, measure the error, adjust
    the answer, feed it back are the canonical steps followed in Google’s data centers,
    enabling such applications as Google Translate, Google Soundwriter, Google Maps,
    Google Assistant, Waymo cars, search, Google Now, and so on, in real time.[⁴](notes.html#ch07note-4)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 二十六年后，达利和乔皮仍然在努力。在2017年8月的Cupertino热门芯片展上，所有大公司都在吹捧他们自己的芯片，用于所谓的“深度学习”，这是指与反馈相关的多层次模式识别、相关性和纠正的大规模加速，从而导致性能的累积增益。他们称之为“学习”的东西起源于早期的人工智能企业。猜测、测量错误、调整答案、反馈，这是谷歌数据中心遵循的经典步骤，使得Google翻译、Google
    Soundwriter、Google地图、Google助手、Waymo汽车、搜索、Google Now等应用能够实时运行[⁴](notes.html#ch07note-4)。
- en: As recently as 2012, Google was still struggling with the difference between
    dogs and cats. YouTube was famous for its cat videos, but it could not efficiently
    teach its machines to recognize the cats. They could count them; the data center
    dogs could dance; but it took sixteen thousand microprocessor cores and six hundred
    kilowatts.[⁵](notes.html#ch07note-5) And it still was a dog, with a 5 percent
    error rate—not an impressive portent for Google’s human face-recognition project
    or for car vision systems that need flawlessly to identify remote objects in real
    time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2012年，谷歌仍在努力区分狗和猫的区别。YouTube以其猫视频而闻名，但无法有效地教会其机器识别猫。它们能够数数；数据中心的狗可以跳舞；但这需要一万六千个微处理器核心和六百千瓦电力[⁵](notes.html#ch07note-5)，而且错误率仍然达到5％，这对于谷歌的人脸识别项目或需要实时无误识别远程对象的汽车视觉系统来说并不是一个令人印象深刻的预兆。
- en: As Claude Shannon showed, these success rates of 95 percent, or even 99.999
    percent, are deceptive, because you have no way of telling which instances are
    the errors.[⁶](notes.html#ch07note-6) The vast majority of the home loans in the
    mortgage crisis were sound, but because no one knew which ones were not, all the
    securities crashed. You don’t want that problem with self-driving cars.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如克劳德·香农所示，这些成功率达到95％，甚至99.999％的数据是具有误导性的，因为您无法判断哪些实例是错误的[⁶](notes.html#ch07note-6)。在抵押危机中，绝大多数住房贷款是安全的，但由于没有人知道哪些是不安全的，所有的证券都崩盘了。您不希望自动驾驶汽车出现这种问题。
- en: 'In a joint appearance in 2012 in Aspen, Peter Thiel chided Eric Schmidt: “You
    don’t have the slightest idea of what you are doing.” He pointed out that the
    company had amassed some $50 billion in cash at the time and was allowing it to
    sit in the bank at near-zero interest rates while its vast data centers still
    could not identify cats as well as a three-year-old could.[⁷](notes.html#ch07note-7)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 2012 年在阿斯彭联合露面时，彼得·蒂尔讽刺埃里克·施密特说：“你根本不知道你在做什么。”他指出，公司当时已经积累了大约 500 亿美元的现金，而公司仍然让它以接近零利率的利息放在银行里，而其庞大的数据中心仍然无法像一个三岁的孩子那样辨别出猫。[^7]
- en: Thiel is the leading critic of Silicon Valley’s prevailing philosophy of “inevitable”
    innovation. Page, on the other hand, is a machine-learning maximalist who believes
    that silicon will soon outperform human beings, however you want to define the
    difference. If the haphazard Turing machine of evolution could produce human brains,
    just imagine what could be accomplished by Google’s constellation of eminent academics
    devoting entire data centers full of multi-gigahertz silicon to training machines
    on petabytes of data. In 2012, though, the results seemed underwhelming.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Thiel 是硅谷“不可避免”创新主流哲学的主要批评者。另一方面，Page 是一位机器学习的极端主义者，他相信硅将很快超越人类，无论你如何定义这种差异。如果漫无目的的图灵机器能够产生人类大脑，想象一下
    Google 的杰出学者团队将整个数据中心充斥着多个吉赫兹硅来训练数据宝库中的机器所能实现的成就。然而，在 2012 年，结果似乎令人失望。
- en: Simultaneously with the dogs and cats crisis in 2012, the leader of the Google
    Brain research team, Jeff Dean, raised the stakes by telling Urs Hölzle, Google’s
    data center dynamo, “We need another Google.” Dean meant that Google would have
    to double the capacity of its data centers just to accommodate new demand for
    its Google Now speech recognition services on Android smartphones.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 2012 年与狗和猫危机同时发生时，谷歌 Brain 研究团队的领导 Jeff Dean 对谷歌的数据中心动力之源 Urs Hölzle 说：“我们需要另一个谷歌。”
    Dean 的意思是谷歌将不得不将其数据中心的容量翻倍，以满足 Android 智能手机上 Google Now 语音识别服务的新需求。
- en: Late in the year, Bill Dally provided an answer. Over breakfast at Dally’s favorite
    Palo Alto café, his Stanford colleague Andrew Ng, who worked with Dean at Google
    Brain, was complaining about the naming of cats. Sixteen thousand costly microprocessor
    cores seemed inefficient. Dally suggested that Nvidia GPUs could help. Graphics
    processors specialize in the matrix multiplication and floating-point mathematical
    operations that teach machines to recognize patterns. A graphical image is an
    array of values readily mapped to a mathematical matrix. Running images through
    as many as twelve layers of matrices, machine learning could be seen as another
    form of iterative graphics processing.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 年末，比尔·达利给出了答案。在达利最喜欢的帕洛阿尔托咖啡馆早餐时，他的斯坦福同事安德鲁·吴（Andrew Ng）抱怨起猫的命名。一万六千个昂贵的微处理器核心似乎效率低下。达利建议使用英伟达GPU。图形处理器专门用于矩阵乘法和浮点数数学运算，这些操作可以教会机器识别模式。图形图像是一个值数组，很容易映射到数学矩阵。通过将图像通过多达十二层矩阵，机器学习可以被视为另一种迭代图形处理。
- en: Prove it, Ng told Dally, and Google would buy his chips.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 吴告诉达利，证明一下，谷歌就会购买他的芯片。
- en: 'The man who built the first crude graphics processor, the precursor of all
    of Google’s data center neural networks, was Frank Rosenblatt, a psychology professor
    at Cornell. In 1958 he described his “perceptron” to the New Yorker: “If a triangle
    is held up to the perceptron’s eye [photosensor], the association units connected
    with the eye pick up the image of the triangle and convey it along a random succession
    of lines to the response units [now called neurons], where the image is registered. . . .
    [A]ll the connections leading to that response are strengthened [i.e., their weights
    are increased], and if a triangle of a different size and shape is held up to
    the perceptron, its image will be passed along the track that the first triangle
    took. If a square is presented, however, a new set of random lines is called into
    play. . . . The more images the perceptron is permitted to scan, the more adroit
    its generalizations. . . . It can tell the difference between a dog and a cat.”[⁸](notes.html#ch07note-8)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个粗糙图形处理器的创造者，也就是谷歌所有数据中心神经网络的前身，是康奈尔大学的心理学教授弗兰克·罗森布拉特。1958年，他向《纽约客》描述了他的“感知机”：“如果一个三角形被拿到感知机的眼睛[光电传感器]前，与眼睛相连的关联单元将捕捉到三角形的图像，并将其沿着一系列随机线路传递到响应单元[现在称为神经元]，在那里图像被注册。。。[所以]所有通向那个响应的连接都会被加强[即它们的权重会增加]，如果拿着不同大小和形状的三角形到感知机前，它的图像将沿着第一个三角形所经过的路径传递。然而，如果呈现的是一个正方形，则会调用一个新的随机线路集。。。感知机被允许扫描的图像越多，其泛化能力就越强。。。它能够区分狗和猫。”[⁸](notes.html#ch07note-8)
- en: Four years later, Ray Kurzweil, then sixteen, visited Rosenblatt after Kurzweil’s
    MIT mentor Marvin Minsky exposed the limitations of the one-layer perceptron that
    Rosenblatt had built. Rosenblatt told Kurzweil that he could surmount these limitations
    by stacking perceptrons on top of one another in layers. “The performance improves
    dramatically,” he said. Rosenblatt died in a boating accident eight years later,
    never having built a multilayered machine.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 四年后，时年十六岁的雷·库兹韦尔（Ray Kurzweil）拜访了罗森布拉特（Rosenblatt），当时库兹韦尔的 MIT 导师马文·明斯基（Marvin
    Minsky）揭示了罗森布拉特构建的单层感知器的局限性。罗森布拉特告诉库兹韦尔，他可以通过将感知器叠加在一起形成多层来克服这些局限性。“性能会有显著改善，”他说。八年后，罗森布拉特在一场船只事故中去世，从未构建过多层机器。
- en: Now at Google, that omission was being remedied. Dally assigned Nvidia’s software
    guru Frank Canizaro to work with Ng on upgrading Nvidia’s proprietary software
    CUDA (Compute Unified Device Architecture) for use in its CUDA Deep Neural Network
    library (cuDNN). The Stanford-Google-Nvidia team solved the cat-and-dog problem
    with merely twelve GPUs burning just four kilowatts, all for just thirty-three
    thousand dollars.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在 Google，这一遗漏正在得到纠正。Dally 指派 Nvidia 的软件专家 Frank Canizaro 与 Ng 合作，升级 Nvidia
    的专有软件 CUDA（Compute Unified Device Architecture）以供其 CUDA 深度神经网络库（cuDNN）使用。斯坦福-谷歌-Nvidia
    团队仅用了十二个 GPU，烧掉了仅四千瓦的电，仅用了三万三千美元就解决了猫狗问题。
- en: Dally was proud of this achievement. The Nvidia machine was roughly 150 times
    as cost-effective as Google’s previous setup. And that’s not even taking into
    account the GPUs’ enormous advantage in energy efficiency. Nvidia processors soon
    pervaded Google’s data centers, giving unprecedented performance in the matrix
    multiplications and accumulations at the heart of machine learning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Dally 为这一成就感到自豪。Nvidia 的机器大约比 Google 以前的设置**成本效益高出 150 倍**。而且这还没有考虑到 GPU 在能源效率上的巨大优势。Nvidia
    处理器很快就渗透到了 Google 的数据中心，在机器学习的核心矩阵乘法和累加运算中实现了前所未有的性能。
- en: Google now deploys ten-to-twelve-layer neural networks generating thirty exa-flops
    of floating point mathematical computing capacity—and matrix multiplications galore.
    In accord with Rosenblatt’s prediction that the “more images the perceptron is
    permitted to scan, the more adroit its generalizations,” the Google machine sorts
    tens of millions of images according to some one billion parameters. It prompts
    Google Brain routinely to claim to be “outperforming humans.” Gee, a billion parameters,
    beats me! In Silicon Valley, where human beings program these machines, it is
    considered cranky to question the claim of “superhuman” powers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，Google 部署了十至十二层的神经网络，产生了三十艾克斯弗洛普的浮点数数学计算能力——以及大量的矩阵乘法。与罗森布拉特的预测相符，“感知器能够扫描的图像越多，其泛化能力就越高”，Google
    的机器根据约十亿参数对数千万张图像进行排序。它促使 Google Brain 经常声称“超过人类”。天啊，十亿参数，我都不懂了！在硅谷，人类编程这些机器，质疑“超人类”能力的说法被视为古怪。
- en: None of this would faze Dally, except for one crucial change at Google. At the
    2017 Hot Chips conference, the company, in a do-it-yourself mood, indicated that
    it would henceforth replace Nvidia’s devices with its own special-purpose silicon.
    Jeff Dean celebrated Jouppi’s souped-up “Tensor” “matrix multiplier,” which eschewed
    graphics and floating point, focusing on the machine learning functions alone.
    It’s a matrix multiplier ASIC (application-specific integrated circuit). Without
    their Tensor processing unit, say the Google guys, they would have had to double
    the size of their data centers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都不会让Dally感到困扰，除了谷歌发生的一个至关重要的变化。在2017年Hot Chips会议上，公司以自助的心态表示，今后将用自己的特定用途硅替换英伟达的设备。Jeff
    Dean庆祝Jouppi的升级版“Tensor”“矩阵乘法器”，它避开了图形和浮点，专注于机器学习功能。这是一个矩阵乘法器ASIC（特定应用集成电路）。谷歌的人说，如果没有他们的Tensor处理单元，他们将不得不将数据中心的规模加倍。
- en: Dally points out that it is always possible to make huge temporary gains by
    putting entire systems onto single slivers of ASIC silicon, special-purpose chips
    hard-wired to perform one complex function. As Dally tells me, in performing parallel
    operations, graphics processors are ten times more cost-effective than general-purpose
    central processing units (CPUs), and ASICs are ten to a hundred times more cost-effective
    than ordinary GPUs. But with ASICs, your market is reduced to just your chosen
    special purposes, and your data centers are no longer all-purpose Turing machines.
    They are ossifying into special-purpose factories like the aluminum plants they
    succeeded in The Dalles.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Dally指出，将整个系统放入ASIC硅片的单个片上始终可能获得巨大的临时收益，这些是专用芯片，硬连线执行一个复杂功能。正如Dally告诉我，在执行并行操作时，图形处理器比通用中央处理单元（CPU）具有十倍的成本效益，而ASIC比普通GPU具有十到一百倍的成本效益。但是，使用ASIC，你的市场仅限于你选择的特定用途，你的数据中心不再是全能的图灵机器。它们正在变成像达尔斯市的铝厂一样的特定目的工厂。
- en: Google can afford to make its own custom ASICs for particular slots in its data
    centers, but Nvidia is dominating the entire domain of massively parallel processing.
    In the third quarter of 2017, after the Hot Chips “setbacks,” Nvidia announced
    a 109 percent increase in revenues from cloud computing sales, to $830 million,
    lifting the company’s market value to almost $130 billion.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌可以负担得起为其数据中心中特定插槽定制自己的定制ASIC，但英伟达正在主导大规模并行处理的整个领域。2017年第三季度，在Hot Chips“挫折”之后，英伟达宣布云计算销售收入增长109％，达到8.3亿美元，将公司的市值提高到近1300亿美元。
- en: Now Nvidia is a potent force providing parallel processors across global industry
    and providing new platforms for life after Google. Is all this to come to an end
    with Google’s new prowess in making hardware as well as software, hiring industry
    hardware titans such as Dave Patterson and Norm Jouppi to contrive world-leading
    chip architectures?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在英伟达是全球行业提供并行处理器的强大力量，并为谷歌之后的生活提供了新平台。随着谷歌在硬件和软件制造方面的新实力，聘用了行业硬件巨头，例如戴夫·帕特森和诺姆·乔皮来设计领先的芯片架构，所有这些是否即将终结？
- en: I was visiting Dally to find out. A fifty-seven-year-old, brown-haired engineer
    with a black hat and backpack and hiking boots, he is dressed Silicon-Valley-mountaineer
    style to take me on a high-altitude adventure in microchips and software, ideas
    and speculations, Google maps and Elon Musk “reality distortion fields” down Route
    101 at five o’clock on a late-August Friday evening.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我去拜访达利想了解一些情况。五十七岁的达利，棕发工程师，黑帽子、背包、登山靴，他的着装就像硅谷登山者，要带我进行高海拔冒险，涉及微芯片和软件、思想和猜测、Google地图和埃隆·马斯克的“现实扭曲场”……我们在八月下旬一个星期五下午五点，沿着101号公路驶向未来。
- en: It’s not quite Doctor Brown’s Back to the Future ride in a DeLorean, but it
    will suffice for some modest time-travel in the history of computing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这不完全是布朗博士驾驶德洛里安时光机的“回到未来”之旅，但它对计算史的一些简单时光旅行已足够。
- en: Since writing his college thesis in the late 1970s, Dally has rebelled against
    the serial step-by-step computing regime known as the von Neumann architecture.
    After working on the “Cosmic Cube” under Chuck Seitz for his Ph.D. at Caltech
    (1983), Dally has led design of parallel machines at MIT (the J-machine and the
    M-machine), introduced massive parallelism to Cray supercomputers (the T-3D and
    3E), and pioneered parallel graphics at Stanford (the Imagine project, a streaming
    parallel device incorporating programmable “shaders,” now ubiquitous in the industry’s
    graphic processors from Nvidia and others).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自上世纪70年代晚期写完他的大学论文以来，达利反抗了被称为冯·诺依曼体系结构的串行步进式计算机制度。在加州理工学院（1983年）攻读博士学位期间，他在Chuck
    Seitz 的“宇宙立方”项目上工作后，带领了麻省理工学院的并行机器设计（J-机器和M-机器），在Cray超级计算机上介绍了大规模并行性（T-3D和3E），并在斯坦福大学开创了并行图形领域，推出了Imagine项目（一种流式并行设备，融合了可编程的“着色器”，现已广泛应用于英伟达等公司的图形处理器中）。
- en: In all these projects Dally was warring against the conventional computer architecture
    of step-by-step serial processing—associated with a memory problem called the
    “von Neumann bottleneck.” You live in the real world, right? The real world offers
    intrinsically parallel problems such as images that flood the eye all at once,
    whether you’re driving a car in the snow or summoning a metaverse with computer-generated
    graphics or pattern-matching in “machine learning” argosies across the seas of
    big data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些项目中，达利一直在与传统的串行处理计算机体系结构作斗争——这与一个被称为“冯·诺依曼瓶颈”的内存问题有关。你生活在现实世界中，对吧？现实世界提供了本质上的并行问题，比如眼前一下子涌入的图像，不管你是在雪地上开车，还是在召唤由计算机生成的图形或者在“机器学习”海量数据中进行模式匹配。
- en: The von Neumann bottleneck was recognized by von Neumann himself. In response,
    he proposed a massively parallel architecture called cellular automata, which
    led to his last book before his death at age fifty-seven. In The Computer and
    the Brain, he contemplated a parallel solution called neural networks, which were
    based on a primitive idea of how billions of neurons might work together in the
    human neural system.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 冯·诺伊曼瓶颈是由冯·诺伊曼本人认识到的。 作为回应，他提出了一种被称为元胞自动机的大规模并行架构，这导致了他在五十七岁去世前的最后一本书。 在《计算机与大脑》中，他考虑了一种被称为神经网络的并行解决方案，这是基于对数十亿神经元如何在人类神经系统中共同工作的一个原始概念。
- en: Von Neumann concluded the brain is a non-von machine nine orders of magnitude
    slower than the gigahertz he prophesied for computers back in 1957\. Amazingly,
    von Neumann anticipated the many-million-fold “Moore’s Law” speedup that we have
    experienced. But he estimated that the brain is nine orders of magnitude (a billion
    times) more energy-efficient than a computer. This is a delta larger even than
    that claimed by the guys from Google Brain for their Tensor chip. In the age of
    Big Blue and Watson at IBM, the comparison remains relevant. When a supercomputer
    defeats a man in a game of chess or Go, the man is using maybe fourteen watts
    of power, while the computer and its networks are tapping into the gigawatt clouds
    on the Columbia.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 冯·诺伊曼得出结论，大脑是一个比他在1957年为计算机预言的几千兆赫慢了九个数量级的非冯机器。 令人惊讶的是，冯·诺伊曼预见了我们经历的许多百万倍的“摩尔定律”加速。
    但他估计，大脑比计算机节能高达九个数量级（十亿倍）。 这甚至比Google Brain团队为其Tensor芯片声称的节能更多。 在IBM的大蓝和沃森的时代，这种比较仍然相关。
    当一台超级计算机在国家超级计算机应用中心击败一名棋手时，这名棋手可能只用了十四瓦的功率，而计算机及其网络则依靠哥伦比亚州的千兆瓦云。
- en: In the age of Big Data, the von Neumann bottleneck has philosophical implications.
    The more knowledge that is put into a von Neumann machine, the bigger and more
    crowded its memory, the further away its average data address, and the slower
    its functioning. Danny Hillis, of the erstwhile Thinking Machines, writes, “This
    inefficiency remains no matter how fast we make the processor, because the length
    of the computation becomes dominated by the time required to move data between
    processor and memory.” That span, traveled in every step in the computation, is
    governed by the speed of light, which on a chip is around nine inches a nanosecond—a
    significant delay on chips that now bear as much as sixty miles of tiny wires.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据时代，冯·诺伊曼瓶颈具有哲学意义。 放入冯·诺伊曼机器中的知识越多，其内存就越大、越拥挤，其平均数据地址就越远，其功能就越慢。 前Thinking
    Machines的丹尼·希利斯写道：“无论我们将处理器速度提高到多快，这种效率都会保持不变，因为计算时间的长度受限于在处理器和内存之间移动数据所需的时间。”
    每一步计算中所走过的距离由光速决定，在芯片上大约是每纳秒九英寸——对于现在承载着长达六十英里微小导线的芯片来说，这是一个显著的延迟。
- en: What Dally saw is that the serial computer has reached the end of the line.
    Most computers (smartphones and tablets and laptops and even self-driving cars)
    are not plugged into the wall any more. Even supercomputers and data centers suffer
    from power constraints, manifested in the problems of cooling the machines, whether
    by giant fans and air conditioners or by sites near rivers or glaciers. As Hölzle
    comments, “By classic definitions, there is little ‘work’ produced by a datacenter
    since most of the energy is converted to heat.”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '-   Dally看到的是串行计算机已经走到了尽头。大多数计算机（智能手机、平板电脑、笔记本电脑，甚至自动驾驶汽车）不再插在墙上。即使是超级计算机和数据中心也受到功率限制的影响，体现在冷却机器的问题上，无论是通过巨大的风扇和空调还是靠近河流或冰川的站点。正如Hölzle所评论的，“按照传统的定义，数据中心产生的‘工作’很少，因为大部分能量都转化为热量。”'
- en: Hitting the energy wall and the light-speed barrier, the chip’s architecture
    will necessarily fragment into separate modules and asynchronous and more parallel
    structures. We might term these processors time-space “mollusks”—Einstein’s word
    for entities in a relativistic world. Setting the size of the integrated circuit
    cell will be a measure comparable in the microcosm to light-years in the cosmos.
    It will enforce a distribution of computing capabilities analogous to the distribution
    of human intelligence.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 撞击能源壁和光速障碍，芯片的架构必然会分裂成单独的模块和异步和更多的并行结构。我们可以将这些处理器称为时空“软体动物”——爱因斯坦对相对论世界中的实体的称呼。设置集成电路单元的大小将是微观世界中与宇宙中的光年可比的一种衡量。它将强制执行与人类智能分布类似的计算能力分布。
- en: As a result, says Dally, harking back to Tredennick, leading-edge-wedge computer
    performance must now be measured not by the conventional metrics of operations
    per second or silicon area but by operations per watt. Based on the natural parallelism
    of images hitting the eye at once, graphics processors are not only as ubiquitous
    as vision but supremely parallel. Thus many of the “cool chips” of today tend
    to be made by Nvidia.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Dally表示，回顾Tredennick的观点，领先的计算机性能现在必须通过每瓦的操作来衡量，而不是传统的每秒操作或硅面积的指标。基于眼睛一次性接收到的图像的自然并行性，图形处理器不仅像视觉一样普遍存在，而且具有极高的并行性。因此，今天许多“酷炫芯片”往往是由Nvidia制造的。
- en: Still, in operations per watt, the prevailing champion is made not of silicon
    but of carbon. It is the original neural network, the human brain and its fourteen
    watts, which is not enough to illuminate the lightbulb over a character’s head
    in a cartoon strip. In the future, computers will pursue the energy ergonomics
    of brains rather than the megawattage of Big Blue or even the giant air-conditioned
    expanses of data centers. All computers will have to use the power-saving techniques
    that have been developed in the battery-powered smartphone industry and then move
    on to explore the energy economics of real carbon brains.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，以每瓦操作而论，目前的冠军并非由硅而成，而是由碳组成。它就是原始的神经网络，人类大脑及其的十四瓦，这还不足以点亮卡通连环画中人物头顶上的灯泡。未来，计算机将追求大蓝（IBM）甚至是巨大的空调数据中心的兆瓦数而不是能耗优化。所有计算机都将必须采用在电池供电的智能手机行业中开发的省电技术，然后继续探索真实碳基大脑的能源经济。
- en: There is a critical difference between programmable machines and programmers.
    The machines are deterministic and the programmers are creative.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 可编程机器和程序员之间存在关键差异。机器是确定性的，而程序员则具有创造性。
- en: 'That means that the AI movement, far from replacing human brains, is going
    to find itself imitating them. The brain demonstrates the superiority of the edge
    over the core: It’s not agglomerated in a few air-conditioned nodes but dispersed
    far and wide, interconnected by myriad sensory and media channels. The test of
    the new global ganglia of computers and cables, worldwide Webs of glass and light
    and air, is how readily they take advantage of unexpected contributions from free
    human minds in all their creativity and diversity, which cannot even be measured
    by the metrics of computer science.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着人工智能运动与其取代人类大脑不如说是在模仿它们。大脑展示了边缘胜过核心的优越性：它不是集结在几个空调节点上，而是分散在各处，通过无数的感觉和媒体渠道相互连接。全球计算机和电缆的新型全球节块的考验是它们如何迅速利用来自各种创造性和多样性的自由人类思想的意外贡献，这甚至不能用计算机科学的度量标准来衡量。
- en: As the Silicon Valley legend Carver Mead of Caltech has shown in his decades
    of experiments in neuro-morphic computation, any real artificial intelligence
    will likely have to use not silicon substrates but carbon-based materials. With
    some 200,000 compounds, carbon is more adaptable and chemically complex than silicon
    by orders of magnitude. Recent years have seen an efflorescence of new carbon
    materials, such as the organic light-emitting diodes and photodetectors now slowly
    taking over the display market. Most promising is graphene, a one-atom-deep sheet
    of transparent carbon that can be curled up in carbon nanotubes, layered in graphite
    blocks, or architected in C-60 “Buckyballs.”
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 硅谷传奇人物加州理工学院的卡弗·米德在他数十年的神经形态计算实验中已经表明，任何真正的人工智能可能都不得不使用碳基材料而不是硅基底物。有大约 200,000
    种化合物的碳比硅具有更高的适应性和化学复杂性。近年来，出现了许多新的碳材料，如现在慢慢占据显示市场的有机发光二极管和光电探测器。最有前途的是石墨烯，一种透明碳的一层厚度的片材，可以卷曲成碳纳米管，在石墨块中堆叠，或者构建成
    C-60 “巴克球”。
- en: Graphene has many advantages. Its tensile strength is sixty times that of steel,
    its conductivity two hundred times that of copper. There is no band gap to slow
    it down, and it provides a relatively huge sixty-micron mean-free path for electrons.
    As the nanotech virtuoso James Tour of Rice University has demonstrated in his
    laboratory, graphene, carbon nanotube swirls, and their compounds make an array
    of nano-machines, vehicles, and engines possible. They offer the still-remote
    promise of new computer architectures such as quantum computers that can actually
    model physical reality and thus may finally yield some real intelligence.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 石墨烯有许多优点。它的拉伸强度是钢的 60 倍，导电性是铜的 200 倍。它没有带隙来减速，为电子提供了相对巨大的 60 微米平均自由程。正如莱斯大学的纳米技术大师詹姆斯·图尔在他的实验室中所证明的那样，石墨烯、碳纳米管漩涡及其化合物使各种纳米机器、车辆和引擎成为可能。它们提供了远未实现的新型计算机架构的希望，如能够实际模拟物理现实并因此最终可能产生一些真正智能的量子计算机。
- en: The current generation in Silicon Valley has yet to come to terms with the findings
    of von Neumann and Gödel early in the last century or with the breakthroughs in
    information theory of Claude Shannon, Gregory Chaitin, Anton Kolmogorov, and John
    R. Pierce. In a series of powerful arguments, Chaitin, the inventor of algorithmic
    information theory, has translated Gödel into modern terms. When Silicon Valley’s
    AI theorists push the logic of their case to explosive extremes, they defy the
    most crucial findings of twentieth-century mathematics and computer science. All
    logical schemes are incomplete and depend on propositions that they cannot prove.
    Pushing any logical or mathematical argument to extremes—whether “renormalized”
    infinities or parallel universe multiplicities—scientists impel it off the cliffs
    of Gödelian incompleteness.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当今硅谷的这一代人还没有接受冯·诺依曼和哥德尔在上个世纪早期的发现，也没有接受克劳德·香农、格雷戈里·查伊廷、安东·科尔莫戈洛夫和约翰·皮尔斯在信息理论方面的突破。在一系列强有力的论证中，发明了算法信息论的查伊廷将哥德尔的理论转化为现代术语。当硅谷的人工智能理论家把他们的案例逻辑推向爆炸性的极端时，他们违背了二十世纪数学和计算机科学最关键的发现。所有逻辑方案都是不完全的，并且依赖于它们无法证明的命题。将任何逻辑或数学论证推向极端——无论是“重标度”无限大还是平行宇宙的多重性——科学家们都会将它推向哥德尔的不完全性的悬崖。
- en: Chaitin’s “mathematics of creativity” suggests that in order to push the technology
    forward it will be necessary to transcend the deterministic mathematical logic
    that pervades existing computers. Anything deterministic prohibits the very surprises
    that define information and reflect real creation. Gödel dictates a mathematics
    of creativity.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Chaitin的“创造力数学”建议，为了推动技术的发展，有必要超越存在于现有计算机中的确定性数学逻辑。任何确定性的东西都会阻碍那些定义信息并反映真正创造力的惊喜。哥德尔规定了一种创造性的数学。
- en: This mathematics will first encounter a major obstacle in the stunning successes
    of the prevailing system of the world not only in Silicon Valley but also in finance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数学首先将在当前世界的主流系统的惊人成功中遇到一个重大障碍，这不仅存在于硅谷，也存在于金融领域。
