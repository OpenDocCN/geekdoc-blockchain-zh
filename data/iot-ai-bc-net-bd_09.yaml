- en: © Nishith Pathak and Anurag Bhandari 2018Nishith Pathak and Anurag BhandariIoT,
    AI, and Blockchain for .NET[https://doi.org/10.1007/978-1-4842-3709-0_9](A458845_1_En_9_Chapter.html)
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 2018尼希思·帕塔克和阿努拉格·班达里尼希思·帕塔克和阿努拉格·班达里IoT, AI, and Blockchain for .NET[https://doi.org/10.1007/978-1-4842-3709-0_9](A458845_1_En_9_Chapter.html)
- en: 9. Capturing, Analyzing, and Visualizing Real-Time Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9. 捕获、分析和可视化实时数据
- en: 'Nishith Pathak^(1 ) and Anurag Bhandari²(1)Kotdwara, Dist. Pauri Garhwal, India(2)Jalandhar,
    Punjab, IndiaHaving come this far, you are now equipped with the knowledge of
    how to create next generation AI 2.0 applications. In your arsenal, you have weapons,
    such as Cognitive Services, IoT Hub, and Blockchain, to help you create truly
    cutting-edge intelligent solutions for clients, customers, or your own organization.
    From a developer’s perspective, you are fully equipped. However, developing solutions
    is only part of a bigger picture, no matter how big it is.While solutions solve
    problems, they usually generate a ton of data along the way, some of which is
    consumed by the application itself to make instantaneous decisions. It is no longer
    uncommon for an application to generate massive mountains of data (think several
    gigabytes per hour), something that is not always possible to consume and analyze
    simultaneously by the application itself. This is especially true in scenarios
    involving IoT. Such data is then stored (in cloud) for later analysis using Big
    Data techniques. “Later” could be minutes, hours, or days.However, real-time analysis
    of such massive amounts of data is sometimes desirable. Healthcare is one domain
    where real-time analysis and notifications could save lives. For example, monitoring
    the pulse rate of all patients in a hospital, to keep tabs on their heart conditions,
    and raising alarms when abnormal rates are detected. Timely attention can make
    all the difference in saving a patient’s life.You saw a preliminary form of real-time
    analysis in Chapter [3](A458845_1_En_3_Chapter.html), where our solution backend
    was continuously reading all data written to our IoT Hub to detect high temperature
    conditions. To keep things simple, our solution catered to only one patient (we
    had exactly one simulated device to fake sending a virtual patient’s vitals to
    the hub). In the real world, a sufficiently large hospital may have hundreds or
    thousands of patients at any given point in time. Although the solution in Chapter
    [3](A458845_1_En_3_Chapter.html) is designed to be scale up to millions of devices—thanks
    to Azure’s IoT Hub service—our primitive solution backend application will crash
    when trying to analyze such volumes of data in real time.Azure Stream Analytics
    (ASA) provides a way to perform complex analyses on large amounts of data in real
    time. Power BI complements ASA by providing beautiful visual charts and statistics—based
    on results produced by ASA—that even completely non-tech-savvy staff can use to
    take decisions.By the end of this chapter, you will learn to:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尼希斯·帕塔克^(1)和阿努拉格·班达里²(1)印度波里加尔瓦尔县科特瓦拉(2)印度旁遮普邦贾兰达尔到了这一步，你现在掌握了如何创建下一代 AI 2.0
    应用的知识。在你的工具库中，你拥有像认知服务、物联网中心和区块链这样的武器，可以帮助你为客户、顾客或你自己的组织创建真正尖端的智能解决方案。从开发者的角度来看，你已经全副武装。然而，开发解决方案只是更大图景的一部分，无论它有多么庞大。虽然解决方案解决了问题，但它们通常会在此过程中生成大量数据，其中一些被应用程序本身用来做即时决策。现在，一个应用程序生成大量数据（比如每小时几个
    gigabytes）已经不再罕见，而应用程序本身并不总能够同时消耗和分析这些数据。这在涉及物联网的情况下尤其
- en: Create, configure, start, and stop an Azure Stream Analytics job
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建、配置、启动和停止 Azure 流分析作业
- en: Use a storage backend (Azure Blob Storage) to store ASA analysis results
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用存储后端（Azure Blob 存储）存储 ASA 分析结果
- en: Create graphical dashboards in Power BI to show ASA analysis results in real
    time
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Power BI 中创建图形仪表板，实时显示 ASA 分析结果
- en: Azure Stream Analytics
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure 流分析
- en: Data that systems deal with is in one of two forms—static or stream. Static
    data is unchanging or slow changing data usually stored in files and databases.
    For example, historic archives, old log files, inventory data, etc. For a thorough
    analysis of such data, no matter how large, one can use data analytics applications,
    libraries, and services. Azure HDInsight, Hadoop, R, and Python are commonly used
    tools for this purpose. You may use Stream Analytics for analysis of static data,
    but that is not what it is made for.Streaming data, on the other hand, is a continuous,
    real-time chain of data records that a system receives at a rate of hundreds or
    thousands of records per second. You may have heard the terms “stream” and “streaming”
    in the context of online videos, especially when they are being made available
    live (directly from recording camera to viewers, in real time). Apart from being
    consumed by end users, video streams can also be analyzed to produce key statistics,
    such as occurrences of a person or object through the video, dominant colors,
    faces, and emotions, etc. We saw hints about video analytics using AI (cognitive
    services) in Chapters [4](A458845_1_En_4_Chapter.html) and [5](A458845_1_En_5_Chapter.html).
    But analyzing video streams is not as common as analyzing other data streams.Some
    examples of data streams that are frequently analyzed are shown in Table [9-1](#Tab1).Table
    9-1Examples of Streaming Data and Analysis That Can Be Performed on Them
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 系统处理的数据分为两种形式——静态或流式。静态数据是不变或变化缓慢的数据，通常存储在文件和数据库中。例如，历史档案、旧日志文件、库存数据等。对于这样的数据进行彻底分析，不管数据有多大，可以使用数据分析应用程序、库和服务。Azure
    HDInsight、Hadoop、R 和 Python 是常用的工具。你可以使用流分析来分析静态数据，但那不是它的主要用途。另一方面，流式数据是系统以每秒数百或数千条记录的速率连续接收到的实时数据链。你可能在在线视频的上下文中听到“流”和“流式”这些术语，特别是当它们实时提供时（直接从录制摄像机到观众，在实时）。除了被最终用户使用外，视频流还可以进行分析，以产生关键统计数据，如视频中的人或物体出现次数、主导颜色、面孔和情感等。我们在第[4](A458845_1_En_4_Chapter.html)和[5](A458845_1_En_5_Chapter.html)章中看到了使用
    AI（认知服务）进行视频分析的暗示。但是分析视频流并不像分析其他数据流那样常见。一些经常分析的数据流示例列在[9-1](#Tab1)表中。表9-1数据流示例及可对其执行的分析
- en: '| Data Stream | Example Analysis Use Case |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 数据流 | 示例分析用例 |'
- en: '| :-- | :-- |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| :-- | :-- |'
- en: '| Pages visited and elements clicked (buttons, textboxes, links, etc.) on a
    website by all its visitors. Plus, data about visitors themselves—IP address (location),
    browser, OS, etc. Collectively known as clickstream. | Finding out most visited
    pages, most read sections, most popular visitor locations, etc. |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: 网站所有访问者访问的页面和点击的元素（按钮、文本框、链接等）。另外，有关访问者自身的数据—IP 地址（位置）、浏览器、操作系统等。总称为点击流。 | 查找最常访问的页面、最受欢迎的部分、最受欢迎的访问者位置等。
- en: '| Social media posts for a trending topic. | Finding out the overall and location-based
    sentiments about the topic. |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: 社交媒体上的热门话题的帖子。 | 查找有关该话题的整体和基于位置的情感。
- en: '| Stock market prices. | Calculating value-at-risk, automatic rebalancing of
    portfolios, suggesting stocks to invest in based on overall intra-hour performance.
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: 股票市场价格。 | 计算价值-at-risk，自动平衡投资组合，根据整体小时内表现建议投资股票。
- en: '| Frequent updates (telemetry data) from a large number of IoT devices. | Depends
    on the type of data collected by the devices. |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: 大量物联网设备的频繁更新（遥测数据）。 | 取决于设备收集的数据类型。
- en: Azure Stream Analytics is specially designed to deal with streaming data. As
    a result, it excels in situations requiring real-time analysis. Like other Azure
    services, ASA is highly scalable and capable of handling up to 1GB data per second.It
    provides an easy-to-use SQL-like declarative language, called Stream Analytics
    query language, to perform simple to complex analysis on streaming data. You write
    queries exactly the way you write in SQL—SELECT, GROUP, JOIN—everything works
    the same way. The only major difference here is that since you are dealing with
    never-ending streaming data, you need to define time-specific boundaries to pick
    the exact data set to analyze. In other words, since static data changes very
    slowly or doesn’t change at all, analysis is performed on the whole data. Streaming
    data is continuous with no predefined end. In most situations, the system performing
    analysis cannot predict when the data will end. So, it must pick small, time-bound
    pieces of data stream to analyze. The shorter the time window, the closer the
    analysis is to real time.NoteIt is important to note that performing extremely
    complex analyses of the scale of Big Data takes significant time. For this reason,
    Stream Analytics solutions (ASA, Apache Storm, etc.) are not recommended for very
    complex analyses. For situations where running an analysis may take minutes to
    hours, streaming data should first be stored in a temporary or permanent storage
    and then processed like static data.A typical workflow in ASA starts with defining
    one or more input sources of data. ASA can out-of-the-box work with IoT Hubs,
    event hubs, and blob storages. ASA can use streaming data from a hub together
    with reference static data to perform analysis. Additionally, functions defined
    in Azure machine learning may also be used in an ASA job to perform real-time
    predictions on the data being analyzed. You will learn more about machine learning
    in Chapter [10](A458845_1_En_10_Chapter.html). The output of an ASA job may be
    stored in a permanent storage (blob, SQL database, data lake, etc.) or sent directly
    to Power BI to generate dashboards, or both. An IoT Hub solution backend may also
    subscribe to an ASA job’s output to send commands to devices based on the output
    received. Figure [9-1](#Fig1) summarizes this workflow.![A458845_1_En_9_Fig1_HTML.png](A458845_1_En_9_Fig1_HTML.png)Figure
    9-1Azure Stream Analytics workflow
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Stream Analytics 是专为处理流数据而设计的。因此，在需要实时分析的情况下，它表现出色。与其他 Azure 服务一样，ASA
    具有高度可扩展性，并且能够处理高达每秒 1GB 的数据。它提供了一种易于使用的类 SQL 声明性语言，称为流分析查询语言，用于对流数据进行简单到复杂的分析。您编写的查询与
    SQL 中的编写方式完全相同——SELECT、GROUP、JOIN——一切都以相同的方式工作。这里唯一的主要区别在于，由于您正在处理永无止境的流数据，因此需要定义特定于时间的边界以选择要分析的确切数据集。换句话说，由于静态数据变化非常缓慢或根本不变，因此对整个数据进行分析。流数据是连续的，没有预定义的结束。在大多数情况下，执行分析的系统无法预测数据何时结束。因此，必须选择小的、时间范围有限的数据流片段进行分析。时间窗口越短，分析越接近实时。请注意，进行大规模大数据复杂分析需要大量时间。因此，不建议使用流分析解决方案（ASA、Apache
    Storm 等）进行非常复杂的分析。对于运行分析可能需要几分钟到几小时的情况，首先应将流数据存储在临时或永久存储中，然后像静态数据一样进行处理。ASA 中的典型工作流程始于定义一个或多个数据输入源。ASA
    可以开箱即用地处理 IoT Hub、事件中心和 Blob 存储。ASA 可以使用来自中心的流数据以及参考静态数据来执行分析。此外，还可以在 ASA 作业中使用
    Azure 机器学习中定义的函数对正在分析的数据进行实时预测。您将在第 [10](A458845_1_En_10_Chapter.html) 章中了解更多关于机器学习的内容。ASA
    作业的输出可以存储在永久存储（Blob、SQL 数据库、数据湖等）中，也可以直接发送到 Power BI 以生成仪表板，或者两者兼而有之。IoT Hub 解决方案后端也可以订阅
    ASA 作业的输出，以根据接收到的输出发送命令给设备。图 [9-1](#Fig1) 总结了此工作流程。![A458845_1_En_9_Fig1_HTML.png](A458845_1_En_9_Fig1_HTML.png)图
    9-1 Azure Stream Analytics 工作流程
- en: Performing IoT Stream Data Analysis
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行 IoT 流数据分析
- en: Let’s start with something basic. In Chapter [3](A458845_1_En_3_Chapter.html),
    we created an IoT Hub in Azure and a simulated device in C# that could send to
    the hub a patient’s vitals every three seconds. We’ll reuse these to perform real-time
    analysis for just one patient. Later, we’ll create more simulated devices to generate
    fake data for more patients and use that data for perform more complex real-time
    analytics.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些基础知识开始。在第[3](A458845_1_En_3_Chapter.html)章中，我们在 Azure 中创建了一个 IoT 中心，并在
    C# 中创建了一个模拟设备，该设备可以每三秒发送一次病人的生命体征到中心。我们将重复使用这些内容，仅为一个患者执行实时分析。稍后，我们将创建更多模拟设备，为更多患者生成假数据，并使用该数据执行更复杂的实时分析。
- en: Creating an Azure Stream Analytics Job
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 Azure 流分析作业
- en: Visit Azure Portal ( [https://portal.azure.com](https://portal.azure.com) )
    and click on the Create a Resource button at the top of the left side menu. Go
    to Data + Analytics ➤ Stream Analytics job. Or, search for “stream analytics”
    if you are unable to find this option otherwise. Fill in the form, as shown in
    Figure [9-2](#Fig2)  . As this job is going to be our way to test ASA, you will
    not need more than one streaming unit. Ideally, you should start small and scale
    up when your hub starts receiving large volumes of data.![A458845_1_En_9_Fig2_HTML.jpg](A458845_1_En_9_Fig2_HTML.jpg)Figure
    9-2Creating a new ASA jobClick on the Create button and wait a few seconds for
    the deployment to complete.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 Azure 门户（[https://portal.azure.com](https://portal.azure.com)），并单击左侧菜单顶部的“创建资源”按钮。转到“数据
    + 分析” ➤ “流分析作业”。或者，如果无法找到此选项，请搜索“流分析”。填写表单，如[图9-2](#Fig2)所示。由于此作业将是我们测试 ASA 的方式，因此您不需要超过一个流处理单元。理想情况下，您应该从小规模开始，并在您的中心开始接收大量数据时扩展。![A458845_1_En_9_Fig2_HTML.jpg](A458845_1_En_9_Fig2_HTML.jpg)图9-2创建新的
    ASA 作业单击“创建”按钮，等待几秒钟，直到部署完成。
- en: Adding an Input to an ASA Job
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加输入到 ASA 作业
- en: By default, an ASA job is created in stopped mode. We’ll first configure it
    by specifying input and output before starting the job. Once the job starts, you
    cannot edit input and output settings.From the newly created ASA job’s overview
    page in Azure Portal, go to Job Topology ➤ Inputs. You will see two options to
    add an input—Add Stream Input and Add Reference Input. The latter option lets
    you add reference data, which is static data that we can use to complement our
    streaming data. In a lot of situations, it may be metadata that can be joined
    with streaming data to aid in analysis. For example, a reference input may be
    a list of all patients along with their names, ages, and other attributes. These
    are things that are not present in streaming data records to reduce redundancy
    and save network bandwidth. While running a real-time Stream Analytics query,
    we may need a patient’s age to more accurately determine diseases.Choose Add Stream
    Input ➤ IoT Hub. Fill in the resulting form, as shown in Figure [9-3](#Fig3)  .
    The easiest way to select an existing IoT Hub is to choose one from the same Azure
    subscription. If your hub is in a different subscription than your ASA job, you
    will have an option to specify the hub’s connection string. The Input alias can
    be anything that describes your input uniquely in the context of the job.![A458845_1_En_9_Fig3_HTML.jpg](A458845_1_En_9_Fig3_HTML.jpg)Figure
    9-3Adding an existing IoT Hub as a stream input to an ASA jobClicking Save will
    add the input to your job and immediately perform a test to check the connection
    to the specified hub. You should receive a "Successful Connection Test" notification
    post.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，ASA 作业以停止模式创建。在启动作业之前，我们将首先通过指定输入和输出来配置它。一旦作业开始，您将无法编辑输入和输出设置。从 Azure
    门户中新创建的 ASA 作业概述页面转到作业拓扑 ➤ 输入。您将看到两个选项来添加输入——添加流输入和添加引用输入。后一种选项允许您添加参考数据，即我们可以用来补充流数据的静态数据。在许多情况下，它可能是元数据，可以与流数据连接以帮助分析。例如，参考输入可能是所有患者的列表，以及他们的姓名、年龄和其他属性。这些是不在流数据记录中的东西，以减少冗余并节省网络带宽。在运行实时流分析查询时，我们可能需要患者的年龄以更准确地确定疾病。选择添加流输入
    ➤ 物联网中心。填写生成的表单，如图[9-3](#Fig3)所示。选择现有的物联网中心的最简单方法是从同一 Azure 订阅中选择一个。如果您的中心位于与
    ASA 作业不同的订阅中，则可以选择指定中心的连接字符串的选项。输入别名可以是在作业上下文中唯一描述您的输入的任何内容。点击保存将添加输入到您的作业，并立即执行测试以检查与指定中心的连接。您应该会收到“连接测试成功”的通知。
- en: Testing Your Input
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试您的输入
- en: 'Before moving to the next step—specifying an output in our ASA job—it’s a good
    idea to run a sample SA query on the input data stream just to ensure that the
    job is configured correctly. We’ll run the query directly on real-time data received
    by our IoT Hub. For this, we’ll have to fire up the simulated device that we created
    in Chapter [3](A458845_1_En_3_Chapter.html) to start sending data to our hub.Open
    the IoTCentralizedPatientMonitoring solution in Visual Studio. With only the SimulatedDevice
    project set as the startup project, run the solution. When our simulated device
    is up and sending messages to the hub, return to Azure Portal. From the ASA job’s
    side menu, go to Job Topology ➤ Query.NoteYou do not need to start a Stream Analytics
    job in order to run test queries. This can be done by providing the job sample
    data for specified inputs, which can be done in two ways—uploading the data file
    (.json, .csv, etc.) or sampling data from a streaming input.The Query page has
    two panes: Inputs and Outputs on the left and Query Editor on the right. Right-click
    the IoT Hub input alias and select Sample Data from Input. Specify three minutes
    as the sampling duration. Figure [9-4](#Fig4) shows how these options look.![A458845_1_En_9_Fig4_HTML.jpg](A458845_1_En_9_Fig4_HTML.jpg)Figure
    9-4Sampling data from a streaming input, such as IoT HubAzure will start collecting
    data (for the next three minutes) being received by the hub in a temporary storage.
    After it is finished sampling, Azure will notify you about it. If, at this point,
    you close your browser’s tab or navigate to another page, the sampled data will
    be lost.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行下一步——指定 ASA 作业输出之前，最好在输入数据流上运行一些样本 SA 查询，以确保作业配置正确。我们将直接在我们的物联网中心收到的实时数据上运行查询。为此，我们将启动我们在第
    [3](A458845_1_En_3_Chapter.html) 章中创建的模拟设备，开始向我们的中心发送数据。在 Visual Studio 中打开 IoTCentralizedPatientMonitoring
    解决方案。只需将 SimulatedDevice 项目设置为启动项目，运行解决方案。当我们的模拟设备启动并向中心发送消息时，返回到 Azure 门户。从 ASA
    作业的侧边栏，转到作业拓扑 ➤ 查询。注意：为了运行测试查询，您不需要启动流分析作业。这可以通过为指定输入提供作业样本数据来完成，有两种方式——上传数据文件（.json、.csv
    等）或从流输入中取样数据。查询页面有两个窗格：左侧是输入和输出，右侧是查询编辑器。右键单击物联网中心输入别名，然后选择来自输入的样本数据。将采样时长指定为三分钟。图
    [9-4](#Fig4) 显示了这些选项的外观。![A458845_1_En_9_Fig4_HTML.jpg](A458845_1_En_9_Fig4_HTML.jpg)图
    9-4 从流输入（如 IoT Hub）采样数据 Azure 将开始收集由中心接收到的数据（接下来的三分钟）存储在临时存储中。采样完成后，Azure 将通知您。如果在此时关闭浏览器标签页或导航到另一页，则采样数据将丢失。
- en: Testing with a Pass-Through Query
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用“透传查询”进行测试
- en: Once the data is sampled and ready to be queried, you may stop the simulated
    device to prevent your IoT Hub from being metered further. Now, modify the query
    in the editor to update YourInputAlias to the name of your hub input. You may
    leave YourOutputAlias as is.SELECT    *INTO    [YourOutputAlias]FROM    [your-iothub-input-alias]As
    you may recall from your SQL knowledge, SELECT * FROM <table> picks up and displays
    all rows and columns from the specified table. In the context of Stream Analytics,
    this is what is called a “pass-through” query. It selects all data from the given
    input and sends it to the given output. Ideally, you’d specify constraints and
    filters (e.g., the WHERE clause) to perform analysis and send only those records
    to an output that match your criteria.Click the Test button. In the absence of
    a permanent storage output, the pass-through query generates results in a browser,
    as shown in Figure [9-5](#Fig5). ASA is smart enough to automatically generate
    column names from attributes of data records.![A458845_1_En_9_Fig5_HTML.jpg](A458845_1_En_9_Fig5_HTML.jpg)Figure
    9-5Results from the pass-through queryTo recall, a message from a device looks
    like this:{"messageId":"1e7c7cf7-5d6b-4938-aac0-688cb870b70e","deviceId":"medicaldevice-patient1","patientBodyTemperature":40.471381505937956,"patientPulseRate":76.017155878253817,"patientRespirationRate":15.639444097708651,"rooomTemperature":29.475085872912352,"roomHumidity":54.724774525838335}You
    can match column names in the SA query result with the attribute names in the
    sample message. Apart from these, we see IoT Hub specific columns such as EventProcessedUtcTime,
    PartitionId, etc.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被采样并准备好查询，你可以停止模拟设备以防止你的IoT Hub被进一步计量。现在，在编辑器中修改查询，将YourInputAlias更新为你的Hub输入的名称。你可以保留YourOutputAlias不变。SELECT    *INTO    [YourOutputAlias]FROM    [your-iothub-input-alias]你可能还记得你的SQL知识，SELECT
    * FROM <table> 会从指定的表中选择并显示所有行和列。在Stream Analytics的上下文中，这就是所谓的“穿越”查询。它从给定的输入选择所有数据，并将其发送到给定的输出。理想情况下，你应该指定约束和过滤器（例如，WHERE子句）来执行分析，并仅将符合你的标准的记录发送到输出。点击测试按钮。在缺乏永久存储输出的情况下，穿越查询会在浏览器中生成结果，如图
    [9-5](#Fig5) 所示。ASA足够智能，能够从数据记录的属性自动生成列名。![A458845_1_En_9_Fig5_HTML.jpg](A458845_1_En_9_Fig5_HTML.jpg)图9-5穿越查询的结果回顾一下，设备的消息看起来像这样：{"messageId":"1e7c7cf7-5d6b-4938-aac0-688cb870b70e","deviceId":"medicaldevice-patient1","patientBodyTemperature":40.471381505937956,"patientPulseRate":76.017155878253817,"patientRespirationRate":15.639444097708651,"rooomTemperature":29.475085872912352,"roomHumidity":54.724774525838335}你可以将SA查询结果中的列名与示例消息中的属性名进行匹配。除此之外，我们还看到了IoT
    Hub特定的列，如EventProcessedUtcTime、PartitionId等。
- en: Testing with a Real-World Query
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用实际查询进行测试
- en: 'Let’s try a more complex query to cater to a real-world scenario. In Chapter
    [3](A458845_1_En_3_Chapter.html), we wrote our solution backend to detect high
    body temperature conditions. Along similar lines, we’ll write a query to detect
    high pulse rate conditions. Our solution backend was very basic and naïve in checking
    for high temperatures in each message received from a medical device. We will
    not do the same to detect high pulse rates. Instead, we will check for high values
    among pulse rates averaged from within time intervals of 30 seconds each. In other
    words, we want to tag pulse rates as high if they are consistently above a certain
    threshold (say, 90) during a period of 30 seconds.The query for this sort of condition
    will look like this:SELECT    System.Timestamp AS Time,    Avg(PatientPulseRate)
    AS AvgPulseRateINTO    [YourOutputAlias]FROM    [your-iothub-input-alias]    TIMESTAMP
    BY EventEnqueuedUtcTime    GROUP BY TumblingWindow(second, 30), DeviceId    HAVING
    Avg(PatientPulseRate) > 90If you run this query, you may get none to three results,
    depending on the sample data. In this case, we received a single result, as shown
    in Figure [9-6](#Fig6).![A458845_1_En_9_Fig6_HTML.jpg](A458845_1_En_9_Fig6_HTML.jpg)Figure
    9-6Result from running a constraint-bound SA queryIn the real-world, an ASA job
    will deal with data from hundreds or thousands of devices, lending this query
    its real worth. Let’s break the query down for better understanding.The most unusual
    thing in the entire query is the keyword TumblingWindow . Stream Analytics deals
    with streaming data. Unlike static data, streaming data has no start or end; it’s
    just a continuous stream of incoming data records that may or may not end in the
    near future. A question then arises: starting when and how long should streaming
    data be analyzed in one go? A bit earlier in the chapter, we talked about analyzing
    time-bound windows of streaming data. TumblingWindow provides us with those time-bound
    windows while performing aggregate operations using GROUP BY. An equivalent keyword
    we use with a JOIN is DATEDIFF.TumblingWindow takes in two parameters—unit of
    time and its measure—in order to group data during that time interval. In our
    query, we group incoming data records every 30 seconds and check for the average
    pulse rate of the patient during that time interval.TipIf you are still confused,
    do not forget that we ran this query on sampled data, which was already a snapshot—a
    three-minute window—of an ongoing stream. So, ASA created six 30-second groups
    and displayed the final result after analyzing them. When we start our ASA job,
    it will start working with an actual stream. Analysis will be performed and a
    result spat out (to specified outputs) every 30 seconds.TIMESTAMP BY provides
    a way to specify which time to consider while creating time-bound windows. By
    default, Stream Analytics will consider the time of arrival of a data record for
    the purpose. This may be fine in some scenarios, but it’s not what we want. In
    healthcare applications, time when data was recorded by the IoT device is more
    important than the time when that data was received by Azure (IoT Hub or Stream
    Analytics), since there may be a delay of a few milliseconds to a couple of seconds
    between the two events. That’s why, in our query, we chose to timestamp by EventEnqueuedUtcTime,
    which is closer to when the data was recorded.Selecting System.Timestamp provides
    us with the end time for each window.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个更复杂的查询，以适应真实场景。在[第 3 章](A458845_1_En_3_Chapter.html)中，我们编写了解决方案后端来检测高体温情况。沿着类似的思路，我们将编写一个查询来检测高脉搏率条件。我们的解决方案后端在检查每条从医疗设备接收到的高温度时非常基础和天真。我们不会用同样的方式来检测高脉搏率。相反，我们将检查从每个时间间隔内平均脉搏率中得出的高值。换句话说，我们想要在
    30 秒内连续高于某个阈值（比如 90）的情况下标记脉搏率为高。这种条件的查询将如下所示：SELECT    System.Timestamp AS Time,    Avg(PatientPulseRate)
    AS AvgPulseRateINTO    [YourOutputAlias]FROM    [your-iothub-input-alias]    TIMESTAMP
    BY EventEnqueuedUtcTime    GROUP BY TumblingWindow(second, 30), DeviceId    HAVING
    Avg(PatientPulseRate) > 90如果你运行这个查询，可能会得到零到三个结果，这取决于样本数据。在这种情况下，我们收到了一个结果，如图 [9-6](#Fig6)
    所示。![A458845_1_En_9_Fig6_HTML.jpg](A458845_1_En_9_Fig6_HTML.jpg)图 9-6 运行约束约束 SA
    查询的结果在现实世界中，ASA 作业将处理来自数百或数千台设备的数据，这使得这个查询具有真正的价值。让我们拆解一下这个查询以便更好地理解。整个查询中最不寻常的事情是关键字
    TumblingWindow。Stream Analytics 处理流数据。与静态数据不同，流数据没有开始或结束；它只是一连串连续的传入数据记录，可能近期内会结束，也可能不会。一个问题就是：从何时开始以及多长时间应该连续分析流数据？在本章的前一部分，我们谈到了分析流数据的时间限定窗口。TumblingWindow
    为我们提供了这些时间限定窗口，同时使用 GROUP BY 执行聚合操作。我们在 JOIN 中使用的等效关键字是 DATEDIFF。TumblingWindow
    接受两个参数——时间单位和其度量——以便在该时间间隔内对数据进行分组。在我们的查询中，我们每 30 秒对传入的数据记录进行分组，并检查该时间间隔内患者的平均脉搏率。提示如果你仍然感到困惑，请不要忘记，我们在样本数据上运行了这个查询，这已经是一个快照——一个三分钟的窗口——了。因此，ASA
    创建了六个 30 秒的组，并在分析它们后显示最终结果。当我们启动 ASA 作业时，它将开始处理实际的流。每 30 秒进行分析，并将结果吐出（到指定的输出）。TIMESTAMP
    BY 提供了一种指定在创建时间限定窗口时要考虑哪个时间的方法。默认情况下，Stream Analytics 将考虑数据记录到达的时间。在某些情况下，这可能是可以接受的，但这不是我们想要的。在医疗保健应用程序中，数据由物联网设备记录的时间比数据由
    Azure（IoT Hub 或 Stream Analytics）接收的时间更重要，因为两个事件之间可能会有几毫秒到几秒的延迟。这就是为什么在我们的查询中，我们选择按
    EventEnqueuedUtcTime 时间戳，这比数据接收的时间更接近。选择 System.Timestamp 为每个窗口提供了结束时间。
- en: Adding an Output to an ASA Job
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 给 ASA 任务添加一个输出
- en: While running test queries on sampled input, ASA generated its output directly
    in the browser. However, this is not how it works in a real-world scenario. ASA
    requires you to specify at least one output sink (container) where the results
    will be stored or displayed on a more permanent basis. Stored results may be analyzed
    further to arrive at decisions.Although ASA supports a long list of output options,
    we’ll go with a popular choice called blob storage . Azure’s blob service provides
    a highly-scalable storage for unstructured data. It also provides a RESTful API
    to access data stored in it. In most cases, you’d want to stored ASA results as
    JSON in a blob and retrieve it later in a web or mobile app via its REST API.Before
    we can specify a blob storage as an output in our ASA job, we need to create  it
    in Azure. The process is the same as for other Azure services. In the portal,
    visit Create a Resource ➤ Storage ➤ Storage Account – Blob, File, Table, Queue.
    Create a new blob, as shown in Figure [9-7](#Fig7).![A458845_1_En_9_Fig7_HTML.jpg](A458845_1_En_9_Fig7_HTML.jpg)Figure
    9-7Creating a new blog storage  to store ASA job outputThe name cannot contain
    special characters (even hyphens and underscores). That is why we have deviated
    here from our usual naming scheme. The access tier you choose will impact your
    pricing. Blob storage does not have a free tier, but the cost of storing data  is
    negligible (almost zero) for small amounts of data. You choose Hot if the blob
    data is going to be accessed frequently; choose Cool if data is going to be stored
    more frequently than accessed. We will use the latter in this case.When Azure
    has finished creating our new storage, we should return to our ASA job and go
    to Job Topology ➤ Outputs ➤ Add ➤ Blob storage. Fill in the form, as shown in
    Figure [9-8](#Fig8).![A458845_1_En_9_Fig8_HTML.jpg](A458845_1_En_9_Fig8_HTML.jpg)Figure
    9-8Adding a blob storage output  in an ASA jobAs in the case of specifying a hub
    as an input, here you get to choose an Azure Blob storage from within the same
    subscription or outside of it. For an external blob, you need to manually enter
    Storage Account and Storage Account Key. Ensure that you have JSON  select as
    serialization format. For JSON, the Format field will have two options—Line Separated
    and Array. If you intend to access blob data via a graphical tool (e.g., Azure
    Storage Explorer), Line Separated works well. If, however, the intention is to
    access blob data via its REST API, the Array option is better since that makes
    it easier to access JSON objects in JavaScript. Leave the rest of the fields as
    is and click on Create. Azure will perform a connection test immediately after
    the new output is added.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抽样输入运行测试查询时，ASA直接在浏览器中生成其输出。但是，在实际场景中，情况并非如此。ASA要求您至少指定一个输出汇（容器），结果将存储或以更持久的方式显示在其中。存储的结果可以进一步分析以做出决策。虽然ASA支持长列表的输出选项，但我们将选择一个称为blob存储的热门选择。Azure的blob服务为非结构化数据提供了高度可扩展的存储。它还提供了用于访问其中存储的数据的RESTful
    API。在大多数情况下，您可能希望将ASA结果以JSON格式存储在blob中，并稍后通过其REST API在Web或移动应用程序中检索它。在我们的ASA作业中指定blob存储作为输出之前，我们需要在Azure中创建它。该过程与其他Azure服务相同。在门户中，访问创建资源
    ➤ 存储 ➤ 存储帐户 - Blob、文件、表、队列。创建一个新的blob，如图[9-7](#Fig7)所示。![A458845_1_En_9_Fig7_HTML.jpg](A458845_1_En_9_Fig7_HTML.jpg)图9-7创建新的博客存储以存储ASA作业输出名称不能包含特殊字符（甚至连连字符和下划线也不行）。这就是我们在这里偏离常规命名方案的原因。您选择的访问层级将影响您的定价。Blob存储没有免费层，但对于少量数据来说，存储数据的成本几乎可以忽略不计（几乎为零）。如果blob数据将经常被访问，则选择热层；如果数据将被更频繁地存储而不是访问，则选择冷层。在这种情况下，我们将使用后者。当Azure完成创建我们的新存储后，我们应返回到我们的ASA作业，并转到作业拓扑
    ➤ 输出 ➤ 添加 ➤ Blob存储。填写表单，如图[9-8](#Fig8)所示。![A458845_1_En_9_Fig8_HTML.jpg](A458845_1_En_9_Fig8_HTML.jpg)图9-8在ASA作业中添加blob存储输出与指定hub作为输入的情况类似，您可以在同一订阅中或订阅外选择Azure
    Blob存储。对于外部blob，您需要手动输入存储帐户和存储帐户密钥。确保已选择JSON作为序列化格式。对于JSON，Format字段将有两个选项 - 行分隔和数组。如果您打算通过图形工具（例如Azure存储资源管理器）访问blob数据，则行分隔效果很好。然而，如果意图是通过其REST
    API访问blob数据，则数组选项更好，因为这样更容易在JavaScript中访问JSON对象。将其余字段保持不变，然后单击创建。在添加新输出后，Azure将立即执行连接测试。
- en: Testing Your Output
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试您的输出
- en: Return to your ASA job’s query editor and paste the same query that we used
    earlier, except this time specify your blob’s output alias after INTO.SELECT    System.Timestamp
    AS Time,    Avg(PatientPulseRate) AS AvgPulseRateINTO    [blob-patientmonitoring]FROM    [anuragbhd-iothub-patientmonitoring]    TIMESTAMP
    BY EventEnqueuedUtcTime    GROUP BY TumblingWindow(second, 30), DeviceId    HAVING
    Avg(PatientPulseRate) > 90Click Save to save the query. Go to the Overview page
    and start the job. It may take a couple of minutes for the job to start. You’ll
    be notified when it’s started. When a job is running, you may not edit query or
    add/edit inputs and outputs.From Visual Studio, start the simulated device again
    if you’d earlier stopped it. You may want to increase its update frequency to
    10 or 20 seconds to reflect a more realistic value.// Wait for 10 seconds before
    repeatingawait Task.Delay(10000);Let it run for a few minutes to allow for our
    specified high pulse rate condition to be met. In the meantime, download and install
    Azure Storage Explorer  from [http://storageexplorer.com](http://storageexplorer.com)
    . This is a nifty little desktop application for accessing Azure storage accounts,
    such as blob, data lake, Cosmos DB, etc.When you open Azure Storage Explorer  for
    the first time, you will be greeted with the Connect to Azure Storage wizard.
    If you are not, manually open the wizard by right-clicking on Storage Accounts
    in the Explorer pane on the left and selecting Connect to Azure Storage. In the
    wizard, select the Use a Connection String or a Shared Access Signature URI option
    and click Next. Copy your Blob’s connection string from Azure Portal ➤ <your-blob>
    ➤ Settings ➤ Access Keys ➤ key1 ➤ Connection String and paste it into the wizard’s
    Connection String  textbox, as shown in Figure [9-9](#Fig9).![A458845_1_En_9_Fig9_HTML.jpg](A458845_1_En_9_Fig9_HTML.jpg)Figure
    9-9Copying the blob storage’s connection string  from Azure Portal to Azure Storage
    ExplorerOnce the application has successfully connected to your blob storage,
    you will see its name under Storage Accounts in the Explorer pane. Expand it and
    go to Blob Containers ➤ <your blob container name>. In the Details pane on the
    right, you should see a listing corresponding to a .json file  . If you do not
    see anything there, the high pulse rate condition has never been met. You could
    wait for a few minutes more or relax the condition a bit by setting Avg(PatientPulseRate)
    > 80 (you will need to stop your ASA job, edit the query, and start the job again).
    Double-click the .json file  to open it locally in your default text editor. You
    should see something similar to the following.{"time":"2018-03-03T05:55:30.0000000Z","avgpulserate":91.907351640320385}{"time":"2018-03-03T05:56:00.0000000Z","avgpulserate":94.212707023856254}{"time":"2018-03-03T06:05:00.0000000Z","avgpulserate":94.652964749677551}{"time":"2018-03-03T06:06:00.0000000Z","avgpulserate":93.091244986168064}{"time":"2018-03-03T06:14:00.0000000Z","avgpulserate":91.6573497614159}{"time":"2018-03-03T06:14:30.0000000Z","avgpulserate":90.980160983425321}{"time":"2018-03-03T06:15:00.0000000Z","avgpulserate":92.107636040126735}NoteEach
    time an ASA job is stopped and started again, the subsequent outputs from the
    job are written to a new .json file (blob) in the same blob container. You will
    see as many blobs in Azure Storage Explorer as the times the ASA job was started.You
    can access these results programmatically in the web and mobile apps using blob
    storage’s comprehensive REST APIs  . Check [https://docs.microsoft.com/en-us/rest/api/storageservices/list-blobs](https://docs.microsoft.com/en-us/rest/api/storageservices/list-blobs)
    and [https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob](https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob)
    for more details.Let your simulated device run for a while longer and keep refreshing
    the Details pane in Azure Storage Explorer to see how Stream Analytics processes
    streaming input and generates output in real time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 回到你的 ASA 工作的查询编辑器，并粘贴我们之前使用的相同查询，但这次在 `INTO` 之后指定你的 blob 的输出别名。点击保存以保存查询。转到概览页面并启动作业。作业启动可能需要几分钟的时间。启动时会通知您。当作业运行时，您可能无法编辑查询或添加/编辑输入和输出。从
    Visual Studio 中重新启动模拟设备（如果您之前停止了它）。您可能希望将其更新频率增加到 10 或 20 秒，以反映更真实的值。在重复之前等待 10
    秒。让它运行一段时间以满足我们指定的高脉搏率条件。与此同时，从 [http://storageexplorer.com](http://storageexplorer.com)
    下载并安装 Azure Storage Explorer。这是一个很棒的小型桌面应用程序，用于访问 Azure 存储帐户，如 blob、数据湖、Cosmos
    DB 等。当您首次打开 Azure Storage Explorer 时，您将看到“连接到 Azure 存储”的向导。如果没有，请通过在左侧的资源管理器窗格中右键单击“存储帐户”，然后选择“连接到
    Azure 存储”来手动打开向导。在向导中，选择“使用连接字符串或共享访问签名 URI”选项，然后单击“下一步”。从 Azure 门户中复制您的 Blob
    的连接字符串 ➤ <your-blob> ➤ 设置 ➤ 访问密钥 ➤ key1 ➤ 连接字符串，并将其粘贴到向导的“连接字符串”文本框中，如图 [9-9](#Fig9)
    所示。![A458845_1_En_9_Fig9_HTML.jpg](A458845_1_En_9_Fig9_HTML.jpg)图 9-9从 Azure 门户复制
    blob 存储的连接字符串到 Azure 存储资源管理器一旦应用程序成功连接到您的 blob 存储，您将在资源管理器窗格下看到其名称。展开它并转到 Blob
    容器 ➤ <your blob container name>。在右侧的详细信息窗格中，您应该看到与 .json 文件 相对应的列表。如果您在那里没有看到任何内容，则高脉搏率条件从未满足。您可以再等待几分钟，或者通过设置
    Avg(PatientPulseRate) > 80 来放宽条件（您需要停止 ASA 作业，编辑查询，然后再次启动作业）。双击 .json 文件 在默认文本编辑器中将其在本地打开。您应该会看到类似以下的内容。{"time":"2018-03-03T05:55:30.0000000Z","avgpulserate":91.907351640320385}{"time":"2018-03-03T05:56:00.0000000Z","avgpulserate":94.212707023856254}{"time":"2018-03-03T06:05:00.0000000Z","avgpulserate":94.652964749677551}{"time":"2018-03-03T06:06:00.0000000Z","avgpulserate":93.091244986168064}{"time":"2018-03-03T06:14:00.0000000Z","avgpulserate":91.6573497614159}{"time":"2018-03-03T06:14:30.0000000Z","avgpulserate":90.980160983425321}{"time":"2018-03-03T06:15:00.0000000Z","avgpulserate":92.107636040126735}注：每次停止
    ASA 作业并重新启动后，作业的后续输出都将写入一个新的 .json 文件（blob）中，该文件与相同的 blob 容器中。您将在 Azure 存储资源管理器中看到与
    ASA 作业启动次数相同的 blob。您可以使用 blob 存储的全面 REST API 在 web 和移动应用程序中以编程方式访问这些结果。查看 [https://docs.microsoft.com/en-us/rest/api/storageservices/list-blobs](https://docs.microsoft.com/en-us/rest/api/storageservices/list-blobs)
    和 [https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob](https://docs.microsoft.com/en-us/rest/api/storageservices/get-blob)
    获取更多详情。让您的模拟设备运行更长时间，并在 Azure 存储资源管理器中保持刷新详细信息窗格，以查看流分析如何实时处理流输入并生成输出。
- en: Visualizing ASA Results Using Power BI
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Power BI 可视化 ASA 结果
- en: While blob storage provides an easy, scalable method to store Stream Analytics
    results, it is difficult to visualize the stored results using either Azure Storage
    Explorer or REST API. Power BI is a collection of business intelligence and analytics
    tools that work with dozens of types of data sources to deliver interactive, visual
    insights via a GUI interface. Some data sources it supports include SQL databases
    (SQL Server, Oracle, and PostgreSQL), blob storage, MS Access, Excel, HDFS (Hadoop),
    Teradata, and XML.Power BI is offered as a web, desktop, and mobile app to quickly
    create dashboards and visualizations filled with charts (line, bar, pie, time-series,
    radar, etc.), maps, cartograms, histograms, grids, clouds, and more. Dashboards
    can be generated and viewed within Power BI or published to your own website or
    blog.ImportantIn order to sign up for or use Power BI, you need a work email address
    that is integrated with Microsoft Office 365\. You cannot use personal email IDs
    (Gmail, Yahoo Mail, or even Hotmail) to sign up for Power BI.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Blob 存储提供了一种便捷、可扩展的方法来存储流分析结果，但是使用 Azure 存储资源管理器或 REST API 难以可视化存储的结果。Power
    BI 是一套商业智能和分析工具，与数十种数据源一起工作，通过 GUI 接口提供交互式、可视化的见解。它支持的一些数据源包括 SQL 数据库（SQL Server、Oracle
    和 PostgreSQL）、Blob 存储、MS Access、Excel、HDFS（Hadoop）、Teradata 和 XML。Power BI 提供网页、桌面和移动应用程序，快速创建填充有图表（线型、柱状、饼状、时间序列、雷达等）、地图、面积图、直方图、网格、云等可视化的仪表板。仪表板可以在
    Power BI 中生成和查看，也可以发布到你自己的网站或博客。重要提示：为了注册或使用 Power BI，你需要一个与 Microsoft Office
    365 集成的工作电子邮件地址。你不能使用个人电子邮件 ID（Gmail、Yahoo 邮箱，甚至 Hotmail）注册 Power BI。
- en: Adding Power BI as an Output in an ASA Job
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 ASA 作业中添加 Power BI 作为输出
- en: 'Stop your ASA job in Azure Portal, if it’s running. Then go to Job Topology
    ➤ Outputs ➤ Add ➤ Power BI. Fill in the details, as shown in Figure [9-10](#Fig10).![A458845_1_En_9_Fig10_HTML.jpg](A458845_1_En_9_Fig10_HTML.jpg)Figure
    9-10Adding Power BI as an output in the Stream Analytics job: before and after
    authorizing a connection to Power BIMake note of the values you fill in for Dataset
    Name and Table Name. We kept these as asa-patientmonitoring and high-pulserates,
    respectively, which are consistent with the naming scheme we follow. Azure will
    automatically create the specified dataset and table in Power BI, if it’s not
    already present. The existing dataset with the same name will be overwritten.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 门户中停止你的 ASA 作业，如果它正在运行。然后转到作业拓扑 ➤ 输出 ➤ 添加 ➤ Power BI。填写细节，如图 [9-10](#Fig10)
    所示。![A458845_1_En_9_Fig10_HTML.jpg](A458845_1_En_9_Fig10_HTML.jpg)图 9-10 在流分析作业中添加
    Power BI 作为输出：在授权连接到 Power BI 之前和之后注意你填写的数据集名称和表名称的值。我们将它们分别保留为 asa-patientmonitoring
    和 high-pulserates，这与我们遵循的命名方案一致。如果尚不存在，Azure 将自动在 Power BI 中创建指定的数据集和表。具有相同名称的现有数据集将被覆盖。
- en: Updating the SA Query
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新 SA 查询
- en: With the output created, let’s head back to query editor. We’ll need to update
    our query to redirect results to our Power BI output. There are two ways to do
    it—replace the current output alias (blob) with the new (Power BI) or add a second
    SELECT INTO statement for the new output. Both approaches are shown next.Replacing
    the existing output:SELECT    System.Timestamp AS Time,    Avg(PatientPulseRate)
    AS AvgPulseRate,    COUNT(*) AS HighPRInstancesINTO    [powerbi-patientmonitoring]FROM    [anuragbhd-iothub-patientmonitoring]    TIMESTAMP
    BY EventEnqueuedUtcTime    GROUP BY TumblingWindow(second, 30), DeviceId    HAVING
    Avg(PatientPulseRate) > 90Adding additional output:WITH HighPulseRates AS (    SELECT        System.Timestamp
    AS Time,        Avg(PatientPulseRate) AS AvgPulseRate,        COUNT(*) AS HighPRInstances    FROM        [anuragbhd-iothub-patientmonitoring]        TIMESTAMP
    BY EventEnqueuedUtcTime        GROUP BY TumblingWindow(second, 30), DeviceId        HAVING
    Avg(PatientPulseRate) > 90)SELECT * INTO [blob-patientmonitoring] FROM HighPulseRatesSELECT
    * INTO [powerbi-patientmonitoring] FROM HighPulseRatesNotice carefully that we
    have selected another value—the count of each recorded high pulse rate instance.
    Although this value will be 1 for each instance, we’ll need it for plotting our
    line chart a little later.It is also possible to send different results to different
    outputs. You do that by writing a separate SELECT ... INTO ... FROM statement,
    one after the other, for each output. Update your query, click Save, and start
    the job again. This is also a good time to fire up that simulated device once
    again. Go to Visual Studio and start the project.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 创建输出后，让我们回到查询编辑器。我们需要更新查询以将结果重定向到我们的 Power BI 输出。有两种方法可以做到这一点——用新的输出别名（Power
    BI）替换当前的输出别名（blob），或者为新输出添加第二个 SELECT INTO 语句。接下来分别展示了这两种方法。替换现有输出：SELECT    System.Timestamp
    AS Time,    Avg(PatientPulseRate) AS AvgPulseRate,    COUNT(*) AS HighPRInstancesINTO    [powerbi-patientmonitoring]FROM    [anuragbhd-iothub-patientmonitoring]    TIMESTAMP
    BY EventEnqueuedUtcTime    GROUP BY TumblingWindow(second, 30), DeviceId    HAVING
    Avg(PatientPulseRate) > 90添加额外输出：WITH HighPulseRates AS (    SELECT        System.Timestamp
    AS Time,        Avg(PatientPulseRate) AS AvgPulseRate,        COUNT(*) AS HighPRInstances    FROM        [anuragbhd-iothub-patientmonitoring]        TIMESTAMP
    BY EventEnqueuedUtcTime        GROUP BY TumblingWindow(second, 30), DeviceId        HAVING
    Avg(PatientPulseRate) > 90)SELECT * INTO [blob-patientmonitoring] FROM HighPulseRatesSELECT
    * INTO [powerbi-patientmonitoring] FROM HighPulseRates请注意，我们精心选择了另一个值——每个记录的高脉率实例的计数。尽管这个值对于每个实例都是
    1，但稍后我们会用它来绘制折线图。还可以将不同的结果发送到不同的输出。为此，您需要为每个输出编写一个单独的 SELECT ... INTO ... FROM
    语句，一个接一个地写。更新您的查询，点击保存，然后重新开始作业。现在也是再次启动模拟设备的好时机。转到 Visual Studio 并启动该项目。
- en: Creating Dashboards in Power BI
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Power BI 中创建仪表板
- en: Go to [https://powerbi.com](https://powerbi.com) and log in with your work account.
    If you expand the My Workspace section in the left sidebar and look at DATASETS,
    you should see the name of the dataset you specified earlier if the ASA job produced
    at least one result.Navigate to Workspaces ➤ My Workspace from the sidebar menu.
    We’ll start by creating a dashboard that displays a count of all high pulse rate
    instances for our patient. Choose Create ➤ Dashboard. Give your dashboard a name
    and click Create. In your newly created dashboard, click Add Tile and choose Custom
    Streaming Data, as shown in Figure [9-11](#Fig11). Click Next.![A458845_1_En_9_Fig11_HTML.jpg](A458845_1_En_9_Fig11_HTML.jpg)Figure
    9-11Adding a new tile in a dashboard in Power BIOn the Choose a Streaming Dataset
    screen, select the name of your dataset and click Next. On the Visualization Design
    screen, select Card as the visualization type, and then add highprinstances in
    the Fields section. On the next screen called Tile Details, fill in a title and
    a subtitle and click Apply. Your new tile will be live immediately.Similarly,
    let’s add a new tile for tracking high pulse rates over 10-minute periods. Add
    Tile ➤ Custom Streaming Data ➤ <your dataset>. On the Visualization Design screen,
    choose Line Chart as the type, Time as the axis, avgpulserate as the value, and
    Last 10 Minutes as the time window to display. Once this tile is live, you should
    see a line chart for your selected data. At this point, your dashboard should
    look like Figure [9-12](#Fig12).![A458845_1_En_9_Fig12_HTML.jpg](A458845_1_En_9_Fig12_HTML.jpg)Figure
    9-12A Power BI dashboard with two tilesFeel free to create more titles using the
    data at hand and then watch them update in real time.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 前往[https://powerbi.com](https://powerbi.com)并使用您的工作账户登录。如果您在左侧边栏的“我的工作区”中展开并查看数据集，您应该可以看到您之前指定的数据集名称，如果ASA作业至少生成了一个结果。从侧边栏菜单导航到工作区➤我的工作区。我们将从创建显示我们患者所有高脉搏率实例计数的仪表板开始。选择创建➤仪表板。为您的仪表板取一个名称并点击创建。在您新创建的仪表板中，点击添加图块并选择自定义流数据，如图[9-11](#Fig11)所示。点击下一步。![A458845_1_En_9_Fig11_HTML.jpg](A458845_1_En_9_Fig11_HTML.jpg)图9-11在Power
    BI中仪表板中添加新图块在选择流数据集屏幕中，选择您的数据集名称并点击下一步。在可视化设计屏幕中，选择卡片作为可视化类型，然后在字段中添加highprinstances。在下一个名为图块详细信息的屏幕上，填写标题和副标题并点击应用。您的新图块将立即生效。同样，让我们添加一个新的图块以跟踪高脉搏率在10分钟时间段内的情况。添加图块➤自定义流数据➤<您的数据集>。在可视化设计屏幕中，选择线状图作为类型，时间作为轴，avgpulserate作为数值，并选择最近10分钟作为显示时间窗口。一旦该图块生效，您应该能够看到您选择的数据的线状图。此时，您的仪表板应该看起来像图[9-12](#Fig12)所示。![A458845_1_En_9_Fig12_HTML.jpg](A458845_1_En_9_Fig12_HTML.jpg)图9-12一个拥有两个图块的Power
    BI仪表板随意创建更多的标题，使用手头的数据，并观察它们实时更新。
- en: Next Steps
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步
- en: 'With the power of Stream Analytics and Power BI in your hands, we encourage
    you to try slightly more complex scenarios. Try solving the problem mentioned
    in the following exercise, or take hints from it to solve a different problem.Real-Time
    Dengue Outbreak DetectionDengue fever is a mosquito-borne viral disease, common
    in certain South and Southeast Asian regions, such as India and Singapore. If
    not detected and treated early, dengue can be fatal. As per online statistics,
    hundreds of millions of people are affected by the disease each year and about
    20,000 die because of it. The disease starts as a regular virus and gradually
    becomes severe in some cases (within 7-10 days). Dengue outbreaks recur every
    year in certain areas, and timely detection of such outbreaks may help in timely
    and effective public health interventions.Symptoms of the diseases include, among
    several others:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有流分析和Power BI的强大功能，我们鼓励您尝试稍微复杂一点的场景。尝试解决以下练习中提到的问题，或者从中获取提示来解决其他问题。实时登革热爆发检测登革热是一种由蚊虫传播的病毒性疾病，在印度和新加坡等一些南亚和东南亚地区很常见。如果不及时检测和治疗，登革热可能会致命。根据在线统计，每年有数亿人感染这种疾病，其中约有2万人因此而死亡。该疾病开始时像普通病毒一样，并且在一些情况下逐渐加重（在7-10天内）。登革热爆发每年在某些地区都会再次发生，及时检测到这些爆发可能有助于及时有效的公共卫生干预。该疾病的症状包括，但不限于：
- en: High fever
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高烧
- en: Rapid breathing
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速呼吸
- en: Severe abdominal pain
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 严重腹痛
- en: 'Your task is to design a real-time dengue outbreak detection system that one
    hospital or several hospitals (perhaps on a Blockchain) can use to take timely
    action.System DesignEach IoT medical device should report at least the following
    telemetry data about a patient:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您的任务是设计一个实时登革热爆发检测系统，一个医院或多个医院（可能在区块链上）可以使用以及采取及时行动。系统设计每个IoT医疗设备应至少报告患者的以下遥测数据：
- en: Body temperature
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体温
- en: Respiration rate
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 呼吸速率
- en: Abdominal pain score
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 腹痛评分
- en: Location
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置
- en: 'The location of a person can be easily tracked via GPS. As for determining
    a pain score, there are devices in the market that do this. Check this: [http://gomerblog.com/2014/06/pain-detector](http://gomerblog.com/2014/06/pain-detector)
    . Alternatively, a proper measurement of functional magnetic resonance imaging
    (fMRI) brain scans can also give indications of pain levels, as reported here
    [https://www.medicalnewstoday.com/articles/234450.php](https://www.medicalnewstoday.com/articles/234450.php)
    .Your job initially is not to use such sophisticated medical devices for taking
    measurements, but to create about a dozen simulated devices for the job that report
    telemetry data for each patient.You can then write an SA query to perform aggregate
    operations on incoming data stream and count the number of cases where telemetry
    values are above certain thresholds at the same location. If you get multiple
    locations reporting a bunch of dengue cases around the same time, you have the
    outbreak pattern.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人的位置可以通过GPS轻易被追踪。至于确定疼痛分数，市场上有设备可以做到这一点。查看这个链接：[http://gomerblog.com/2014/06/pain-detector](http://gomerblog.com/2014/06/pain-detector)。另外，适当测量功能性磁共振成像（fMRI）脑部扫描也可以给出疼痛水平的指示，正如在这里报道的：[https://www.medicalnewstoday.com/articles/234450.php](https://www.medicalnewstoday.com/articles/234450.php)。你的任务起初不是使用这些复杂的医疗设备来进行测量，而是为这项工作创建大约十几个模拟设备，这些设备会为每个患者报告遥测数据。然后，你可以编写一个SA查询，对流入的数据流进行聚合操作，并计算出在相同位置遥测值高于某个阈值的情况数量。如果你在同一时间收到多个地点报告了一堆登革热病例，你就找到了暴发模式。
- en: Recap
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复盘
- en: 'In this chapter, you learned how to perform complex real-time analyses on streaming
    data, such as that received by an IoT Hub from hundreds or thousands of connected
    devices. You learned how to:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学会了如何对来自物联网中心的数百甚至数千台连接设备接收的流式数据进行复杂实时分析。您学会了如何：
- en: Create a new Stream Analytics job in the Azure Portal
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Azure Portal中创建一个新的流分析作业
- en: Add (and test) an input (hub) to an ASA job
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入（中心）添加（并测试）到ASA作业
- en: Write queries that work on time-bound windows of streaming data
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写在流式数据的时间限制窗口上工作的查询
- en: Add (and test) a blob storage output to an ASA job
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一个ASA工作添加（并测试）到blob存储输出
- en: Access Stream Analytics results stored in a blob using Azure Storage Explorer
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Azure Storage Explorer访问存储在blob中的流分析结果
- en: Add Power BI as an output to an ASA job
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Power BI添加为ASA工作的输出
- en: Create a dashboard with multiple tiles in Power BI to visualize ASA results
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Power BI中创建一个具有多个瓷砖的仪表板，以可视化ASA的结果
- en: In the next chapter, you will take data analytics to the next level by using
    Azure machine learning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将通过使用Azure机器学习将数据分析提升到新的水平。
