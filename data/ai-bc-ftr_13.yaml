- en: © The Author(s), under exclusive license to Springer Nature Switzerland AG 2021Y.
    Maleh et al. (eds.)Artificial Intelligence and Blockchain for Future Cybersecurity
    ApplicationsStudies in Big Data90[https://doi.org/10.1007/978-3-030-74575-2_11](https://doi.org/10.1007/978-3-030-74575-2_11)
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: © 作者，独家许可给 Springer Nature Switzerland AG 2021
- en: Spark Based Intrusion Detection System Using Practical Swarm Optimization Clustering
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于 Spark 的入侵检测系统使用实用粒子群优化聚类
- en: 'Mohamed Aymen Ben HajKacem^([1](#Aff7)  ), Mariem Moslah^([1](#Aff7)) and Nadia Essoussi^([1](#Aff7) [ ](#ContactOfAuthor3))(1)LARODEC,
    Institut Supérieur de Gestion de Tunis, Université de Tunis, 41 Avenue de la liberté,
    cité Bouchoucha, 2000 Le Bardo, TunisiaNadia EssoussiEmail: [nadia.essoussi@isg.rnu.tn](mailto:nadia.essoussi@isg.rnu.tn)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Mohamed Aymen Ben HajKacem^([1](#Aff7)), Mariem Moslah^([1](#Aff7)) 和 Nadia
    Essoussi^([1](#Aff7))[ ](#ContactOfAuthor3)
- en: Abstract
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Given the availability growth of data in large networks, intrusion detection
    systems become an important challenge since they require efficient methods to
    discover attacks from such networks. This paper proposes a new Spark based intrusion
    detection system using particle swarm optimization clustering, referred to as
    IDS-SPSO, for large scale data able to provide good tradeoff between scalability
    and accuracy. The use of Particle swarm optimization clustering is argued to avoid
    the sensitivity problem of initial cluster centers as well as premature convergence.
    In addition, we propose in this work to take advantage of parallel processing
    based on the Spark framework. Experiments performed on several large collections
    of real intrusion data have shown the effectiveness of the proposed intrusion
    detection system in terms of scalability and clustering accuracy.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于大型网络中数据的可用性增长，入侵检测系统成为一个重要挑战，因为它们需要有效的方法从这样的网络中发现攻击。本文提出了一种基于 Spark 的入侵检测系统，使用粒子群优化聚类，称为
    IDS-SPSO，用于大规模数据，能够在可扩展性和准确性之间提供良好的权衡。据称，使用粒子群优化聚类可以避免初始聚类中心的敏感性问题以及过早收敛的问题。此外，我们在这项工作中提出利用基于
    Spark 框架的并行处理。对几个大型真实入侵数据集进行的实验表明，所提出的入侵检测系统在可扩展性和聚类准确性方面的有效性。
- en: KeywordsIntrusion detectionBig dataClusteringPSOSpark
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：入侵检测、大数据、聚类、PSO、Spark
- en: 1 Introduction
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Given the ever increasing growth and popularity of Internet, network intrusion
    detection becomes an important challenge to provide protection and security for
    information. This is explained by the large number of users and the large amount
    of data exchanged which makes it difficult to distinguish between the normal connections
    and attacks. To this end, intrusion detection systems (IDSs) are designed to deal
    with large amounts of data in order to protect a system against network attacks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于互联网的不断增长和普及，网络入侵检测成为提供信息保护和安全的重要挑战。这是因为用户数量和数据交换量巨大，难以区分正常连接和攻击。为此，入侵检测系统（IDSs）被设计用于处理大量数据，以保护系统免受网络攻击。
- en: Several machine learning techniques were applied for IDSs in the literature [[1](#CR1),
    [6](#CR6), [11](#CR11), [23](#CR23), [26](#CR26), [33](#CR33)]. Clustering is
    one of the machine learning techniques that is used to organize data into groups
    of similar data points called also clusters [[18](#CR18)]. Clustering methods
    can be mainly categorized into five classes namely hierarchical, density-based,
    grid-based, model-based and partitional methods [[39](#CR39)]. K-means [[24](#CR24)]
    as one of the partitional clustering methods, remains the most efficient because
    of its simplicity and linear time complexity. However, it is sensitive to the
    selection of initial cluster centers, since it can produce local optimal solutions
    when the initial cluster centers are not properly selected [[10](#CR10)].
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中对 IDSs 应用了几种机器学习技术 [[1](#CR1), [6](#CR6), [11](#CR11), [23](#CR23), [26](#CR26),
    [33](#CR33)]。聚类是用于将数据组织成相似数据点组的一种机器学习技术，也称为聚类 [[18](#CR18)]。聚类方法主要可以分为五类，即层次、密度、网格、模型和分区方法 [[39](#CR39)]。作为分区聚类方法之一的
    K-means [[24](#CR24)] 由于其简单性和线性时间复杂度而仍然是最有效的。然而，由于初始聚类中心的选择敏感，当未正确选择初始聚类中心时，它可能产生局部最优解 [[10](#CR10)]。
- en: To deal with this issue, several optimization algorithms were introduced to
    solve the data clustering problem [[8](#CR8), [14](#CR14), [17](#CR17), [20](#CR20),
    [27](#CR27), [31](#CR31), [34](#CR34)]. Genetic optimization algorithm which is
    based on a mutation operator to deal with clustering task was designed in [[20](#CR20)].
    Simulated annealing optimization was also used for data clustering in [[8](#CR8)].
    Particle Swarm Optimization (PSO), was proposed to solve the clustering problem,
    by using multiple search directions with social behavior to enhance the quality
    of the clustering result [[34](#CR34)]. Among these algorithms, Particle Swarm
    Optimization (PSO) has gain a great popularity because of its efficiency [[29](#CR29)].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，引入了几种优化算法来解决数据聚类问题[[8](#CR8), [14](#CR14), [17](#CR17), [20](#CR20),
    [27](#CR27), [31](#CR31), [34](#CR34)]。基于变异算子的遗传优化算法被设计用来处理聚类任务[[20](#CR20)]。模拟退火优化也被用于数据聚类[[8](#CR8)]。粒子群优化（PSO）被提出来解决聚类问题，通过使用具有社会行为的多个搜索方向来增强聚类结果的质量[[34](#CR34)]。在这些算法中，粒子群优化（PSO）因其效率而广受欢迎[[29](#CR29)]。
- en: On the other hands, conventional intrusion detection methods based on clustering
    fail to scale with larger sizes of network traffic and are computationally expensive
    in terms of memory. To deal with large scale data, several distributed clustering
    methods were designed in the literature [[2](#CR2)–[5](#CR5), [12](#CR12), [15](#CR15),
    [30](#CR30), [40](#CR40), [41](#CR41)]. Most of these methods use the MapReduce
    framework [[13](#CR13)] for data processing. However, MapReduce is unsuitable
    for iterative algorithms since it requires repeated times of reading and writing
    to disks. Spark [[9](#CR9), [32](#CR32)] is introduced to overcome the limitations
    of MapReduce, particular for processing iterative algorithms. It is an in–memory
    parallel framework for processing Big data using a cluster of machines. Compared
    with the MapReduce framework, Spark is more efficient and approximately 10 to
    100 times faster for data processing task [[7](#CR7)].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，基于聚类的传统入侵检测方法难以应对网络流量规模较大的情况，并且在内存方面计算开销巨大。为了处理大规模数据，文献中设计了几种分布式聚类方法[[2](#CR2)–[5](#CR5),
    [12](#CR12), [15](#CR15), [30](#CR30), [40](#CR40), [41](#CR41)]。其中大多数方法使用了 MapReduce
    框架[[13](#CR13)] 进行数据处理。然而，MapReduce 不适合迭代算法，因为它需要多次读写磁盘。Spark[[9](#CR9), [32](#CR32)]
    被引入来克服 MapReduce 的局限性，特别适用于处理迭代算法。它是一个用于使用一组机器的内存并行处理大数据的框架。与 MapReduce 框架相比，Spark
    在数据处理任务上更有效率，大约快 10 到 100 倍[[7](#CR7)]。
- en: This paper proposes a new Spark based intrusion detection system (IDS-SPSO).
    The proposed system builds the intrusion detection model using PSO clustering.
    To the best of our knowledge, this is the first work that implements parallel
    intrusion detection system using PSO and Spark framework. The aim is to show how
    the proposed system takes advantage of Spark and PSO to deal with real large scale
    intrusion data to achieve high accuracy quality and scalability.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种基于 Spark 的新型入侵检测系统（IDS-SPSO）。所提出的系统利用 PSO 聚类构建入侵检测模型。据我们所知，这是第一项使用 PSO
    和 Spark 框架实现并行入侵检测系统的工作。旨在展示所提出的系统如何利用 Spark 和 PSO 处理真实的大规模入侵数据，以达到高准确性和可扩展性。
- en: 'The remainder of this paper is organized as follows: Sect. [1](#Sec1) presents
    background definitions related to the Particle Swarm Optimization (PSO) and parallel
    frameworks. Section [2](#Sec2) discusses the related works in the area of intrusion
    detection methods based on clustering. Then, Sect. [3](#Sec6) describes the proposed
    parallel intrusion detection system while Sect. [4](#Sec7) presents the experimental
    results performed on large real intrusion data. Finally, Sect. [5](#Sec15) gives
    concluding remarks and some future works.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的剩余部分安排如下：第 [1](#Sec1) 节介绍与粒子群优化（PSO）和并行框架相关的背景定义。第 [2](#Sec2) 节讨论基于聚类的入侵检测方法领域的相关工作。然后，第
    [3](#Sec6) 节描述了所提出的并行入侵检测系统，而第 [4](#Sec7) 节展示了在大规模真实入侵数据上进行的实验结果。最后，第 [5](#Sec15)
    节给出了结论性的评论和一些未来的工作。
- en: 2 Preliminaries
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步
- en: This section first presents background definitions related to the Particle Swarm
    Optimization (PSO) followed by the parallel frameworks which are used in this
    work.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先介绍与粒子群优化（PSO）相关的背景定义，然后介绍本文中使用的并行框架。
- en: 2.1 Particle Swarm Optimization
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 粒子群优化
- en: Particle Swarm Optimization (PSO) was introduced by the electrical engineer
    Eberhart and the social psychologist Kendy [[29](#CR29)]. This algorithm was proposed
    to simulate the social behavior of birds when searching for food. When a bird
    recognizes a food area, it broadcasts the information to all the swarm. Hence,
    all the birds follow him and this way they raise the probability of finding the
    food since it is a collaborative work. So, the behavior of birds within swarms
    was turned into an intelligent algorithm capable of solving several optimization
    problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 粒子群优化（PSO）是由电气工程师埃伯哈特和社会心理学家肯迪引入的[[29](#CR29)]。该算法旨在模拟鸟类寻找食物时的社会行为。当一只鸟发现一个食物区域时，它会向整个群体广播信息。因此，所有鸟都跟随它，这样它们提高了找到食物的概率，因为这是一项协作工作。因此，群体内鸟类的行为被转化为一种智能算法，能够解决多个优化问题。
- en: PSO is a population based optimization algorithm. It consists of a swarm of
    particles where each particle represents a potential solution to the optimization
    problem. Each particle ![$$P_i$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq1.png)
    is characterized at the time *t*, by the current position ![$$x_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq2.png)
    in the search space, the velocity ![$$v_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq3.png),
    the personal best position ![$$pbestP_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq4.png)
    and the fitness value ![$$pbestF_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq5.png).
    The personal best position represents the best fitness value the particle has
    ever seen, which is calculated by:![$$\begin{aligned} pbestP_{i}(t+1)={\left\{
    \begin{array}{ll} pbestP_{i}(t)\, if \, f( pbestP_{i}(t))&lt;= f(x_ {i}(t+1))\\
    x_{i}(t+1) \, if \, f(pbestP_{i}(t)) &gt; f(x_{i}(t+1))\\ \end{array}\right. }
    \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ1.png)(1)The
    personal best position represents the best fitness value any particle has ever
    experienced, which is calculated by:![$$\begin{aligned} gbestP(t+1)= min \, (f(y)
    , f(gbestP(t))) \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ2.png)(2)where
    ![$$\, y \in \lbrace { pbestP_{0}(t),...,pbestP_{S}(t)\rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq6.png).
    The following equation is used to update the particles positions within the problem
    search space.![$$\begin{aligned} x_{i}(t+1) \leftarrow x_{i}(t)+v_{i}(t) \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ3.png)(3)While
    the following equation is used to update the particle velocities.![$$\begin{aligned}
    v_{i}(t+1) \leftarrow w v_{i}(t)+c_{1}r_{1}(pbestP_{i}(t)-x_{i} (t))+c_{2}r_{2}(gbestP(t)-x_{i}(t))
    \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ4.png)(4)where
    *w* is the inertia weight, ![$$x_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq7.png)
    is the position of the particle ![$$P_i$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq8.png)
    at the time *t*, ![$$v_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq9.png)
    is the velocity of the particle ![$$P_i$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq10.png)
    at the time *t*, ![$$c_{1}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq11.png)
    and ![$$c_{2}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq12.png)
    are two acceleration coefficients, and ![$$r_{1}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq13.png)
    and ![$$r_{2}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq14.png)
    are two random values in the range [0, 1]. The main algorithm of PSO is outlined
    in Algorithm 1\.![../images/507793_1_En_11_Chapter/507793_1_En_11_Figa_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Figa_HTML.png)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: PSO 是一种基于群体的优化算法。它由一群粒子组成，每个粒子代表优化问题的一个潜在解决方案。每个粒子 ![$$P_i$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq1.png)
    在时间 *t* 被其在搜索空间中的当前位置 ![$$x_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq2.png)、速度
    ![$$v_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq3.png)、个人最佳位置
    ![$$pbestP_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq4.png)
    和适应度值 ![$$pbestF_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq5.png)
    表征。个人最佳位置表示粒子曾经见过的最佳适应度值，其计算方式为：![$$\begin{aligned} pbestP_{i}(t+1)={\left\{ \begin{array}{ll}
    pbestP_{i}(t)\, if \, f( pbestP_{i}(t))&lt;= f(x_ {i}(t+1))\\ x_{i}(t+1) \, if
    \, f(pbestP_{i}(t)) &gt; f(x_{i}(t+1))\\ \end{array}\right. } \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ1.png)(1)个人最佳位置表示任何粒子曾经经历过的最佳适应度值，其计算方式为：![$$\begin{aligned}
    gbestP(t+1)= min \, (f(y) , f(gbestP(t))) \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ2.png)(2)其中
    ![$$\, y \in \lbrace { pbestP_{0}(t),...,pbestP_{S}(t)\rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq6.png)。下列方程用于更新问题搜索空间内的粒子位置：![$$\begin{aligned}
    x_{i}(t+1) \leftarrow x_{i}(t)+v_{i}(t) \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ3.png)(3)而下列方程用于更新粒子速度：![$$\begin{aligned}
    v_{i}(t+1) \leftarrow w v_{i}(t)+c_{1}r_{1}(pbestP_{i}(t)-x_{i} (t))+c_{2}r_{2}(gbestP(t)-x_{i}(t))
    \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ4.png)(4)其中
    *w* 是惯性权重，![$$x_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq7.png)
    是粒子 ![$$P_i$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq8.png)
    在时间 *t* 的位置，![$$v_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq9.png)
    是粒子 ![$$P_i$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq10.png)
    在时间 *t* 的速度，![$$c_{1}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq11.png)
    和 ![$$c_{2}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq12.png)
    是两个加速系数，![$$r_{1}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq13.png)
    和 ![$$r_{2}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq14.png)
    是两个在区间 [0, 1] 中的随机值。PSO 的主要算法概述如算法 1\.![../images/507793_1_En_11_Chapter/507793_1_En_11_Figa_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Figa_HTML.png)
- en: 2.2 MapReduce Framework
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 MapReduce 框架
- en: MapReduce [[13](#CR13)] is a parallel programming framework for data processing.
    As shown in Fig. [1](#Fig1), MapReduce is composed of three phases namely *map*,
    *shuffle* and *reduce*. Each phase processes data through ![$${&lt;}key/value{&gt;}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq15.png)
    pairs. The map phase applies the map function by taking in parallel each ![$${&lt;}key^{}/value^{}{&gt;}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq16.png)
    and generates a set of intermediate ![$${&lt;}key^{'}/value^{'}{&gt;}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq17.png)
    pairs. Then, the shuffle phase merges all intermediate values which share the
    same intermediate key as a list. The reduce phase applies the reduce function
    to group all intermediate values associated with the same intermediate key. Note
    the implementation of the MapReduce framework is available in Hadoop [[35](#CR35)].
    And the inputs and outputs of MapReduce are stored in a distributed file system
    which is called Hadoop Distributed File System (HDFS). Despite of its performance
    to deal with Big data, MapReduce framework is unsuitable to fit when executing
    iterative algorithms [[22](#CR22)]. Since, it requires at each iteration reading
    and writing data from disks, which can increase the running time.![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig1_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig1_HTML.png)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce [[13](#CR13)] 是用于数据处理的并行编程框架。如图 [1](#Fig1) 所示，MapReduce 由三个阶段组成，即
    *map*、*shuffle* 和 *reduce*。每个阶段通过 ![$${&lt;}key/value{&gt;}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq15.png)
    对来处理数据。映射阶段通过并行处理每个 ![$${&lt;}key^{}/value^{}{&gt;}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq16.png)
    并生成一组中间 ![$${&lt;}key^{'}/value^{'}{&gt;}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq17.png)
    对应用映射函数。然后，shuffle 阶段将所有共享相同中间键的中间值合并为列表。reduce 阶段应用 reduce 函数以组合与同一中间键关联的所有中间值。请注意，MapReduce
    框架的实现可在 Hadoop [[35](#CR35)] 中使用。MapReduce 的输入和输出存储在分布式文件系统中，称为 Hadoop 分布式文件系统（HDFS）。尽管
    MapReduce 框架在处理大数据方面表现出色，但在执行迭代算法时不适用 [[22](#CR22)]。因为它要求在每次迭代中从磁盘读取和写入数据，这可能会增加运行时间。![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig1_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig1_HTML.png)
- en: Fig. 1
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: Flowchart of MapReduce framework
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 框架的流程图
- en: 2.3 Spark Framework
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 Spark 框架
- en: Spark framework supports iterative computation and has an improved processing
    speed compared to MapReduce since it utilizes in-memory computations using the
    resilient distributed datasets (RDDs). These RDDs can be cached in memory to be
    used in multiple consecutive operations. Spark [[42](#CR42)] is introduced to
    run with Hadoop [[35](#CR35)], especially by reading data from HDFS. Moreover,
    it provides a set of in-memory operators, beyond the standard MapReduce, with
    the aim of processing data more rapidly on distributed environments compared to
    MapReduce [[32](#CR32)]. Spark framework proposes two types of operators which
    can deal with RDD called, transformations and actions. The transformations are
    designed to execute a function to the whole records and generate new RDD. Map,
    ReduceBykey and MapPartition are examples of transformations. The actions are
    designed to return a value to the program and store the final result of the computation
    in a file system. Filter and Count are examples of actions. The Data flow of Spark
    framework is shown in Fig. [2](#Fig2).![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig2_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig2_HTML.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 框架支持迭代计算，并且与 MapReduce 相比具有改进的处理速度，因为它利用了内存计算，使用弹性分布式数据集（RDD）进行计算。这些 RDD
    可以缓存在内存中，以在多个连续操作中使用。Spark [[42](#CR42)] 被引入以与 Hadoop [[35](#CR35)] 一起运行，特别是通过从
    HDFS 读取数据。此外，它提供了一组内存操作符，超出了标准的 MapReduce，旨在在分布式环境中比 MapReduce 更快地处理数据 [[32](#CR32)]。Spark
    框架提出了两种可以处理 RDD 的操作符，称为转换和动作。转换被设计为对所有记录执行函数并生成新的 RDD。Map、ReduceByKey 和 MapPartition
    是转换的示例。动作被设计为向程序返回一个值，并将计算的最终结果存储在文件系统中。Filter 和 Count 是动作的示例。Spark 框架的数据流程如图
    [2](#Fig2) 所示。![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig2_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig2_HTML.png)
- en: Fig. 2
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: Data flow of Spark framework
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 框架的数据流程
- en: 3 Related Works
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 相关工作
- en: Several intrusion detection based on machine learning techniques were proposed
    in the literature [[1](#CR1), [6](#CR6), [11](#CR11), [23](#CR23), [26](#CR26),
    [33](#CR33)]. These methods can be divided into supervised or unsupervised according
    the type of the used data during the processing are labelled or not. Several techniques
    have been designed for intrusion detection systems using unsupervised approach
    such as clustering-based methods [[16](#CR16), [19](#CR19), [21](#CR21), [28](#CR28)].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出了基于机器学习技术的多个入侵检测方法[[1](#CR1), [6](#CR6), [11](#CR11), [23](#CR23), [26](#CR26),
    [33](#CR33)]。这些方法可以根据处理过程中所使用的数据类型是否标记来划分为监督或无监督。已设计出多种用于入侵检测系统的无监督方法，如基于聚类的方法[[16](#CR16),
    [19](#CR19), [21](#CR21), [28](#CR28)]。
- en: Peng et al. [[28](#CR28)] proposed mini batch k-means clustering method for
    intrusion detection. They employ the principal component analysis technique to
    reduce the number of dimensions of the used data set in order to enhance the clustering
    efficiency. However, this method considers only a small sample size of intrusion
    data set which can leads a loss of quality.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Peng 等人[[28](#CR28)]提出了用于入侵检测的小批量k均值聚类方法。他们采用主成分分析技术来减少所用数据集的维数，以增强聚类效率。然而，该方法仅考虑了入侵数据集的小样本量，可能导致质量损失。
- en: Leung et al. [[21](#CR21)] designed a density-based clustering method which
    employs the frequent pattern tree in order to solve the high dimensionality of
    the used data set. This method was tested on one million records and achieved
    a good detection results.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Leung 等人[[21](#CR21)]设计了一种基于密度的聚类方法，该方法采用频繁模式树来解决所用数据集的高维度问题。该方法在一百万条记录上进行了测试，并取得了良好的检测结果。
- en: Jiang et al. [[19](#CR19)] proposed a fuzzy c-means clustering intrusion detection
    method where they employ a weighting strategy for the record membership calculation.
    This method was tested with five data samples where each sample having ten thousand
    records. The results show high false positives rates with satisfactory detection
    rates.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Jiang 等人[[19](#CR19)]提出了一种模糊c均值聚类入侵检测方法，在其中采用了一种加权策略进行记录成员计算。该方法在五个数据样本上进行了测试，每个样本有一万条记录。结果显示，虽然存在高假阳性率，但检测率令人满意。
- en: Harish et al. [[16](#CR16)] proposed a modified version of fuzzy c-means clustering
    method for anomaly detection. They employ Principal component analysis (PCA) as
    feature selection technique to deal with curse of dimensionality. In addition,
    this methods is based on using gaussian kernel as distance measure to compute
    the distance between cluster center and samples. The advantage of using gaussian
    kernel is that it reduces the effect of noise.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Harish 等人[[16](#CR16)]提出了模糊c均值聚类方法的修改版本用于异常检测。他们采用主成分分析（PCA）作为特征选择技术来处理维度诅咒。此外，该方法基于使用高斯核作为距离度量来计算聚类中心和样本之间的距离。使用高斯核的优点是可以减少噪声的影响。
- en: Wankhade et al. [[36](#CR36)] proposed ensemble clustering method to deal with
    intrusion detection problem. This method is based on k-means and divide and merge
    strategy which is used to select the accurate number of cluster centers. The experimental
    results have shown that this strategy can improve detection rate and lower false
    alarm rate.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Wankhade 等人[[36](#CR36)]提出了集成聚类方法来解决入侵检测问题。该方法基于k均值和分割与合并策略，用于选择准确的聚类中心数。实验结果表明，该策略可以提高检测率并降低误报率。
- en: Although the attested performance of existing intrusion detection methods, they
    fails to organize large network traffic. To solve the large scale intrusion data,
    parallel methods were proposed to perform distributed computations [[2](#CR2)–[5](#CR5),
    [12](#CR12), [15](#CR15), [30](#CR30), [40](#CR40), [41](#CR41)]. Most of these
    methods use the MapReduce as a parallel programming framework.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有入侵检测方法的表现已经得到验证，但它们无法对大规模网络流量进行组织。为了解决大规模入侵数据，提出了并行方法来执行分布式计算[[2](#CR2)–[5](#CR5),
    [12](#CR12), [15](#CR15), [30](#CR30), [40](#CR40), [41](#CR41)]。其中大多数方法使用MapReduce作为并行编程框架。
- en: Aljarah et al. [[2](#CR2)] proposed a parallel intrusion detection system through
    the MapReduce framework referred to as IDS-MRPSO. In addition, they build a clustering
    model by solving the intrusion detection problem using PSO optimization algorithm.
    Finally, the proposed system has been tested using real large scale of intrusion
    data with different training subset sizes to evaluate the scalability and the
    detection quality. However, MapReduce framework is not appropriate to deal with
    iterative algorithms since it requires at each iteration reading and writing data
    from disks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Aljarah 等人 [[2](#CR2)] 提出了一种通过 MapReduce 框架实现的并行入侵检测系统，称为 IDS-MRPSO。此外，他们通过解决入侵检测问题来构建聚类模型，采用
    PSO 优化算法。最后，该系统使用真实的大规模入侵数据进行了测试，以评估可伸缩性和检测质量的不同训练子集大小。然而，MapReduce 框架不适合处理迭代算法，因为它在每次迭代时需要从磁盘读取和写入数据。
- en: Wang and Han [[37](#CR37)] proposed a network intrusion detection based on parallel
    DPC clustering. This method is based on cut off distance strategy which reduces
    the number of comparisons between data points and clusters. Furthermore, they
    proposed fitting the DPC clustering using Spark framework in order to deal with
    the scalability. However this method remains sensitive to the random selection
    of initial cluster centers [[10](#CR10)].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 和 Han [[37](#CR37)] 提出了基于并行 DPC 聚类的网络入侵检测方法。该方法基于截断距离策略，减少了数据点和聚类之间的比较次数。此外，他们提出使用
    Spark 框架拟合 DPC 聚类以处理可伸缩性问题。然而，该方法仍对初始聚类中心的随机选择敏感 [[10](#CR10)]。
- en: It is important to note that our proposed system is the first work which is
    based on fitting a parallel intrusion detection system through Spark framework.
    Compared with the MapReduce, Spark is a good in-memory parallel framework for
    data processing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的系统是第一个基于 Spark 框架拟合并行入侵检测系统的工作，这一点很重要。与 MapReduce 相比，Spark 是一个很好的内存并行框架，用于数据处理。
- en: 4 Proposed Intrusion Detection System (IDS-SPSO)
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 提出的入侵检测系统（IDS-SPSO）
- en: 'Large network traffic data needs an efficient intrusion detection system to
    protect it against attacks. The proposed intrusion detection system incorporates
    the data clustering process based on the PSO algorithm. Furthermore, PSO clustering
    is distributed using Spark framework in order to scale with large network traffic.
    As shown in Fig. [3](#Fig3), the proposed intrusion detection system consists
    of three main phases: *pre-processing phase*, *data detector modeling phase*,
    and *validation phase*. The first phase is devoted to apply set of data pre-processing
    techniques such as missing values removal, categorical feature elimination and
    data normalization. Once the pre-processing phase is completed, we propose in
    the second phase to apply Spark based PSO clustering method (S-PSO) [[25](#CR25)]
    on training data in order to generate global best centroids vectors. In the third
    phase, we evaluate the quality of the detection model by computing distances between
    the testing data and the final global best centroids vectors.![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig3_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig3_HTML.png)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 大型网络流量数据需要高效的入侵检测系统来防御攻击。所提出的入侵检测系统结合了基于 PSO 算法的数据聚类过程。此外，PSO 聚类使用 Spark 框架进行分布，以便与大型网络流量一起扩展。如图
    [3](#Fig3) 所示，所提出的入侵检测系统包括三个主要阶段：*预处理阶段*、*数据检测建模阶段* 和 *验证阶段*。第一阶段致力于应用一系列数据预处理技术，例如删除缺失值、消除分类特征和数据归一化。一旦完成预处理阶段，我们建议在第二阶段对训练数据应用基于
    Spark 的 PSO 聚类方法（S-PSO） [[25](#CR25)]，以生成全局最佳质心向量。在第三阶段，我们通过计算测试数据与最终全局最佳质心向量之间的距离来评估检测模型的质量。![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig3_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig3_HTML.png)
- en: Fig. 3
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3
- en: Flowchart of IDS-SPSO system
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: IDS-SPSO 系统的流程图
- en: 4.1 Pre-processing Phase
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 预处理阶段
- en: First, we remove the records which contain missing values since we use these
    records in the distance computation when building clusters. So, we cannot use
    in the distance computation a record which contains a missing value. Then, we
    eliminate the categorical features. We propose in this work to consider only numerical
    features in the distance computation, because we need a special distance metric
    for the categorical features. After that, we apply the normalization to the obtained
    data set in order to avoid the bias problem for some features which have a large
    variability between minimum and maximum values. The normalization process is performed
    using the following equation:![$$\begin{aligned} x_{ij_{new}}=\frac{x_{ij}-x_{j_{min}}}{x_{j_{max}}-x_{j_{min}}}
    \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ5.png)(5)where
    ![$$x_{ij}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq18.png)
    is the value of record *i* for feature *j*, ![$$x_{ij_{new}}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq19.png)
    is the normalized value of record *i* for feature *j*, ![$$x_{j_{min}}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq20.png)
    is the minimum value of feature *j* and ![$$x_{j_{max}}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq21.png)
    is the maximum value of feature *j*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们移除包含缺失值的记录，因为在构建聚类时我们使用这些记录进行距离计算。因此，在距离计算中，我们不能使用包含缺失值的记录。然后，我们消除分类特征。在这项工作中，我们建议仅考虑数值特征进行距离计算，因为对于分类特征，我们需要一种特殊的距离度量。之后，我们对得到的数据集进行归一化处理，以避免某些特征在最小值和最大值之间具有较大的变化性而导致的偏差问题。归一化过程使用以下方程进行：![$$\begin{aligned}
    x_{ij_{new}}=\frac{x_{ij}-x_{j_{min}}}{x_{j_{max}}-x_{j_{min}}} \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ5.png)(5)其中
    ![$$x_{ij}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq18.png)
    是特征 *j* 的记录 *i* 的值，![$$x_{ij_{new}}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq19.png)
    是特征 *j* 的记录 *i* 的归一化值，![$$x_{j_{min}}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq20.png)
    是特征 *j* 的最小值，![$$x_{j_{max}}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq21.png)
    是特征 *j* 的最大值。
- en: 4.2 Data Detector Modeling Phase
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数据检测建模阶段
- en: The data detector modeling phase consists on applying S-PSO method [[25](#CR25)]
    to the data results from the pre-processing phase. The authors proposed an efficient
    PSO clustering method using Spark. The experimental results on large scale data
    show that S-PSO scales very well with increasing data and achieved a good clustering
    accuracy. This method reads the data set only once in contrast to existing MapReduce
    implementation of PSO clustering. Hence, it exploits the flexibility provided
    by Spark framework, by using in-memory operations that alleviate the consumption
    time of existing MapReduce solution [[2](#CR2)].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据检测建模阶段包括将 S-PSO 方法应用于预处理阶段产生的数据。作者提出了一种利用 Spark 的高效 PSO 聚类方法。对大规模数据的实验结果显示，随着数据的增加，S-PSO
    的扩展性非常好，并且达到了良好的聚类准确性。与现有的 PSO 聚类的 MapReduce 实现相比，该方法仅需一次读取数据集。因此，它利用了 Spark 框架提供的灵活性，通过使用内存操作来减轻现有
    MapReduce 解决方案的消耗时间 [[2](#CR2)]。
- en: S-PSO method is composed of three MapReduce jobs namely, *Data assignment and
    fitness computation*, *Personal and global best update* and *Position and velocity
    update*. The main process of the S-PSO method is described in Fig. [4](#Fig4).![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig4_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig4_HTML.png)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: S-PSO 方法由三个 MapReduce 作业组成，分别是*数据分配和适应度计算*、*个人和全局最佳更新*以及*位置和速度更新*。S-PSO 方法的主要流程如图 [4](#Fig4)所示。![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig4_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig4_HTML.png)
- en: Fig. 4
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4
- en: Flowchart of S-PSO method
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: S-PSO 方法的流程图
- en: 4.2.1 Data Assignment and Fitness Computation
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 数据分配和适应度计算
- en: In the first MapReduce job, S-PSO starts by creating an initial population which
    is composed of particle’s position, velocity, personal best position and personal
    best fitness. To this end, the positions of particles are randomly initialized
    from the input data set and they represent the initial cluster’s centroids.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个 MapReduce 作业中，S-PSO 首先创建一个初始种群，该种群由粒子的位置、速度、个人最佳位置和个人最佳适应度组成。为此，粒子的位置从输入数据集中随机初始化，并且它们代表了初始聚类的中心。
- en: Then, the data set is divided into chunks and each chunk is assigned to a map
    function. The particle’s information are broadcast to all chunks. The map function
    first assigns each data point to the nearest cluster centroid in each particle
    by computing distances. Then, the map function generates a key value pair as output
    where the key represents the couple particleID and centroidID and the value represents
    the minimum distance between a data point and the centroidID in a particleID.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，数据集被分成块，并且每个块被分配给一个映射函数。粒子的信息被广播到所有块中。映射函数首先通过计算距离将每个数据点分配给每个粒子中最近的聚类质心。然后，映射函数生成一个键值对作为输出，其中键表示粒子ID和质心ID的组合，值表示在粒子ID中数据点与质心ID之间的最小距离。
- en: Once all the data points are affected to the nearest cluster centroid, a reduce
    function is applied to compute the fitness value by combining merging data from
    different map functions. The fitness value is computed using the total sum of
    squares errors by:![$$\begin{aligned} Fitness=\frac{\sum _{j=1}^{k}\sum _{i=1}^{|C_{j}|}d(r_{i},C_{j})}{k}
    \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ6.png)(6)where
    ![$$d(r_{i},C_{j})$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq22.png)
    represents the distance between the record ![$$r_{i}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq23.png)
    and the cluster’s centroid ![$$C_{j}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq24.png),
    ![$$|C_{j}| $$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq25.png)
    represents the number of records assigned to the centroid ![$$C_{j}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq26.png)
    and *k* represents the number of clusters. Then, the reduce function generated
    key value pairs as output where the key represents the particleID and the value
    represents the fitness value.Let ![$$R=\lbrace {r_{1}...r_{n} \rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq27.png)
    the input data set. Let ![$$P(t)=\lbrace {P_{1}(t) ... P_{S}(t)\rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq28.png)
    the set of the particle’s information where ![$$P_{c}(t)=\lbrace {x_{c}(t),v_{c}(t),
    pbestP_{c}(t),pbestF_{c}(t)\rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq29.png)
    represents the information of particle *c* in the iteration *t* where ![$$x_{c}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq30.png)
    is the position, ![$$v_{c}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq31.png)
    is the velocity, ![$$pbestP_{c}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq32.png)
    is the best position and ![$$pbestF_{c}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq33.png)
    is the best fitness. Let ![$$F=\lbrace {F_{1}...F_{S} \rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq34.png)
    the set of fitness values where ![$$F_{c}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq35.png)
    is the fitness value of the particle *c*. The main steps of Data assignment and
    fitness computation MapReduce job is described in Algorithm 2\.![../images/507793_1_En_11_Chapter/507793_1_En_11_Figb_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Figb_HTML.png)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有数据点被分配给最近的聚类中心，就会应用一个 reduce 函数来计算适应度值，通过合并来自不同映射函数的数据来计算。 适应度值是使用总平方误差来计算的：![$$\begin{aligned}
    适应度=\frac{\sum _{j=1}^{k}\sum _{i=1}^{|C_{j}|}d(r_{i},C_{j})}{k} \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ6.png)(6)其中![$$d(r_{i},C_{j})$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq22.png)表示记录![$$r_{i}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq23.png)与聚类的质心![$$C_{j}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq24.png)之间的距离，![$$|C_{j}|$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq25.png)表示分配给质心![$$C_{j}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq26.png)的记录数，*k*表示聚类数。
    然后，减少函数生成键值对作为输出，其中键表示粒子ID，值表示适应度值。假设![$$R=\lbrace {r_{1}...r_{n} \rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq27.png)是输入数据集。
    假设![$$P(t)=\lbrace {P_{1}(t) ... P_{S}(t)\rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq28.png)是粒子信息集，其中![$$P_{c}(t)=\lbrace
    {x_{c}(t),v_{c}(t), pbestP_{c}(t),pbestF_{c}(t)\rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq29.png)表示迭代*t*中粒子*c*的信息，其中![$$x_{c}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq30.png)是位置，![$$v_{c}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq31.png)是速度，![$$pbestP_{c}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq32.png)是最佳位置，![$$pbestF_{c}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq33.png)是最佳适应度。
    假设![$$F=\lbrace {F_{1}...F_{S} \rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq34.png)是适应度值集，其中![$$F_{c}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq35.png)是粒子*c*的适应度值。
    数据分配和适应度计算 MapReduce 作业的主要步骤描述在算法 2 中。![../images/507793_1_En_11_Chapter/507793_1_En_11_Figb_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Figb_HTML.png)
- en: 4.2.2 Pbest and Gbest Update
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 Pbest 和 Gbest 更新
- en: Once the new particle’s fitness are computed, they are automatically distributed
    to RDDs collections. However, the computation of pbest and gbest is not an expensive
    operation. So, it does not need to be executed in parallel manner. Then, each
    particle updates its personal best position and the global best position.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出新粒子的适应度，它们将自动分配到 RDDs 集合中。然而，个体最佳和全局最佳的计算并不是一个昂贵的操作。因此，它不需要以并行方式执行。然后，每个粒子更新其个体最佳位置和全局最佳位置。
- en: Let ![$$pbestF(t)=\lbrace {pbestF_{1}(t) ... pbestF_{S}(t) \rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq36.png)
    is the set of personal best fitness values where ![$$pbestF_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq37.png)
    is the pbestF of the particle *i* at iteration *t*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '让 ![$$pbestF(t)=\lbrace {pbestF_{1}(t) ... pbestF_{S}(t) \rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq36.png)
    为个体最佳适应度值集合，其中 ![$$pbestF_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq37.png)
    是粒子 *i* 在迭代 *t* 时的个体最佳适应度。 '
- en: Let ![$$pbestP(t)=\lbrace { pbestP_{1}(t) ... pbestP_{S}(t) \rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq38.png)
    is the set of personal best position where ![$$pbestP_{1}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq39.png)
    is the pbestP of the particle *i* at iteration *t*. Let *gbestP* is the position
    of the best particle. The main steps of the pbest and gbest update MapReduce job
    is described in Algorithm 3\.![../images/507793_1_En_11_Chapter/507793_1_En_11_Figc_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Figc_HTML.png)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让 ![$$pbestP(t)=\lbrace { pbestP_{1}(t) ... pbestP_{S}(t) \rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq38.png)
    为个体最佳位置集合，其中 ![$$pbestP_{1}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq39.png)
    是粒子 *i* 在迭代 *t* 时的个体最佳位置。让 *gbestP* 为最佳粒子的位置。个体最佳和全局最佳更新 MapReduce 作业的主要步骤见算法 3\.![../images/507793_1_En_11_Chapter/507793_1_En_11_Figc_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Figc_HTML.png)
- en: 4.2.3 Position and Velocity Update
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 位置和速度更新
- en: During this MapReduce job, S-PSO starts by assigning the particles information
    to different map functions. Then, the map function performs the velocity and position
    update using the Eqs. [3](#Equ3) and [4](#Equ4). While the reduce function groups
    all the intermediate key value pairs computed from the different map functions.
    Once the reduce phase is completed, the data set and particle’s information are
    distributed in RDDs collections which are stored in memory for the next iteration.
    For more details about the S-PSO method, the readers can refer to [[25](#CR25)].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 MapReduce 作业中，S-PSO 首先将粒子信息分配给不同的映射函数。然后，映射函数使用公式 [3](#Equ3) 和 [4](#Equ4)
    执行速度和位置更新。而减少函数将来自不同映射函数计算的所有中间键值对分组。一旦减少阶段完成，数据集和粒子信息将分布在存储在内存中的 RDDs 集合中，以供下一次迭代使用。有关
    S-PSO 方法的更多详细信息，读者可以参考 [[25](#CR25)]。
- en: Let ![$$x(t)=\lbrace { x_{1}(t) ... x_{S}(t)\rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq40.png)
    the set of position values where ![$$x_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq41.png)
    is the position of the particle *i* at iteration *t*. Let ![$$v(t)=\lbrace {v_{1}(t)
    ... v_{S}(t) \rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq42.png)
    the set of velocity values where ![$$v_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq43.png)
    is the velocity of the particle *i* at iteration *t*. The main steps of Position
    and velocity update MapReduce job is described in Algorithm 4\.![../images/507793_1_En_11_Chapter/507793_1_En_11_Figd_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Figd_HTML.png)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让 ![$$x(t)=\lbrace { x_{1}(t) ... x_{S}(t)\rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq40.png)
    为位置值集合，其中 ![$$x_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq41.png)
    是粒子 *i* 在迭代 *t* 时的位置。让 ![$$v(t)=\lbrace {v_{1}(t) ... v_{S}(t) \rbrace }$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq42.png)
    为速度值集合，其中 ![$$v_{i}(t)$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq43.png)
    是粒子 *i* 在迭代 *t* 时的速度。位置和速度更新 MapReduce 作业的主要步骤见算法 4\.![../images/507793_1_En_11_Chapter/507793_1_En_11_Figd_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Figd_HTML.png)
- en: 4.3 Evaluation Phase
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评估阶段
- en: Once the data detector modeling phase is completed, we extract the global best
    centroid vectors from the final particle’s information. During this phase, we
    evaluate the detection model by computing distances between the testing records
    and the global best centroids vectors. After that, we affected the testing records
    to the their nearest clusters by computing distances. The main steps of the evaluation
    phase is described in Algorithm 5\.![../images/507793_1_En_11_Chapter/507793_1_En_11_Fige_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fige_HTML.png)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据探测建模阶段完成，我们从最终粒子信息中提取全局最佳质心向量。在此阶段，我们通过计算测试记录与全局最佳质心向量之间的距离来评估检测模型。之后，我们通过计算距离将测试记录分配给它们最近的簇。评估阶段的主要步骤在算法5中描述。![../images/507793_1_En_11_Chapter/507793_1_En_11_Fige_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fige_HTML.png)
- en: Finally, the cluster labeling process is applied to predict the correct labels
    for clusters which are generated in the testing data assignment step. The assignment
    of cluster labels is performed by retrieving the maximum percentage of intersections
    between the true labels of the testing data, and the assigned clusters that are
    generated by applying the testing data assignment step.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，簇标记过程被应用于预测在测试数据分配步骤中生成的簇的正确标签。通过检索测试数据的真实标签与应用测试数据分配步骤生成的分配的簇之间的最大交集百分比来执行簇标签的分配。
- en: Figure [5](#Fig5) illustrates an example to better understanding the cluster
    labeling process. For cluster ![$$C_1$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq44.png),
    the percentage of the normal records is ![$$\frac{3}{4}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq45.png)
    while the percentage of the attack records is ![$$\frac{1}{4}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq46.png).
    Hence, cluster ![$$C_1$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq47.png)
    is a normal cluster. For cluster ![$$C_2$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq48.png),
    the percentage of the normal records is ![$$\frac{1}{3}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq49.png)
    while the percentage of the attack records is ![$$\frac{2}{3}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq50.png).
    So, cluster ![$$C_2$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq51.png)
    is a attack cluster. Similarly, for cluster ![$$C_3$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq52.png),
    the percentage of the normal records ![$$\frac{1}{3}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq53.png)
    is while the percentage of the attack records is ![$$\frac{2}{3}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq54.png).
    So, cluster ![$$C_3$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq55.png)
    is a attack cluster.![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig5_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig5_HTML.png)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#Fig5)说明了一个示例，更好地理解了簇标记过程。对于簇 ![$$C_1$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq44.png)，正常记录的百分比是
    ![$$\frac{3}{4}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq45.png)，而攻击记录的百分比是
    ![$$\frac{1}{4}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq46.png)。因此，簇
    ![$$C_1$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq47.png)
    是一个正常簇。对于簇 ![$$C_2$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq48.png)，正常记录的百分比是
    ![$$\frac{1}{3}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq49.png)，而攻击记录的百分比是
    ![$$\frac{2}{3}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq50.png)。因此，簇
    ![$$C_2$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq51.png)
    是一个攻击簇。类似地，对于簇 ![$$C_3$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq52.png)，正常记录的百分比是
    ![$$\frac{1}{3}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq53.png)，而攻击记录的百分比是
    ![$$\frac{2}{3}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq54.png)。因此，簇
    ![$$C_3$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq55.png)
    是一个攻击簇。![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig5_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig5_HTML.png)
- en: Fig. 5
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5
- en: An illustrative example of the clusters labeling process
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 簇标记过程的说明性示例
- en: 4.4 Time Complexity Analysis
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 时间复杂度分析
- en: In order to show the effectiveness of the proposed system, we describe in the
    following the evaluation of the time complexity of the S-PSO method. Given *n*
    is the data set size, *k* is the number of clusters, *c* is the number of data
    chunks, *l* is the number of iterations and *s* is the swarm size.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示所提出系统的有效性，我们以下面的方式描述了对S-PSO方法时间复杂度的评估。给定*n*为数据集大小，*k*为聚类数，*c*为数据块数，*l*为迭代次数，*s*为粒子群大小。
- en: The data assignment step is the most expensive operation in PSO algorithm since
    it requires computing distances between each record to all the clusters of each
    particle in the swarm. Then, this step has to be repeated several times until
    convergences. Thus, the time complexity of PSO is evaluated by *O*(*n*.*k*.*s*.*l*).
    The S-PSO first divides the input data into *c* chunks that could be executed
    in parallel manner. So, S-PSO requires processing *n*/*c* records for each iteration.
    Hence, the time complexity of S-PSO is evaluated by *O*(*n*/*c*.*k*.*s*).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: PSO算法中数据分配步骤是最昂贵的操作，因为它需要计算每个记录到粒子群中所有聚类的距离。然后，这个步骤必须重复多次直到收敛。因此，PSO的时间复杂度为*O*(*n*.*k*.*s*.*l*)。S-PSO首先将输入数据划分为可以以并行方式执行的*c*个块。因此，S-PSO需要每次迭代处理*n*/*c*条记录。因此，S-PSO的时间复杂度为*O*(*n*/*c*.*k*.*s*)。
- en: 5 Experiments and Results
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验和结果
- en: 5.1 Environment
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 环境
- en: The experiments are realized on a cluster of 4 machines where each node has
    2-core 2.30 GHz CPU E5400 and 1 GB of memory. The experiments are performed using
    Apache Spark version 2.1.1, scala version 2.1.1, Apache Hadoop 2.7.0 and Ubuntu
    16.04.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 实验是在一组4台机器的集群上进行的，每个节点都有2核2.30GHz的CPU E5400和1GB内存。实验使用Apache Spark版本2.1.1、scala版本2.1.1、Apache
    Hadoop 2.7.0和Ubuntu 16.04进行。
- en: 5.2 Data Set Description
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 数据集描述
- en: In order to evaluate the performance of the proposed system, we used a Big intrusion
    detection data set^([1](#Fn1)) which was employed as the benchmark at the Knowledge
    Discovery and Data Mining in 1999\. This data set contains a standard set of data
    which includes a wide variety of normal and attack connections in a military network
    environment.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估所提出系统的性能，我们使用了一个大型入侵检测数据集^([1](#Fn1))，该数据集被用作1999年知识发现与数据挖掘中的基准。这个数据集包含了一组标准数据，其中包括了军事网络环境中各种正常和攻击连接。
- en: Each record in the collected data set represents a connection between two IP
    addresses. The data contains 4,898,431 connection records which are classified
    into normal traffic and four kinds of attacks namely, denial of service (DoS),
    probe (PRB), remote to local (R2L) and user to root (U2R). Each connection is
    described by 3 categorical and 38 numerical features for a total of 41 features.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 收集的数据集中每个记录表示两个IP地址之间的连接。数据包含4,898,431个连接记录，这些记录被分类为正常流量和四种攻击，即拒绝服务（DoS）、探测（PRB）、远程到本地（R2L）和用户到根（U2R）。每个连接由3个分类特征和38个数值特征描述，共41个特征。
- en: A set of pre-processing techniques were applied on the training and testing
    data sets. We first start by removing the records that have missing values. Then,
    we reduce the number of features to 38 by eliminating the 3 categorical features.
    Finally, we apply the normalization process on the training and testing data sets.Table
    1
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一组预处理技术被应用在训练和测试数据集上。我们首先删除具有缺失值的记录。然后，我们通过消除3个分类特征将特征数量减少到38。最后，我们对训练和测试数据集应用归一化处理。表1
- en: Summary of the data samples
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据样本摘要
- en: '| Data set | Number of connections | Normal | Attack |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 连接数 | 正常 | 攻击 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Train20 | 979,686 | 194,556 | 785,130 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Train20 | 979,686 | 194,556 | 785,130 |'
- en: '| Train40 | 1,959,372 | 389,112 | 1,570,260 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Train40 | 1,959,372 | 389,112 | 1,570,260 |'
- en: '| Train80 | 3,918,745 | 778,225 | 3,140,520 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Train80 | 3,918,745 | 778,225 | 3,140,520 |'
- en: '| Train100 | 4,898,431 | 972,781 | 3,925,650 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Train100 | 4,898,431 | 972,781 | 3,925,650 |'
- en: In order to evaluate the impact of the data size on the performance of the detector
    model, we extract 4 different data samples from the whole training data set. To
    simplify the names of the data samples, we will use the following notations Train20,
    Train40, Train80 and Train100 to denote an extracted data set which stores 20%,
    40%, 80% and 100% of the whole training data set. Statistics of these data sets
    are summarized in Table [1](#Tab1).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估数据大小对检测器模型性能的影响，我们从整个训练数据集中提取了4个不同的数据样本。为了简化数据样本的命名，我们将使用以下符号Train20、Train40、Train80和Train100来表示存储整个训练数据集的20%、40%、80%和100%的提取数据集。这些数据集的统计信息总结在表[1](#Tab1)中。
- en: 5.3 Evaluation Measures
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 评估措施
- en: In order to evaluate the scalability of the proposed system, we use the Speedup
    measure [[38](#CR38)] which consists on fixing the data set size and varying the
    number of machines. The Speedup measure is defined as follows:![$$\begin{aligned}
    Speedup=\frac{T_{1}}{T_{m}}, \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ7.png)(7)where
    ![$$T_{1}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq56.png)
    is the running time of processing data on 1 machine and ![$$T_{m}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq57.png)
    is the running time of processing data on *m* machines.In order to evaluate the
    quality of clustering of the proposed system, we used true positives, true negatives,
    false positives, and false negatives. A true positive (TP) indicates that the
    intrusion detection system detects precisely a particular attack having occurred.
    A true negative (TN) indicates that the intrusion detection system has not made
    a mistake in detecting a normal connection. A false positive (FP) indicates that
    a particular attack has been detected by the intrusion detection system but that
    such an attack did not actually occur. A false negative (FN) indicates that the
    intrusion detection system is unable to detect the intrusion after a particular
    attack has occurred. We use in this paper the True Positive Rate (TPR) and False
    Positive Rate (FPR), which are defined in Eq. [8](#Equ8) and [10](#Equ10) respectively.![$$\begin{aligned}
    TPR=\frac{TP}{TP+FN} \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ8.png)(8)![$$\begin{aligned}
    FPR=\frac{FP}{FP+TN} \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ9.png)(9)Furthermore,
    we use the Area Under Curve (AUC) measure [[43](#CR43)] to combine the TPR and
    FPR which is considered a good indicator of these rates. The AUC can be defined
    as follows:![$$\begin{aligned} AUC=\frac{(1-FPR)\times (1+TPR)}{2}+\frac{FPR\times
    TPR}{2} \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ10.png)(10)A
    greater value of these measures indicates better quality results.Table 2
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估所提出系统的可扩展性，我们使用了速度提升（Speedup）度量[[38](#CR38)]，该度量是通过固定数据集大小并改变机器数量来定义的。速度提升的定义如下：![$$\begin{aligned}
    Speedup=\frac{T_{1}}{T_{m}}, \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ7.png)(7)其中![$$T_{1}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq56.png)是在一台机器上处理数据的运行时间，![$$T_{m}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_IEq57.png)是在*m*台机器上处理数据的运行时间。为了评估所提出系统的聚类质量，我们使用了真正例、真负例、假正例和假负例。真正例（TP）表示入侵检测系统精确地检测到发生了特定的攻击。真负例（TN）表示入侵检测系统在检测正常连接时没有出错。假正例（FP）表示入侵检测系统检测到了特定的攻击，但实际上并没有发生这样的攻击。假负例（FN）表示入侵检测系统无法在特定攻击发生后检测到入侵。本文使用了真正率（TPR）和假正率（FPR），它们分别在方程[8](#Equ8)和[10](#Equ10)中定义。![$$\begin{aligned}
    TPR=\frac{TP}{TP+FN} \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ8.png)(8)![$$\begin{aligned}
    FPR=\frac{FP}{FP+TN} \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ9.png)(9)此外，我们使用曲线下面积（AUC）度量[[43](#CR43)]
    来结合TPR和FPR，这被认为是这些率的良好指标。AUC可以定义如下：![$$\begin{aligned} AUC=\frac{(1-FPR)\times
    (1+TPR)}{2}+\frac{FPR\times TPR}{2} \end{aligned}$$](../images/507793_1_En_11_Chapter/507793_1_En_11_Chapter_TeX_Equ10.png)(10)这些度量的值越大，表示结果质量越好。表2
- en: Comparison of the accuracy of IDS-SPSO versus IDS-MRPSO
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: IDS-SPSO与IDS-MRPSO准确性的比较
- en: '| Dataset | Method | TPR | FPR | AUC |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | TPR | FPR | AUC |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Train20 | IDS-MRPSO | 0.903 | 0.038 | 0.933 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Train20 | IDS-MRPSO | 0.903 | 0.038 | 0.933 |'
- en: '| IDS-SPSO | 0.848 | 0.096 | 0.875 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| IDS-SPSO | 0.848 | 0.096 | 0.875 |'
- en: '| Train40 | IDS-MRPSO | 0.911 | 0.021 | 0.945 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Train40 | IDS-MRPSO | 0.911 | 0.021 | 0.945 |'
- en: '| IDS-SPSO | 0.856 | 0.085 | 0.888 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| IDS-SPSO | 0.856 | 0.085 | 0.888 |'
- en: '| Train80 | IDS-MRPSO | 0.935 | 0.013 | 0.961 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Train80 | IDS-MRPSO | 0.935 | 0.013 | 0.961 |'
- en: '| IDS-SPSO | 0.879 | 0.068 | 0.904 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| IDS-SPSO | 0.879 | 0.068 | 0.904 |'
- en: '| Train100 | IDS-MRPSO | 0.939 | 0.013 | 0.963 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Train100 | IDS-MRPSO | 0.939 | 0.013 | 0.963 |'
- en: '| IDS-SPSO | 0.883 | 0.059 | 0.905 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| IDS-SPSO | 0.883 | 0.059 | 0.905 |'
- en: '![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig6_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig6_HTML.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig6_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig6_HTML.png)'
- en: Fig. 6
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6
- en: Comparison of the running time of IDS-SPSO versus IDS-MRPSO
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: IDS-SPSO与IDS-MRPSO运行时间的比较
- en: 5.4 Results
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 结果
- en: 'We use in the experiments the following parameters: the number of particles
    to 10, the number of iterations to 50, the inertia weight to 0.72 and the acceleration
    coefficients to 1.49\. We first evaluate the accuracy of the proposed IDS-SPSO
    compared to IDS-MRPSO system. Table [2](#Tab2) reports the TPR, FPR, and AUC values
    obtained by the proposed system using different training data samples sizes compared
    to IDS-MRPSO system. The obtained results show that the proposed IDS-SPSO gives
    nearly same results of existing IDS-MRPSO system. In addition, we observed from
    this table that the TPR value of IDS-SPSO using the whole training data (i.e.
    Train100) reaches the best value compared to smaller training data sets. Furthermore,
    Table [2](#Tab2) shows that IDS-SPSO obtains the lowest FPR for Train100 data
    set. For instance, the IDS-MRCPSO system has a high TPR of 0.883 for Train100,
    while it has a TPR of 0.848 for Train20\. In addition, it has a low FPR of 0.059
    for Train100, while it has a PDR of 0.096 for Train20\. Hence, we observed that
    the proposed system can distinguish effectively between the normal and attacks
    data records. Finally, we concluded that the obtained results show the improvement
    of accuracy when using larger training data sets.![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig7_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig7_HTML.png)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们使用以下参数：粒子数量为10，迭代次数为50，惯性权重为0.72，加速系数为1.49。我们首先评估了所提出的 IDS-SPSO 相对于 IDS-MRPSO
    系统的准确性。表 [2](#Tab2) 报告了所提出的系统使用不同的训练数据样本大小与 IDS-MRPSO 系统相比获得的 TPR、FPR 和 AUC 值。得到的结果显示，所提出的
    IDS-SPSO 给出了与现有 IDS-MRPSO 系统几乎相同的结果。此外，我们从这张表中观察到，IDS-SPSO 使用整个训练数据（即 Train100）的
    TPR 值达到了最佳值，相比较较小的训练数据集。此外，表 [2](#Tab2) 显示 IDS-SPSO 对于 Train100 数据集获得了最低的 FPR。例如，IDS-MRCPSO
    系统在 Train100 上的 TPR 高达0.883，而在 Train20 上为 0.848。此外，IDS-MRCPSO 在 Train100 上的 FPR
    为0.059，而在 Train20 上为0.096。因此，我们观察到所提出的系统能够有效区分正常和攻击数据记录。最后，我们得出结论，当使用较大的训练数据集时，准确性有所提高。![../images/507793_1_En_11_Chapter/507793_1_En_11_Fig7_HTML.png](../images/507793_1_En_11_Chapter/507793_1_En_11_Fig7_HTML.png)
- en: Fig. 7
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7
- en: Evaluation of Speedup results for KDD data set samples from 20% to 100% sizes
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对 KDD 数据集样本从 20% 到 100% 大小的加速结果评估
- en: We then evaluate the running time of the proposed system compared to the IDS-MRPSO
    system. Figure [6](#Fig6) shows the running time results for the 4 training data
    samples using different numbers of machines. The obtained results show that the
    proposed system is faster than existing IDS-MRPSO system. For instance, the IDS-SPO
    is faster by a factor of 1.52 and 2.66 than IDS-MRPSO respectively for Train20
    and Train100 data sets. From this Figure, we can also observe the improvement
    of running time when the number of machines is increased. For example the running
    time on 1 machine takes 870, 1740, 3480 and 4350 s for Train20, Train40, Train80,
    and Train100, respectively, while the running time on 4 machines takes 245, 477,
    901 and 1092 s for the same samples respectively.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们评估了所提出系统的运行时间与 IDS-MRPSO 系统的运行时间。图 [6](#Fig6) 显示了使用不同数量的机器对 4 个训练数据样本的运行时间结果。得到的结果显示，所提出的系统比现有的
    IDS-MRPSO 系统更快。例如，IDS-SPO 对于 Train20 和 Train100 数据集分别比 IDS-MRPSO 快了 1.52 倍和 2.66
    倍。从这张图中，我们还可以观察到当机器数量增加时运行时间的改善。例如，对于相同的样本，1 台机器的运行时间分别为 870、1740、3480 和 4350
    秒，而 4 台机器的运行时间分别为 245、477、901 和 1092 秒。
- en: We then evaluate the scalability of the proposed system, by running multiple
    experiments with different number of machines. Figure [7](#Fig7) shows the Speedup
    results using different training data sizes with different numbers of machines.
    From this Figure, we observed that Speedup results become important especially
    when the data size is increased. For example, the Speedup value when running IDS-SPSO
    using 4 machines for Train20 is 3.57 while it is 3.90 for Train100 data. In addition,
    the proposed system shows approximately a linear speedup when the number of machines
    increases. This is explained by the benefits of the in-memory processing of Spark
    framework, which can significantly reduce the network cost when we increase the
    number of machines.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后通过在不同数量的机器上运行多个实验来评估所提出系统的可伸缩性。图 [7](#Fig7) 显示了使用不同数量的机器以及不同训练数据大小时的加速比结果。从这个图表中，我们观察到当数据量增加时，加速比结果变得尤其重要。例如，使用
    4 台机器运行 IDS-SPSO 在 Train20 数据时的加速比值为 3.57，而在 Train100 数据时为 3.90。此外，所提出的系统在机器数量增加时显示出近似线性的加速比。这可以通过
    Spark 框架的内存处理的优势来解释，当我们增加机器数量时，可以显著降低网络成本。
- en: 6 Conclusion
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we proposed an intrusion detection system IDS-SPSO for large
    scale of network traffic. The proposed system incorporates clustering analysis
    to build the detection model by solving the intrusion detection problem using
    particle swarm optimization clustering algorithm. We have also shown in this work
    that the intrusion detection system can be efficiently distributed through Spark
    framework. Experiments were realized on a real intrusion data set in order to
    evaluate the scalability of the proposed system. The experimental results show
    the efficiency of the IDS-SPSO when we increase both the number of machines and
    the training data size. Furthermore, the experiments results show that using larger
    training data leads to better detection rates by keeping the false alarm very
    low.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种用于大规模网络流量的入侵检测系统 IDS-SPSO。所提出的系统通过使用粒子群优化聚类算法解决入侵检测问题来将聚类分析纳入构建检测模型。我们还在这项工作中展示了入侵检测系统可以通过
    Spark 框架有效地分布。实验是在真实入侵数据集上实现的，以评估所提出系统的可伸缩性。实验结果显示，当我们增加机器数量和训练数据大小时，IDS-SPSO
    的效率得到了提高。此外，实验结果表明，使用更大的训练数据可以通过保持假警报非常低来获得更好的检测率。
- en: Our future work is to incorporate algorithms which are capable of providing
    automatically the number of clusters. Furthermore, we will extend the proposed
    system by employing feature selection techniques to extract the most important
    features when building clusters.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们未来的工作是将能够自动提供聚类数量的算法纳入。此外，我们将通过采用特征选择技术来扩展所提出的系统，在构建聚类时提取最重要的特征。
